{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 69\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "FOLDER_NAME = \"SHORT_NOTEBOOK1_CSV_FILES\"\n",
    "GLOBAL_VERBOSE = False\n",
    "PREPROCESSORS = [\"quarters\",\"statistical\", \"trimmedMean\"]\n",
    "LETTERS = [\"A\", \"B\", \"C\"]\n",
    "if not os.path.exists(FOLDER_NAME):\n",
    "    os.makedirs(FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# using a class to mimic a namespace:\n",
    "class pre:\n",
    "\n",
    "    def clean_NaN(df):\n",
    "        df = df.copy()\n",
    "        df.dropna(subset=['target'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def remove_long_sequences(df, col_name, seq_len):\n",
    "        df = df.copy()\n",
    "        # Identify sequences of zeros\n",
    "        df['group'] = (df[col_name] != 0).cumsum()\n",
    "        df['group_count'] = df.groupby('group')[col_name].transform('count')\n",
    "        \n",
    "        # Create a mask to identify rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        mask = (df[col_name] == 0) & (df['group_count'] > seq_len) #& (df['is_in_shadow:idx'] < 1)\n",
    "        \n",
    "        # Remove rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        df_cleaned = df[~mask].drop(columns=['group', 'group_count'])\n",
    "        return df_cleaned.copy()\n",
    "\n",
    "\n",
    "    def remove_repeating_nonzero(df, col_name, repeat_count=5):\n",
    "        df = df.copy()\n",
    "        # create a mask to identify rows with repeating nonzero values in the target column\n",
    "        mask = ((df[col_name] != 0) & (df[col_name].shift(1) == df[col_name]))\n",
    "        # create a mask to identify rows with repeating nonzero values that occur more than repeat_count times\n",
    "        repeat_mask = mask & (mask.groupby((~mask).cumsum()).cumcount() >= repeat_count)\n",
    "        # create a mask to identify the complete sequence of repeating nonzero values\n",
    "        seq_mask = repeat_mask | repeat_mask.shift(-5)\n",
    "        # remove rows with repeating nonzero values that occur more than repeat_count times\n",
    "        df = df[~seq_mask]\n",
    "        return df\n",
    "\n",
    "    def clean(df):\n",
    "        df = df.copy()\n",
    "        df=pre.clean_NaN(df)\n",
    "        df=pre.remove_long_sequences(df, 'target', 60)\n",
    "        df=pre.remove_repeating_nonzero(df, 'target')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def encode(data, col, max_val):\n",
    "        data = data.copy()\n",
    "        data = data.copy()\n",
    "        data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "        data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "        return data\n",
    "\n",
    "    def create_time_features(df):\n",
    "        df = df.copy()\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        df[\"dayofyear\"]=df.index.dayofyear\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"week\"] = df.index.isocalendar().week\n",
    "\n",
    "        #zero indexing:\n",
    "        df[\"dayofyear\"]-=1\n",
    "        df[\"month\"]-=1\n",
    "        df[\"week\"]-=1\n",
    "\n",
    "\n",
    "        #Cycling the time features:\n",
    "        df = pre.encode(df, \"hour\", 24)\n",
    "        df = pre.encode(df, \"month\", 12)\n",
    "        df = pre.encode(df, \"week\", 53)\n",
    "        df = pre.encode(df, \"dayofyear\", 366)\n",
    "\n",
    "        df.drop(columns=[\"hour\", \"month\", \"week\", \"dayofyear\"], inplace=True)\n",
    "\n",
    "\n",
    "        df[\"mult1\"]=(1-df[\"is_in_shadow:idx\"])*df['direct_rad:W']\n",
    "        df[\"mult2\"]=(1-df[\"is_in_shadow:idx\"])*df['clear_sky_rad:W']\n",
    "        df[\"date_calc\"]=pd.to_datetime(df[\"date_calc\"])\n",
    "        df.index=pd.to_datetime(df.index)\n",
    "        df[\"uncertainty\"]=(df.index-df[\"date_calc\"]).apply(lambda x: x.total_seconds()/3600)\n",
    "        df[\"uncertainty\"].fillna(0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def create_features(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        df.dropna(subset=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "        df[\"total_solar_rad\"]=df[\"direct_rad:W\"]+df[\"diffuse_rad:W\"]\n",
    "        #df[\"clear_sky_%\"]=df[\"total_solar_rad\"]/df[\"clear_sky_rad:W\"]*100\n",
    "        #df[\"clear_sky_%\"].fillna(0, inplace=True)\n",
    "        df[\"spec humid\"]=df[\"absolute_humidity_2m:gm3\"]/df[\"air_density_2m:kgm3\"]\n",
    "        df[\"temp*total_rad\"]=df[\"t_1000hPa:K\"]*df[\"total_solar_rad\"]\n",
    "        df[\"wind_angle\"]=(np.arctan2(df[\"wind_speed_u_10m:ms\"],df[\"wind_speed_v_10m:ms\"]))*180/np.pi\n",
    "        #df[\"total_snow_depth\"] = df[\"snow_depth:cm\"] + df[\"fresh_snow_1h:cm\"]\n",
    "        #df[\"total_precip_5min\"] = df[\"precip_5min:mm\"] + df[\"snow_melt_10min:mm\"]\n",
    "        #df[\"total_precip_type\"] = df[\"precip_type_5min:idx\"] + df[\"snow_water:kgm2\"]\n",
    "        df[\"total_pressure\"] = df[\"pressure_50m:hPa\"] + df[\"pressure_100m:hPa\"]\n",
    "        df[\"total_sun_angle\"] = df[\"sun_azimuth:d\"] + df[\"sun_elevation:d\"]\n",
    "        df[\"solar intensity\"]=1361*np.cos(np.radians(90-df[\"sun_elevation:d\"]))\n",
    "        df[\"solar intensity\"].clip(lower=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def shift_target(df, target_col):\n",
    "        df = df.copy()\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Store the original indices\n",
    "        original_indices = df.index\n",
    "\n",
    "        # Reindex the DataFrame to include all 15-minute intervals\n",
    "        all_intervals = pd.date_range(start=df.index.min(), end=df.index.max(), freq='15T')\n",
    "        df = df.reindex(all_intervals)\n",
    "\n",
    "        # Shift the target variable by 1 period (15 minutes) forward and backward\n",
    "        df[target_col + '_shifted_forward'] = df[target_col].shift(-1)\n",
    "        df[target_col + '_shifted_backward'] = df[target_col].shift(1)\n",
    "\n",
    "        # Forward fill the missing values for the forward shift\n",
    "        df[target_col + '_shifted_forward'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill the missing values for the backward shift\n",
    "        df[target_col + '_shifted_backward'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Keep only the original indices\n",
    "        df = df.loc[original_indices]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def add_lagged_features(df):\n",
    "        df = df.copy()\n",
    "        features_to_lag = [ \"total_solar_rad\", \"temptotal_rad\", \"clear_sky_radW\", \"diffuse_radW\", \"direct_radW\",  \"total_cloud_coverp\", \"solarintensity\", \"total_sun_angle\", \"pressure_100mhPa\"] #these were the most important features, and therefore assumed to be the most important to lag \n",
    "        \n",
    "        for feature in features_to_lag:\n",
    "            df = pre.shift_target(df, feature)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    #experiments showed that flaml preformed slightly better with fewer features, no lagged features:\n",
    "    def general_read_flaml(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        #X_final = add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "\n",
    "    def general_read(letter):\n",
    "\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        X_final = pre.add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "\n",
    "    #the general read function returns a list of 4-row DataFrames, that was in order to have the same lenght of X and Y, therefore this function was needed.\n",
    "    def concatenate_dfs(df_list):\n",
    "        \"\"\"\n",
    "        Concatenates a list of DataFrames into a single DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df_list (list of pd.DataFrame): List of DataFrame objects to concatenate.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A single DataFrame containing all rows from the input DataFrames in the order they appear in the list.\n",
    "        \"\"\"\n",
    "        return pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "    #preprocessor which could be used in a pipeline:\n",
    "    class QuartersAsColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None):\n",
    "            # Ensure input is a DataFrame\n",
    "            X = X.copy()\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            #make sure index is datetime:\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "\n",
    "            original_index = X.index\n",
    "\n",
    "\n",
    "            X['hour'] = X.index.floor('H')\n",
    "            X['minute'] = X.index.minute\n",
    "\n",
    "            # Melt the DataFrame to long format\n",
    "            df_melted = pd.melt(X, id_vars=['hour', 'minute'], value_vars=X.columns[:-2]).copy()  # excluding 'hour' and 'minute'\n",
    "\n",
    "            # Create a multi-level column name combining variable and minute\n",
    "            df_melted['variable_minute'] = df_melted['variable'] + '_' + df_melted['minute'].astype(str) + 'min'\n",
    "\n",
    "            # Drop the 'variable_minute' column\n",
    "\n",
    "\n",
    "            # Pivot the data to get one row per hour and columns for each variable and minute\n",
    "            X = df_melted.pivot(index='hour', columns='variable_minute', values='value').copy()\n",
    "            #rename index to date_forecast:\n",
    "            X.index.rename(\"date_forecast\", inplace=True)\n",
    "\n",
    "\n",
    "            #drop irrelevant columns:\n",
    "            #hour_sin\thour_cos\tmonth_sin\tmonth_cos\tweek_sin\tweek_cos\tdayofyear_sin\tdayofyear_cos\tmult1\tmult2\tuncertainty\n",
    "\n",
    "\n",
    "            irrelevant_cols = [\"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\", \"week_sin\", \"week_cos\", \"dayofyear_sin\", \"dayofyear_cos\", \"uncertainty\"]\n",
    "            variantes = [\"_0min\", \"_15min\", \"_30min\", \"_45min\"]\n",
    "            for variant in variantes:\n",
    "                for col in irrelevant_cols:\n",
    "                    if variant == \"_0min\":\n",
    "                        #remove _0min from column name;\n",
    "                        X.rename(columns={col+variant:col}, inplace=True)\n",
    "                    else:\n",
    "                        X.drop(columns=[col+variant], inplace=True)\n",
    "            \n",
    "\n",
    "            reindex_map = original_index.floor('H').unique()\n",
    "            X = X.reindex(reindex_map)\n",
    "            X.index = reindex_map\n",
    "\n",
    "            #drop hour_\n",
    "\n",
    "            if \"object\" in X.dtypes.unique():\n",
    "                print(\"waring: object in QuarterAsColumnsTransformer\")\n",
    "                print(X.dtypes.unique())\n",
    "                for col in X.columns:\n",
    "                    print(col)\n",
    "\n",
    "            #X = X.select_dtypes(include=[np.number])\n",
    "            return X\n",
    "        \n",
    "    #preprocessor which could be used in a pipeline:\n",
    "    class StatisticalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X, y=None):\n",
    "            X_copy = X.copy()\n",
    "            X_copy.index = pd.to_datetime(X_copy.index)\n",
    "            X_copy['hour'] = X_copy.index.floor('H')\n",
    "            \n",
    "            # Compute mean, std\n",
    "            aggregated = X_copy.groupby('hour').agg(['mean', 'std'])\n",
    "            \n",
    "            # Filter hours with exactly 4 data points\n",
    "            valid_hours = X_copy.groupby('hour').size()\n",
    "            valid_hours = valid_hours[valid_hours == 4].index\n",
    "            \n",
    "            X_final = aggregated.loc[valid_hours]\n",
    "            \n",
    "            # Flatten the multi-index to form new column names\n",
    "            X_final.columns = ['_'.join(col).strip() for col in X_final.columns.values]\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "            #drop minute_mean and minute_std if they exist:\n",
    "            if \"minute_mean\" in X_final.columns:\n",
    "                X_final.drop(columns=[\"minute_mean\", \"minute_std\"], inplace=True)\n",
    "            \n",
    "            X_final = X_final.select_dtypes(include=[np.number])\n",
    "            # print(X_final.dtypes.unique())\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "\n",
    "\n",
    "            return X_final\n",
    "        \n",
    "    \n",
    "\n",
    "    class HourMonthTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.encoding_map = {}\n",
    "            self.y_ = None  # To store y during fit\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"y cannot be None for fitting the encoder\")\n",
    "\n",
    "            # Store the target values for encoding later\n",
    "            self.y_ = y\n",
    "\n",
    "            try:\n",
    "                # Extract hour and month from the index and use y provided during fit\n",
    "                df = pd.DataFrame({'target': self.y_, 'hour': X.index.hour, 'month': X.index.month})\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "            # Compute mean target value for each hour of each month\n",
    "            self.encoding_map = df.groupby(['month', 'hour'])['target'].mean().to_dict()\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if self.y_ is None:\n",
    "                raise ValueError(\"The encoder has not been fitted with target values\")\n",
    "\n",
    "            # Extract hour and month from the index\n",
    "            X_transformed = X.copy()\n",
    "            X_transformed['hour'] = X.index.hour\n",
    "            X_transformed['month'] = X.index.month\n",
    "\n",
    "            # Map the mean target values\n",
    "            X_transformed['target_encoded'] = X_transformed.apply(\n",
    "                lambda row: self.encoding_map.get((row['month'], row['hour']), np.nan), axis=1)\n",
    "\n",
    "            # Optionally drop 'hour' and 'month' if they're not needed\n",
    "            X_transformed.drop(['hour', 'month'], axis=1, inplace=True)\n",
    "\n",
    "            # Check for object dtypes and print warning if any\n",
    "            if \"object\" in X_transformed.dtypes.values:\n",
    "                print(\"Warning: object dtype in HourMonthTargetEncoder\")\n",
    "                print(X_transformed.dtypes)\n",
    "\n",
    "            # Ensure that only numeric types are returned\n",
    "            X_transformed = X_transformed.select_dtypes(include=[np.number])\n",
    "\n",
    "            return X_transformed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_preprocessor(data, preprocessor_name):\n",
    "        data = data.copy()\n",
    "        # Assuming `pre.choose_transformer` returns a callable object that can be used to transform the data\n",
    "        preprocessor = pre.choose_transformer(preprocessor_name)\n",
    "        return preprocessor.transform(data)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def train_test_split_may_june_july(X, y,letter):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        return X_train, y_train, X_may_june_july, y_may_june_july\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def new_train_test_split(X, y,letter, split_date):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        split_date = pd.Timestamp(split_date).normalize()\n",
    "        print(f\"Split date: {split_date}\")\n",
    "\n",
    "        if isinstance(split_date, str):\n",
    "            split_date = pd.Timestamp(split_date)\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "\n",
    "        X_train = X[X.index.normalize() < split_date]\n",
    "        y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "        X_test = X[X.index.normalize() >= split_date]\n",
    "        y_test = y[y.index.normalize() >= split_date]\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        # Append May and June data to test set\n",
    "        X_test = pd.concat([X_may_june_july, X_test])\n",
    "        y_test = pd.concat([y_may_june_july, y_test])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "    def choose_scaler(scaler_string):\n",
    "        if scaler_string == \"minmax\":\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_string == \"standard\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_string == \"robust\":\n",
    "            return RobustScaler()\n",
    "\n",
    "    def choose_transformer(transformer_string):\n",
    "        if transformer_string == \"quarters\":\n",
    "            return pre.QuartersAsColumnsTransformer()\n",
    "        elif transformer_string == \"statistical\":\n",
    "            return pre.StatisticalFeaturesTransformer()\n",
    "        elif transformer_string == \"trimmedMean\":\n",
    "            return pre.TrimmedMeanTransformer()\n",
    "\n",
    "    def choose_encoder(encoder_boolian):\n",
    "        if encoder_boolian == True:\n",
    "            return pre.HourMonthTargetEncoder()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def generate_predefined_split(X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        This function takes in separate training and validation datasets, combines them,\n",
    "        and creates a PredefinedSplit object that can be used with sklearn's GridSearchCV\n",
    "        or other model selection utilities. This allows for specifying which samples are\n",
    "        used for training and which are used for validation.\n",
    "\n",
    "        Parameters:\n",
    "        X_train (array-like): Training features.\n",
    "        X_val (array-like): Validation features.\n",
    "        y_train (array-like): Training labels.\n",
    "        y_val (array-like): Validation labels.\n",
    "\n",
    "        Returns:\n",
    "        X (array-like): The combined dataset of features.\n",
    "        y (array-like): The combined dataset of labels.\n",
    "        split_index (PredefinedSplit): An instance of PredefinedSplit with the indices set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Combine the training and validation sets\n",
    "        X = np.concatenate((X_train, X_val), axis=0)\n",
    "        y = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "        # Generate the indices array where -1 indicates the sample is part of the training set,\n",
    "        # and 0 indicates the sample is part of the validation set.\n",
    "        train_indices = -1 * np.ones(len(X_train))\n",
    "        val_indices = 0 * np.ones(len(X_val))\n",
    "        test_fold = np.concatenate((train_indices, val_indices))\n",
    "\n",
    "        # Create the PredefinedSplit object\n",
    "        predefined_split = PredefinedSplit(test_fold)\n",
    "\n",
    "        return X, y, predefined_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ensPre:\n",
    "    def preptest(df,features_list,stats_dict):\n",
    "        df.drop(columns=[\"ceiling_height_agl:m\",\"snow_density:kgm3\",\"cloud_base_agl:m\"],inplace=True)\n",
    "        df=ensPre.create_features(df)\n",
    "        #df=unleash_hell(df)\n",
    "        df=ensPre.create_time_features(df)\n",
    "        #df=df[features_list[:-1]]\n",
    "        df=ensPre.cyclical_features(df)\n",
    "        df=ensPre.add_stats_features(df, stats_dict)\n",
    "        df=ensPre.shift_target(df,\"total_solar_rad\")\n",
    "        df.dropna(inplace=True)\n",
    "        df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "        return df\n",
    "    def readDataSet(letter):\n",
    "        # read X_train_observed.parquet file for the current letter\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        \n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        \n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.groupby(df.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        df2 = df2.groupby(df2.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "        y.index = pd.to_datetime(y.index)\n",
    "\n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        df = df.truncate(before=y.index[0], after=y.index[-1])\n",
    "        y=y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        df=pd.concat([df,y],axis=1)\n",
    "        df.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def readtest(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_test_estimated.parquet\")\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.groupby(df.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        return df\n",
    "    def cleanNaN(df):\n",
    "        df.dropna(subset=['target'], inplace=True)\n",
    "        return df\n",
    "    # define a function to remove rows with sequences longer than 50 zeros\n",
    "    #remove NaN first to avoid NaN 0 sequence not being removed\n",
    "    def remove_long_sequences(df, col_name, seq_len):\n",
    "        # Identify sequences of zeros\n",
    "        df['group'] = (df[col_name] != 0).cumsum()\n",
    "        df['group_count'] = df.groupby('group')[col_name].transform('count')\n",
    "        \n",
    "        # Create a mask to identify rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        mask = (df[col_name] == 0) & (df['group_count'] > seq_len) & (df['is_in_shadow:idx'] < 1)\n",
    "        \n",
    "        # Remove rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        df_cleaned = df[~mask].drop(columns=['group', 'group_count'])\n",
    "        return df_cleaned.copy()\n",
    "    \n",
    "    def remove_repeating_nonzero(df, col_name, repeat_count=5):\n",
    "        # create a mask to identify rows with repeating nonzero values in the target column\n",
    "        mask = ((df[col_name] != 0) & (df[col_name].shift(1) == df[col_name]))\n",
    "        # create a mask to identify rows with repeating nonzero values that occur more than repeat_count times\n",
    "        repeat_mask = mask & (mask.groupby((~mask).cumsum()).cumcount() >= repeat_count)\n",
    "        # create a mask to identify the complete sequence of repeating nonzero values\n",
    "        seq_mask = repeat_mask | repeat_mask.shift(-5)\n",
    "        # remove rows with repeating nonzero values that occur more than repeat_count times\n",
    "        df = df[~seq_mask]\n",
    "        return df\n",
    "    def clean(df):\n",
    "        df=ensPre.cleanNaN(df)\n",
    "        df=ensPre.remove_long_sequences(df, 'target', 30)\n",
    "        df=ensPre.remove_repeating_nonzero(df, 'target')\n",
    "        #df=remove_outliers_hourly_and_month(df, 'target')\n",
    "        return df\n",
    "    def remove_outliers_hourly_and_month(df, target_col):\n",
    "        for month in range (1,13):\n",
    "            df_month = df[df.index.month == month]\n",
    "            for i in range(24):\n",
    "                df_hour=df_month[df_month.index.hour == i]\n",
    "                Q1 = df_hour[target_col].quantile(0.25)\n",
    "                Q3 = df_hour[target_col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df_hour_rem = df_hour[(df_hour[target_col] >= lower_bound) & (df_hour[target_col] <= upper_bound)]\n",
    "                if i==0:\n",
    "                    df_rem=df_hour_rem\n",
    "                else:\n",
    "                    df_rem=pd.concat([df_rem,df_hour_rem],axis=0)\n",
    "            if month==1:\n",
    "                df_rem_month=df_rem\n",
    "            else:\n",
    "                df_rem_month=pd.concat([df_rem_month,df_rem],axis=0)\n",
    "        df_rem_month.sort_index(inplace=True)\n",
    "        return df_rem_month\n",
    "    def calculate_hourly_monthly_means(df):\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Create new columns for the hour and the month\n",
    "        df['hour'] = df.index.hour\n",
    "        df['month'] = df.index.month\n",
    "\n",
    "        # Calculate the mean of the target variable for each hour during each month\n",
    "        stats = df.groupby(['month', 'hour'])['target'].agg(['mean', 'std'])\n",
    "\n",
    "        # Convert the DataFrame to a dictionary of dictionaries\n",
    "        stats_dict = stats.to_dict('index')\n",
    "\n",
    "        return stats_dict\n",
    "\n",
    "    def add_stats_features(df, stats_dict):\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        df['mean'] = df.apply(lambda row: stats_dict[(row['month'], row['hour'])]['mean'], axis=1)\n",
    "        df['std'] = df.apply(lambda row: stats_dict[(row['month'], row['hour'])]['std'], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cyclical_features(df):\n",
    "        df[\"hour_sin\"] = np.sin(df.index.hour*(2.*np.pi/24))\n",
    "        df[\"hour_cos\"] = np.cos(df.index.hour*(2.*np.pi/24))\n",
    "        df[\"dayofyear_sin\"] = np.sin(df.index.dayofyear*(2.*np.pi/365))\n",
    "        df[\"dayofyear_cos\"] = np.cos(df.index.dayofyear*(2.*np.pi/365))\n",
    "        return df.copy()\n",
    "        \n",
    "\n",
    "\n",
    "    def create_time_features(df):\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        #df[\"week\"]=df.index.isocalendar().week\n",
    "        df[\"dayofyear\"]=df.index.dayofyear\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"mult1\"]=(1-df[\"is_in_shadow:idx\"])*df['direct_rad:W']\n",
    "        df[\"mult2\"]=(1-df[\"is_in_shadow:idx\"])*df['clear_sky_rad:W']\n",
    "        df[\"date_calc\"]=pd.to_datetime(df[\"date_calc\"])\n",
    "        df.index=pd.to_datetime(df.index)\n",
    "        df[\"uncertainty\"]=(df.index-df[\"date_calc\"]).apply(lambda x: x.total_seconds()/3600)\n",
    "        df[\"uncertainty\"].fillna(0, inplace=True)\n",
    "        df.drop(columns=[\"date_calc\"],inplace=True)\n",
    "        return df.copy()\n",
    "\n",
    "    def trim(df,features_list):\n",
    "        df.drop(columns=[\"ceiling_height_agl:m\",\"snow_density:kgm3\",\"cloud_base_agl:m\"],inplace=True)\n",
    "        df=ensPre.clean(df)\n",
    "        df=ensPre.create_features(df)\n",
    "        #df=unleash_hell(df)\n",
    "        df=ensPre.create_time_features(df)\n",
    "        #df=df[features_list]\n",
    "        df=ensPre.cyclical_features(df)\n",
    "        stats_dict=ensPre.calculate_hourly_monthly_means(df)\n",
    "        df=ensPre.add_stats_features(df, stats_dict)\n",
    "        df=ensPre.shift_target(df,\"total_solar_rad\")\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "        #df = df[~df.index.month.isin([1, 2, 11, 12])]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_features(df):\n",
    "\n",
    "        df.dropna(subset=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "\n",
    "        df[\"total_solar_rad\"]=df[\"direct_rad:W\"]+df[\"diffuse_rad:W\"]\n",
    "\n",
    "        #df[\"clear_sky_%\"]=df[\"total_solar_rad\"]/df[\"clear_sky_rad:W\"]*100\n",
    "        #df[\"clear_sky_%\"].fillna(0, inplace=True)\n",
    "        df[\"spec humid\"]=df[\"absolute_humidity_2m:gm3\"]/df[\"air_density_2m:kgm3\"]\n",
    "        df[\"temp*total_rad\"]=df[\"t_1000hPa:K\"]*df[\"total_solar_rad\"]\n",
    "        df[\"wind_angle\"]=(np.arctan2(df[\"wind_speed_u_10m:ms\"],df[\"wind_speed_v_10m:ms\"]))*180/np.pi\n",
    "        #df[\"total_snow_depth\"] = df[\"snow_depth:cm\"] + df[\"fresh_snow_1h:cm\"]\n",
    "        #df[\"total_precip_5min\"] = df[\"precip_5min:mm\"] + df[\"snow_melt_10min:mm\"]\n",
    "        #df[\"total_precip_type\"] = df[\"precip_type_5min:idx\"] + df[\"snow_water:kgm2\"]\n",
    "        df[\"total_pressure\"] = df[\"pressure_50m:hPa\"] + df[\"pressure_100m:hPa\"]\n",
    "        df[\"total_sun_angle\"] = df[\"sun_azimuth:d\"] + df[\"sun_elevation:d\"]\n",
    "        df[\"solar intensity\"]=1361*np.cos(np.radians(90-df[\"sun_elevation:d\"]))\n",
    "        df[\"solar intensity\"].clip(lower=0, inplace=True)\n",
    "        df[\"cloud\"]=df[\"clear_sky_rad:W\"]-df[\"total_solar_rad\"]\n",
    "        df[\"cloud2\"]=df[\"clear_sky_rad:W\"]*(1-df[\"effective_cloud_cover:p\"])\n",
    "        df['temp*total_rad_squared'] = df['temp*total_rad']**2\n",
    "        df['total_solar_rad_squared']=df['total_solar_rad']**2\n",
    "        df['t_1000hPa:K_daystd']=df['t_1000hPa:K'].groupby(df.index.date).transform('std')\n",
    "        df['wind_angle_squared']=df['wind_angle']**2\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    def unleash_hell(df):\n",
    "        cols=df.columns\n",
    "        new_cols=[]\n",
    "        for col in cols:\n",
    "            if col==\"target\" or col==\"date_calc\":\n",
    "                continue\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('max').rename(f\"{col}_daymax\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('min').rename(f\"{col}_daymin\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('mean').rename(f\"{col}_daymean\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('std').rename(f\"{col}_daystd\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('sum').rename(f\"{col}_daysum\"))\n",
    "            #new_cols.append(pd.Series(np.log(1+df[col].values + 1e-8), index=df.index, name=f\"{col}_log\")) # convert numpy array to pandas Series\n",
    "            new_cols.append((df[col]**2).rename(f\"{col}_squared\"))\n",
    "        \"\"\"\n",
    "        for col1, col2 in itertools.combinations(cols, 2):\n",
    "            new_cols.append((df[col1] + df[col2]).rename(f\"{col1}+{col2}\"))\n",
    "            new_cols.append((df[col1] - df[col2]).rename(f\"{col1}-{col2}\"))\n",
    "            new_cols.append((df[col1] * df[col2]).rename(f\"{col1}*{col2}\"))\n",
    "            new_cols.append((df[col1] / df[col2]).rename(f\"{col1}/{col2}\"))\n",
    "        \"\"\"\n",
    "        df = pd.concat([df] + new_cols, axis=1)\n",
    "        return df.copy()\n",
    "            \n",
    "    def shift_target(df, target_col):\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Store the original indices\n",
    "        original_indices = df.index\n",
    "\n",
    "        # Reindex the DataFrame to include all hours\n",
    "        all_hours = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "        df = df.reindex(all_hours)\n",
    "\n",
    "        # Shift the target variable by 1 hour forward and backward\n",
    "        df[target_col + '_shifted_forward'] = df[target_col].shift(-1)\n",
    "        df[target_col + '_shifted_backward'] = df[target_col].shift(1)\n",
    "\n",
    "        # Forward fill the missing values for the forward shift\n",
    "        df[target_col + '_shifted_forward'].ffill(inplace=True)\n",
    "\n",
    "        # Backward fill the missing values for the backward shift\n",
    "        df[target_col + '_shifted_backward'].bfill(inplace=True)\n",
    "\n",
    "        # Keep only the original indices\n",
    "        df = df.loc[original_indices]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def new_train_test_split(X, y,letter,holdout=False):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        Len=len(X)\n",
    "        Lensum=len(X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)])\n",
    "        X=pd.concat([X,y],axis=1)\n",
    "    \n",
    "\n",
    "        # Convert index to DatetimeIndex object\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "\n",
    "        num_samples = int(0.05 * len(X))\n",
    "        fraction = num_samples / len(X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)])\n",
    "        X_test = X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)].sample(frac=fraction, random_state=69)\n",
    "\n",
    "        X = X.drop(X_test.index)\n",
    "\n",
    "        if holdout:\n",
    "            X_holdout=X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)].sample(frac=fraction*2, random_state=69)\n",
    "            X_train = X.drop(X_holdout.index)\n",
    "            y_holdout=X_holdout[\"target\"]\n",
    "            X_holdout=X_holdout.drop(columns=[\"target\"])\n",
    "        else:\n",
    "            X_train = X\n",
    "            y_holdout = None\n",
    "            X_holdout = None\n",
    "\n",
    "        \n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        y_train=X_train[\"target\"]\n",
    "        y_test=X_test[\"target\"]\n",
    "        X_train=X_train.drop(columns=[\"target\"])\n",
    "        X_test=X_test.drop(columns=[\"target\"])\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, X_holdout, y_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "\n",
    "class post:    \n",
    "    def readRawTest(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_test_estimated.parquet\")\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "\n",
    "    def readAndBasicPreprocess(letter):\n",
    "        X = post.readRawTest(letter)\n",
    "        X.drop(columns=['cloud_base_agl:m'], inplace=True)\n",
    "        X.drop(columns=['ceiling_height_agl:m'], inplace=True)\n",
    "        X.drop(columns=['snow_density:kgm3'], inplace=True)\n",
    "        X=pre.create_features(X)\n",
    "        X=pre.create_time_features(X)\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "        X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        X = pre.add_lagged_features(X)\n",
    "        return X\n",
    "\n",
    "    def makePrediction(A_model, B_model, C_model, filename):\n",
    "        A_x_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_y_pred=A_model.predict(A_x_test)\n",
    "        A_y_pred=pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_x_test= post.readAndBasicPreprocess(\"B\")\n",
    "        B_y_pred=B_model.predict(B_x_test)\n",
    "        B_y_pred=pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_x_test= post.readAndBasicPreprocess(\"C\")\n",
    "        C_y_pred=C_model.predict(C_x_test)\n",
    "        C_y_pred=pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    \n",
    "    def make_dnn_prediction(A_model, A_preprocessing, A_target_scaling, B_model, B_preprocessing, B_target_scaling, C_model, C_preprocessing, C_target_scaling, filename):\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test_dnn = pd.DataFrame(A_preprocessing.transform(A_X_test))\n",
    "        A_y_pred_dnn = A_model.predict(A_X_test_dnn)\n",
    "        A_y_pred_dnn = A_target_scaling.inverse_transform(A_y_pred_dnn).reshape(-1)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred_dnn, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test_dnn = pd.DataFrame(B_preprocessing.transform(B_X_test))\n",
    "        B_y_pred_dnn = B_model.predict(B_X_test_dnn)\n",
    "        B_y_pred_dnn = B_target_scaling.inverse_transform(B_y_pred_dnn).reshape(-1)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred_dnn, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test_dnn = pd.DataFrame(C_preprocessing.transform(C_X_test))\n",
    "        C_y_pred_dnn = C_model.predict(C_X_test_dnn)\n",
    "        C_y_pred_dnn = C_target_scaling.inverse_transform(C_y_pred_dnn).reshape(-1)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred_dnn, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "    def compute_mae(ser1, ser2):\n",
    "        \"\"\"Compute Mean Absolute Error between two Series.\"\"\"\n",
    "        return np.abs(ser1 - ser2).mean()\n",
    "\n",
    "    def plot_mae_grid(dataframes_dict):\n",
    "        \"\"\"Plot a grid of MAE values for a dictionary of DataFrames.\"\"\"\n",
    "        \n",
    "        labels = list(dataframes_dict.keys())\n",
    "        dataframes = list(dataframes_dict.values())\n",
    "        n = len(dataframes)\n",
    "        \n",
    "        mae_grid = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    mae_grid[i][j] = post.compute_mae(dataframes[i][\"prediction\"], dataframes[j][\"prediction\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        cax = ax.matshow(mae_grid, cmap=\"viridis\")\n",
    "        \n",
    "        ax.grid(False)\n",
    "        plt.xticks(range(n), labels, rotation=45)\n",
    "        plt.yticks(range(n), labels)\n",
    "        \n",
    "        # Add annotations\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                text = ax.text(j, i, f\"{mae_grid[i, j]:.2f}\",\n",
    "                            ha=\"center\", va=\"center\", color=\"w\" if mae_grid[i, j] > (mae_grid.max() / 2) else \"black\")\n",
    "        \n",
    "        plt.colorbar(cax)\n",
    "        plt.title('MAE Between DataFrames on \"prediction\" Column', pad=20)\n",
    "        plt.show()\n",
    "\n",
    "    def makePredictionWithModelAndPreprocessor(A_model, B_model, C_model, preprocessor, filename):\n",
    "        \"\"\"\n",
    "        Assumes same preprocessing for all locations\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test = preprocessor.transform(A_X_test)\n",
    "        A_y_pred = A_model.predict(A_X_test)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test = preprocessor.transform(B_X_test)\n",
    "        B_y_pred = B_model.predict(B_X_test)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test = preprocessor.transform(C_X_test)\n",
    "        C_y_pred = C_model.predict(C_X_test)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "    def make_average_prediction(preds_dict,filename):\n",
    "        \"\"\"\n",
    "        Generates a prediction by taking the average of the predictions in preds_dict.\n",
    "        \"\"\"\n",
    "        lenght = len(preds_dict)\n",
    "        data = 0\n",
    "        for value in preds_dict.values():\n",
    "            data += value[\"prediction\"]\n",
    "        data = data / lenght\n",
    "        data = pd.DataFrame(data, columns=['prediction'])\n",
    "        data.index.name = \"id\"\n",
    "        data[\"prediction\"] = data['prediction'].apply(lambda x: 0 if x < 0.1 else x)\n",
    "        data.loc[(data.index % 24).isin([22, 23, 0]), \"prediction\"] = 0\n",
    "        data.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining The DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#impoting lightning:\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "#import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as DataSet\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "SEED = 69\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class SolarForecastingDataset(DataSet):\n",
    "    def __init__(self, features_df, target_series):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with features and target labels.\n",
    "\n",
    "        :param features_df: DataFrame containing the features.\n",
    "        :param target_series: Series containing the target labels.\n",
    "        \"\"\"\n",
    "        self.features = features_df\n",
    "        self.targets = target_series\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extracting the features and the target label for the given index\n",
    "        feature_vector = self.features.iloc[index].values\n",
    "        target_label = self.targets.iloc[index]\n",
    "\n",
    "        return {\n",
    "            \"feature_vector\": torch.tensor(feature_vector, dtype=torch.float),\n",
    "            \"target_label\": torch.tensor(target_label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class SolarForecastingDatasetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_features_df, train_targets_series, test_features_df, test_targets_series, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_features_df = train_features_df\n",
    "        self.train_targets_series = train_targets_series\n",
    "        self.test_features_df = test_features_df\n",
    "        self.test_targets_series = test_targets_series\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SolarForecastingDataset(self.train_features_df, self.train_targets_series)\n",
    "        self.test_dataset = SolarForecastingDataset(self.test_features_df, self.test_targets_series)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,worker_init_fn=seed_worker, generator=g, shuffle=True,)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "class FullyConnectedDNN(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, dropout_prob=0.1):\n",
    "        super(FullyConnectedDNN, self).__init__()\n",
    "        # Create fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)):\n",
    "            in_features = input_size if i == 0 else layer_sizes[i - 1]\n",
    "            out_features = layer_sizes[i]\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "        self.output_layer = nn.Linear(layer_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def weighted_mae_loss(input, target, exponent=1, constant=1):\n",
    "    assert input.size() == target.size()\n",
    "\n",
    "    # Calculate the absolute error\n",
    "    absolute_errors = torch.abs(input - target)\n",
    "\n",
    "    # Apply exponential scaling with a constant\n",
    "    adjusted_target = target + constant\n",
    "    weighted_errors = absolute_errors * (adjusted_target ** exponent)\n",
    "\n",
    "    return weighted_errors.mean()\n",
    "\n",
    "\n",
    "class SolarPowerProductionPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, output_size, weight_decay=1e-5, dropout_prob=0.1, learning_rate=0.01, verbose=True, loss_exponent=1.0, loss_beta=1.0):\n",
    "        super().__init__()\n",
    "        self.model = FullyConnectedDNN(input_size, layer_sizes, output_size, dropout_prob=dropout_prob)\n",
    "        self.criterion = self.criterion = lambda input, target: weighted_mae_loss(input, target, exponent=loss_exponent, constant=1)\n",
    "        #self.criterion = CustomMAELoss(loss_alpha, loss_beta)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"train_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"val_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"test_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        # Save the best model path to the pl_module\n",
    "        pl_module.best_model_path = self.best_model_path\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "            predictions = model(features)[1]  \n",
    "            if not isinstance(predictions, torch.Tensor):\n",
    "                raise TypeError(\"Model output is not a tensor. Got type: {}\".format(type(predictions)))\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Check for tensor types before concatenation\n",
    "    if not all(isinstance(p, torch.Tensor) for p in all_predictions):\n",
    "        raise TypeError(\"Not all elements in predictions are tensors.\")\n",
    "\n",
    "    if not all(isinstance(l, torch.Tensor) for l in all_labels):\n",
    "        raise TypeError(\"Not all elements in labels are tensors.\")\n",
    "\n",
    "    all_predictions_tensor = torch.cat(all_predictions, dim=0)\n",
    "    all_labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    all_predictions_np = all_predictions_tensor.cpu().numpy()\n",
    "    all_labels_np = all_labels_tensor.cpu().numpy()\n",
    "    \n",
    "    return all_predictions_np, all_labels_np\n",
    "\n",
    "class HenrikDNN:\n",
    "\n",
    "    def __init__(self,n_features = None, layer_sizes = [100,50], output_size = 1, drop_out_prob = 0.1, learning_rate = 0.01, weight_decay = 1e-5, max_epochs = 100, paitience = 5, batch_size = 16, val_chack_interval = 1, pruning_callback = None, verbose = True, loss_expontent = 1):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.verbose = verbose\n",
    "        self.output_size = output_size\n",
    "        self.drop_out_prob = drop_out_prob\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_expontent = loss_expontent\n",
    "        self.paitience = paitience\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.val_chack_interval = val_chack_interval\n",
    "        self.pruning_callback = pruning_callback\n",
    "        SEED = 69\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "        self.pl_model = SolarPowerProductionPredictor(self.n_features, self.layer_sizes, self.output_size, weight_decay=self.weight_decay, dropout_prob=self.drop_out_prob, learning_rate=self.learning_rate, verbose=self.verbose, loss_exponent=self.loss_expontent)\n",
    "\n",
    "        self.checkpoint_callback = CustomModelCheckpoint(\n",
    "            dirpath='HenrikDNN_checkpoints',\n",
    "            save_top_k=1,\n",
    "            verbose=self.verbose,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            filename='model-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "\n",
    "        self.early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.paitience\n",
    "        )\n",
    "\n",
    "\n",
    "        self.callbacks = [self.early_stopping_callback, self.checkpoint_callback]\n",
    "        if self.pruning_callback is not None:\n",
    "            self.callbacks.append(self.pruning_callback)\n",
    "        seed_everything(69, workers=True)\n",
    "\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=self.callbacks,\n",
    "            enable_progress_bar=self.verbose,\n",
    "            accelerator=\"cpu\",\n",
    "            check_val_every_n_epoch = 2,\n",
    "            deterministic=True\n",
    "\n",
    "            #val_check_interval=self.val_chack_interval\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train: Training df with datetime index. \n",
    "            y_train: Training df, with datetime index. Each row in y_train corresponds to four rows in X_train.\n",
    "    \n",
    "        \"\"\"\n",
    "        #print the seed:\n",
    "        SEED = 69\n",
    "        print(\"hei\")\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "        self.data_module = SolarForecastingDatasetDataModule(X_train, y_train, X_val, y_val, batch_size=self.batch_size)\n",
    "        self.trainer.fit(self.pl_model, self.data_module)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        trained_model = SolarPowerProductionPredictor.load_from_checkpoint(\n",
    "            self.pl_model.best_model_path,\n",
    "            input_size=self.n_features,\n",
    "            layer_sizes=self.layer_sizes,\n",
    "            output_size=self.output_size,\n",
    "            dropout_prob=self.drop_out_prob,\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            verbose=self.verbose,\n",
    "            loss_exponent=self.loss_expontent\n",
    "        )\n",
    "\n",
    "        X_dataloader = torch.utils.data.DataLoader(\n",
    "            SolarForecastingDataset(X, pd.Series(np.zeros(X.shape[0]))),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g\n",
    "        )\n",
    "\n",
    "        predictions, _ = get_predictions(trained_model, X_dataloader)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_on_specific_day_May_june(X, y, split_date):\n",
    "    \"\"\"\n",
    "    Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "    from training set to test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Quarter-hourly input data with DateTime index.\n",
    "    - y: Hourly target data with DateTime index.\n",
    "    - split_date: Date (string or datetime object) to split the data on.\n",
    "    \n",
    "    Returns:\n",
    "    X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    split_date = pd.Timestamp(split_date).normalize()\n",
    "\n",
    "    # Ensure split_date is a datetime object\n",
    "    if isinstance(split_date, str):\n",
    "        split_date = pd.Timestamp(split_date)\n",
    "\n",
    "    print(f\"Split date: {split_date}\")\n",
    "\n",
    "    # Split the data based on the provided date\n",
    "    X_train = X[X.index.normalize() < split_date]\n",
    "    y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "    X_test = X[X.index.normalize() >= split_date]\n",
    "    y_test = y[y.index.normalize() >= split_date]\n",
    "\n",
    "    # Define conditions to move May and June of split_date's year from train to test\n",
    "    may_june_condition_X = ((X_train.index.month == 5) | (X_train.index.month == 6) | (X_train.index.month == 7)) & (X_train.index.year == split_date.year)\n",
    "    may_june_condition_y = ((y_train.index.month == 5) | (y_train.index.month == 6) | (y_train.index.month == 7)) & (y_train.index.year == split_date.year)\n",
    "    \n",
    "    X_may_june = X_train[may_june_condition_X]\n",
    "    y_may_june = y_train[may_june_condition_y]\n",
    "\n",
    "    # Remove May and June data from training set\n",
    "    X_train = X_train[~may_june_condition_X]\n",
    "    y_train = y_train[~may_june_condition_y]\n",
    "\n",
    "    # Append May and June data to test set\n",
    "    X_test = pd.concat([X_may_june, X_test])\n",
    "    y_test = pd.concat([y_may_june, y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training A, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 69\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d0558da93e4c33a986e3ed1ecb06d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e5f5b84a740e1aad60879c22ceff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3b4408933842fca602a639691877a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 498: 'val_loss' reached 0.08600 (best 0.08600), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.09.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa4d6b219044b378c70a9c9887ddc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 996: 'val_loss' reached 0.07759 (best 0.07759), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=03-val_loss=0.08-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d0ef66073d4642b27d47e96e0145f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 1494: 'val_loss' reached 0.07565 (best 0.07565), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=05-val_loss=0.08.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a878d2b93f2b4614a3eb725ce114447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 1992: 'val_loss' was not in top 1\n",
      "Exception ignored in: <function tqdm.__del__ at 0x29fe95bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    def __del__(self):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bf11dd4c5e41f684611b43943ade59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 2490: 'val_loss' reached 0.07462 (best 0.07462), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=09-val_loss=0.07.ckpt' as top 1\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location A: 304.6198138684842\n",
      "new best score for A: 304.6198138684842\n",
      "training B, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 69\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31db040676b84a0997a1847460a6189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc16c495a534636b3aa566d7dfb3e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6200a7731a04648ad258a6545d81b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 386: 'val_loss' reached 0.12922 (best 0.12922), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.13-v1.ckpt' as top 1\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location B: 94.49376177059104\n",
      "new best score for B: 94.49376177059104\n",
      "training C, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 69\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caf90407ac641c997e0b5052bb1c2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ddcd625ec34509abab07e94059c5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5039cdc5d5542dba22f28b26f0e0f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 304: 'val_loss' reached 0.03944 (best 0.03944), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.04-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb993b1dfb894527b158191c33a5e72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 608: 'val_loss' reached 0.03843 (best 0.03843), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=03-val_loss=0.04.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ea53ee7644440ab7ee3f628dadb6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 912: 'val_loss' reached 0.03737 (best 0.03737), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=05-val_loss=0.04-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319588debd8340d7bd9e2ba642852b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location C: 55.733790880227346\n",
      "new best score for C: 55.733790880227346\n"
     ]
    }
   ],
   "source": [
    "def trainDNN(letter):\n",
    "    X, y = pre.general_read(letter)\n",
    "    X = pre.concatenate_dfs(X)\n",
    "    X_train, y_train,X_val, y_val = pre.train_test_split_may_june_july(X,y , letter)\n",
    "    y_train = y_train[\"target\"]\n",
    "    y_val = y_val[\"target\"]\n",
    "\n",
    "    if letter == \"A\":\n",
    "\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [119,101], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.0000969995972939842, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.9702228408589507, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 2.499126185711371e-8 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_preprocessor = 'quarters' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_encoder = True #<----- from opta hyper parameter tuning\n",
    "    \n",
    "    elif letter == \"B\":\n",
    "        \n",
    "        dnn_params = {\n",
    "            'layer_sizes': [200,180], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000018846485346070986, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.963895316469423, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 6.586949775384596e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "        \n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'statistical'\n",
    "        dnn_target_encoder = False\n",
    "    \n",
    "    elif letter == \"C\":\n",
    "        #52.31801357204807\tquarters\ttrue\t144\t131\t0.000060363753946263044\t-1.5104000124283146\t2.335809622586189e-7\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [144,131], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000060363753946263044, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': -1.5104000124283146, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 2.335809622586189e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'quarters'\n",
    "        dnn_target_encoder = True\n",
    "\n",
    "    dnn_feature_scaler = pre.choose_scaler(dnn_feature_scaler)\n",
    "    dnn_target_scaler = pre.choose_scaler(dnn_target_scaler)\n",
    "    dnn_preprocessor = pre.choose_transformer(dnn_preprocessor)\n",
    "    dnn_target_encoder = pre.choose_encoder(dnn_target_encoder)\n",
    "    \n",
    "    dnn_preprocessing = Pipeline([\n",
    "        ('custom_transformer', dnn_preprocessor),\n",
    "        ('target_encoder', dnn_target_encoder), \n",
    "        ('feature_scaler', dnn_feature_scaler)\n",
    "    ])\n",
    "\n",
    "    dnn_target_preprocessing = Pipeline([\n",
    "        ('target_scaler', dnn_target_scaler)\n",
    "    ])\n",
    "\n",
    "    #fit the preprocessing:\n",
    "    dnn_preprocessing.fit(X_train, y_train)\n",
    "    dnn_target_preprocessing.fit(pd.DataFrame(y_train))\n",
    "\n",
    "    #transform the data:\n",
    "    X_train_dnn = pd.DataFrame((dnn_preprocessing.transform(X_train)))\n",
    "    y_train_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_train)))\n",
    "    X_val_dnn = pd.DataFrame(dnn_preprocessing.transform(X_val))\n",
    "    y_val_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_val)))\n",
    "\n",
    "    #fit the model:\n",
    "    dnn_model = HenrikDNN(n_features =X_train_dnn.shape[1] , **dnn_params)\n",
    "    dnn_model.train(X_train_dnn, y_train_dnn, X_val_dnn, y_val_dnn)\n",
    "    #predict:\n",
    "    dnn_pred = dnn_model.predict(X_val_dnn)\n",
    "    #scale back:\n",
    "    dnn_pred = dnn_target_preprocessing.inverse_transform(dnn_pred).reshape(-1)\n",
    "\n",
    "    print(f\"MAE DNN location {letter}: {mean_absolute_error(y_val, dnn_pred)}\")\n",
    "\n",
    "    return dnn_model, dnn_preprocessing, dnn_target_preprocessing, mean_absolute_error(y_val, dnn_pred)\n",
    "\n",
    "\n",
    "models = {\"A\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"B\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"C\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_iterations = 7\n",
    "\n",
    "\n",
    "for letter in [\"A\", \"B\", \"C\"]:\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"training {letter}, iteration {i+1} of {num_iterations}\")\n",
    "        model, preprocessor, target_preprocessor, score = trainDNN(letter)\n",
    "        if score < models[letter][\"best_score\"]:\n",
    "            print(f\"new best score for {letter}: {score}\")\n",
    "            models[letter][\"best_score\"] = score\n",
    "            models[letter][\"best_model\"] = model\n",
    "            models[letter][\"best_preprocessor\"] = preprocessor\n",
    "            models[letter][\"best_target_preprocessor\"] = target_preprocessor\n",
    "\n",
    "post.make_dnn_prediction(models[\"A\"][\"best_model\"],\n",
    "                         models[\"A\"][\"best_preprocessor\"],\n",
    "                         models[\"A\"][\"best_target_preprocessor\"],\n",
    "                         models[\"B\"][\"best_model\"],\n",
    "                         models[\"B\"][\"best_preprocessor\"],\n",
    "                         models[\"B\"][\"best_target_preprocessor\"],\n",
    "                         models[\"C\"][\"best_model\"],\n",
    "                         models[\"C\"][\"best_preprocessor\"],\n",
    "                         models[\"C\"][\"best_target_preprocessor\"],\n",
    "                         f\"{FOLDER_NAME}/DNN.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask's AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-12 16:18:36] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 16:18:36] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 16:18:36] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 16:18:36] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 16:18:36] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2344} INFO - Estimated sufficient time budget=2836s. Estimated necessary time budget=24s.\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2391} INFO -  at 1.4s,\testimator lgbm's best error=721.2925,\tbest estimator lgbm's best error=721.2925\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2391} INFO -  at 1.6s,\testimator lgbm's best error=647.8535,\tbest estimator lgbm's best error=647.8535\n",
      "[flaml.automl.logger: 11-12 16:18:36] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2391} INFO -  at 1.8s,\testimator lgbm's best error=647.8535,\tbest estimator lgbm's best error=647.8535\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2391} INFO -  at 2.1s,\testimator lgbm's best error=635.6789,\tbest estimator lgbm's best error=635.6789\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2218} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2391} INFO -  at 2.3s,\testimator xgboost's best error=863.6133,\tbest estimator lgbm's best error=635.6789\n",
      "[flaml.automl.logger: 11-12 16:18:37] {2218} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:38] {2391} INFO -  at 2.8s,\testimator lgbm's best error=472.8438,\tbest estimator lgbm's best error=472.8438\n",
      "[flaml.automl.logger: 11-12 16:18:38] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:38] {2391} INFO -  at 3.3s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:38] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:39] {2391} INFO -  at 3.8s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:39] {2218} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:39] {2391} INFO -  at 4.4s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:39] {2218} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2391} INFO -  at 4.7s,\testimator xgboost's best error=773.8896,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2218} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2391} INFO -  at 5.0s,\testimator extra_tree's best error=421.1442,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2218} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2391} INFO -  at 5.5s,\testimator extra_tree's best error=408.7660,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:40] {2218} INFO - iteration 12, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:18:41] {2391} INFO -  at 6.6s,\testimator rf's best error=433.5866,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:41] {2218} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:18:44] {2391} INFO -  at 9.3s,\testimator rf's best error=433.5866,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 16:18:44] {2218} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2391} INFO -  at 9.8s,\testimator lgbm's best error=346.0072,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2218} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2391} INFO -  at 10.0s,\testimator xgboost's best error=773.8896,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2218} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2391} INFO -  at 10.5s,\testimator lgbm's best error=335.2427,\tbest estimator lgbm's best error=335.2427\n",
      "[flaml.automl.logger: 11-12 16:18:45] {2218} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:18:46] {2391} INFO -  at 10.8s,\testimator xgboost's best error=773.8896,\tbest estimator lgbm's best error=335.2427\n",
      "[flaml.automl.logger: 11-12 16:18:46] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:46] {2391} INFO -  at 11.3s,\testimator lgbm's best error=335.2427,\tbest estimator lgbm's best error=335.2427\n",
      "[flaml.automl.logger: 11-12 16:18:46] {2218} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:47] {2391} INFO -  at 11.8s,\testimator lgbm's best error=334.5276,\tbest estimator lgbm's best error=334.5276\n",
      "[flaml.automl.logger: 11-12 16:18:47] {2218} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:48] {2391} INFO -  at 12.9s,\testimator lgbm's best error=321.6975,\tbest estimator lgbm's best error=321.6975\n",
      "[flaml.automl.logger: 11-12 16:18:48] {2218} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:49] {2391} INFO -  at 14.3s,\testimator lgbm's best error=318.6070,\tbest estimator lgbm's best error=318.6070\n",
      "[flaml.automl.logger: 11-12 16:18:49] {2218} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:50] {2391} INFO -  at 15.3s,\testimator lgbm's best error=318.6070,\tbest estimator lgbm's best error=318.6070\n",
      "[flaml.automl.logger: 11-12 16:18:50] {2218} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:18:53] {2391} INFO -  at 18.3s,\testimator lgbm's best error=313.1798,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:53] {2218} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:18:56] {2391} INFO -  at 20.8s,\testimator xgboost's best error=360.3246,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:56] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:56] {2391} INFO -  at 21.1s,\testimator extra_tree's best error=384.1186,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:56] {2218} INFO - iteration 26, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:18:57] {2391} INFO -  at 22.5s,\testimator rf's best error=376.4654,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:57] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:58] {2391} INFO -  at 22.8s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:58] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:58] {2391} INFO -  at 23.2s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:58] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:18:59] {2391} INFO -  at 23.7s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:18:59] {2218} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:00] {2391} INFO -  at 25.3s,\testimator lgbm's best error=313.1798,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:19:00] {2218} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2391} INFO -  at 29.6s,\testimator lgbm's best error=313.1798,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2218} INFO - iteration 32, current learner catboost\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2391} INFO -  at 29.9s,\testimator catboost's best error=534.4084,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2218} INFO - iteration 33, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2391} INFO -  at 30.3s,\testimator xgb_limitdepth's best error=945.4703,\tbest estimator lgbm's best error=313.1798\n",
      "[flaml.automl.logger: 11-12 16:19:05] {2493} INFO - selected model: LGBMRegressor(colsample_bytree=0.535650204044578,\n",
      "              learning_rate=0.07655185546743029, max_bin=255,\n",
      "              min_child_samples=34, n_estimators=1, n_jobs=-1, num_leaves=23,\n",
      "              reg_alpha=0.23233127114562543, reg_lambda=0.19099356212910557,\n",
      "              verbose=-1)\n",
      "[flaml.automl.logger: 11-12 16:19:05] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 16:19:05] {1931} INFO - Time taken to find the best model: 18.319865226745605\n",
      "[flaml.automl.logger: 11-12 16:19:20] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 16:19:20] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 16:19:20] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 16:19:20] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 16:19:20] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 16:19:20] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:20] {2344} INFO - Estimated sufficient time budget=2965s. Estimated necessary time budget=25s.\n",
      "[flaml.automl.logger: 11-12 16:19:20] {2391} INFO -  at 1.2s,\testimator lgbm's best error=173.3460,\tbest estimator lgbm's best error=173.3460\n",
      "[flaml.automl.logger: 11-12 16:19:20] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2391} INFO -  at 1.4s,\testimator lgbm's best error=155.8314,\tbest estimator lgbm's best error=155.8314\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2391} INFO -  at 1.6s,\testimator lgbm's best error=155.8314,\tbest estimator lgbm's best error=155.8314\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2391} INFO -  at 1.8s,\testimator lgbm's best error=152.3728,\tbest estimator lgbm's best error=152.3728\n",
      "[flaml.automl.logger: 11-12 16:19:21] {2218} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:22] {2391} INFO -  at 2.7s,\testimator lgbm's best error=111.8476,\tbest estimator lgbm's best error=111.8476\n",
      "[flaml.automl.logger: 11-12 16:19:22] {2218} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:22] {2391} INFO -  at 3.0s,\testimator xgboost's best error=215.4396,\tbest estimator lgbm's best error=111.8476\n",
      "[flaml.automl.logger: 11-12 16:19:22] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2391} INFO -  at 3.4s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2391} INFO -  at 3.7s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2218} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2391} INFO -  at 4.0s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 16:19:23] {2218} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:24] {2391} INFO -  at 4.5s,\testimator lgbm's best error=72.5610,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:24] {2218} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:24] {2391} INFO -  at 4.9s,\testimator xgboost's best error=192.3452,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:24] {2218} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:25] {2391} INFO -  at 5.2s,\testimator extra_tree's best error=83.8868,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:25] {2218} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:25] {2391} INFO -  at 5.6s,\testimator extra_tree's best error=83.3841,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:25] {2218} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:19:26] {2391} INFO -  at 6.4s,\testimator rf's best error=89.5367,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:26] {2218} INFO - iteration 14, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:19:27] {2391} INFO -  at 8.1s,\testimator rf's best error=88.7012,\tbest estimator lgbm's best error=72.5610\n",
      "[flaml.automl.logger: 11-12 16:19:27] {2218} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:28] {2391} INFO -  at 8.6s,\testimator lgbm's best error=71.7041,\tbest estimator lgbm's best error=71.7041\n",
      "[flaml.automl.logger: 11-12 16:19:28] {2218} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:28] {2391} INFO -  at 9.2s,\testimator lgbm's best error=71.7041,\tbest estimator lgbm's best error=71.7041\n",
      "[flaml.automl.logger: 11-12 16:19:28] {2218} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:29] {2391} INFO -  at 9.3s,\testimator xgboost's best error=192.3452,\tbest estimator lgbm's best error=71.7041\n",
      "[flaml.automl.logger: 11-12 16:19:29] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:29] {2391} INFO -  at 9.9s,\testimator lgbm's best error=71.7041,\tbest estimator lgbm's best error=71.7041\n",
      "[flaml.automl.logger: 11-12 16:19:29] {2218} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:30] {2391} INFO -  at 11.0s,\testimator lgbm's best error=70.8024,\tbest estimator lgbm's best error=70.8024\n",
      "[flaml.automl.logger: 11-12 16:19:30] {2218} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:32] {2391} INFO -  at 12.8s,\testimator lgbm's best error=70.8024,\tbest estimator lgbm's best error=70.8024\n",
      "[flaml.automl.logger: 11-12 16:19:32] {2218} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:33] {2391} INFO -  at 13.4s,\testimator lgbm's best error=70.8024,\tbest estimator lgbm's best error=70.8024\n",
      "[flaml.automl.logger: 11-12 16:19:33] {2218} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:34] {2391} INFO -  at 14.5s,\testimator lgbm's best error=70.6347,\tbest estimator lgbm's best error=70.6347\n",
      "[flaml.automl.logger: 11-12 16:19:34] {2218} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2391} INFO -  at 15.3s,\testimator lgbm's best error=70.6347,\tbest estimator lgbm's best error=70.6347\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2218} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2391} INFO -  at 15.5s,\testimator xgboost's best error=192.3452,\tbest estimator lgbm's best error=70.6347\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2391} INFO -  at 15.7s,\testimator extra_tree's best error=74.1033,\tbest estimator lgbm's best error=70.6347\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2218} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2391} INFO -  at 16.0s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:35] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2391} INFO -  at 16.3s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2391} INFO -  at 16.6s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2391} INFO -  at 17.1s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:36] {2218} INFO - iteration 30, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:37] {2391} INFO -  at 18.2s,\testimator xgboost's best error=87.8386,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:37] {2218} INFO - iteration 31, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2391} INFO -  at 19.4s,\testimator xgboost's best error=87.8386,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2218} INFO - iteration 32, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2391} INFO -  at 19.6s,\testimator xgboost's best error=81.3006,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2218} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2391} INFO -  at 19.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2218} INFO - iteration 34, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2391} INFO -  at 20.0s,\testimator xgboost's best error=81.3006,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:39] {2218} INFO - iteration 35, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2391} INFO -  at 20.4s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2218} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2391} INFO -  at 20.6s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2218} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2391} INFO -  at 21.1s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:40] {2218} INFO - iteration 38, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:41] {2391} INFO -  at 21.5s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:41] {2218} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:42] {2391} INFO -  at 22.6s,\testimator xgboost's best error=75.3048,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:42] {2218} INFO - iteration 40, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:42] {2391} INFO -  at 22.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:42] {2218} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:43] {2391} INFO -  at 23.3s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:43] {2218} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:43] {2391} INFO -  at 24.0s,\testimator lgbm's best error=70.2339,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:43] {2218} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:44] {2391} INFO -  at 24.5s,\testimator xgboost's best error=70.7825,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:44] {2218} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:44] {2391} INFO -  at 24.8s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:44] {2218} INFO - iteration 45, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:19:46] {2391} INFO -  at 26.9s,\testimator xgboost's best error=70.7825,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:46] {2218} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:48] {2391} INFO -  at 28.6s,\testimator lgbm's best error=70.2339,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:48] {2218} INFO - iteration 47, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:19:48] {2391} INFO -  at 28.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:48] {2218} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2391} INFO -  at 29.9s,\testimator lgbm's best error=70.2339,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2218} INFO - iteration 49, current learner catboost\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2391} INFO -  at 30.0s,\testimator catboost's best error=215.5810,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2218} INFO - iteration 50, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2391} INFO -  at 30.1s,\testimator xgb_limitdepth's best error=227.6787,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 16:19:49] {2493} INFO - selected model: ExtraTreesRegressor(max_features=0.8821150021741903, max_leaf_nodes=39,\n",
      "                    n_estimators=4, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 11-12 16:19:49] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 16:19:49] {1931} INFO - Time taken to find the best model: 15.95606279373169\n",
      "[flaml.automl.logger: 11-12 16:20:01] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 16:20:01] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 16:20:01] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 16:20:01] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 16:20:01] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 16:20:01] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:01] {2344} INFO - Estimated sufficient time budget=2214s. Estimated necessary time budget=19s.\n",
      "[flaml.automl.logger: 11-12 16:20:01] {2391} INFO -  at 0.9s,\testimator lgbm's best error=151.2353,\tbest estimator lgbm's best error=151.2353\n",
      "[flaml.automl.logger: 11-12 16:20:01] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2391} INFO -  at 1.1s,\testimator lgbm's best error=135.2081,\tbest estimator lgbm's best error=135.2081\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2391} INFO -  at 1.3s,\testimator lgbm's best error=135.2081,\tbest estimator lgbm's best error=135.2081\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2391} INFO -  at 1.5s,\testimator lgbm's best error=133.7742,\tbest estimator lgbm's best error=133.7742\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2218} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2391} INFO -  at 1.7s,\testimator xgboost's best error=191.7063,\tbest estimator lgbm's best error=133.7742\n",
      "[flaml.automl.logger: 11-12 16:20:02] {2218} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2391} INFO -  at 2.2s,\testimator lgbm's best error=94.6564,\tbest estimator lgbm's best error=94.6564\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2391} INFO -  at 2.5s,\testimator lgbm's best error=66.7640,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2391} INFO -  at 2.8s,\testimator lgbm's best error=66.7640,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:03] {2218} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2391} INFO -  at 3.2s,\testimator lgbm's best error=66.7640,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2218} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2391} INFO -  at 3.5s,\testimator xgboost's best error=180.3067,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2218} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2391} INFO -  at 3.6s,\testimator extra_tree's best error=77.6340,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2218} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2391} INFO -  at 3.9s,\testimator extra_tree's best error=77.6340,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:04] {2218} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2391} INFO -  at 4.1s,\testimator extra_tree's best error=66.8520,\tbest estimator lgbm's best error=66.7640\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2218} INFO - iteration 13, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2391} INFO -  at 4.3s,\testimator extra_tree's best error=60.0754,\tbest estimator extra_tree's best error=60.0754\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2218} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2391} INFO -  at 4.6s,\testimator extra_tree's best error=60.0754,\tbest estimator extra_tree's best error=60.0754\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2218} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2391} INFO -  at 4.9s,\testimator lgbm's best error=58.8968,\tbest estimator lgbm's best error=58.8968\n",
      "[flaml.automl.logger: 11-12 16:20:05] {2218} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:06] {2391} INFO -  at 5.1s,\testimator extra_tree's best error=60.0754,\tbest estimator lgbm's best error=58.8968\n",
      "[flaml.automl.logger: 11-12 16:20:06] {2218} INFO - iteration 17, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:06] {2391} INFO -  at 5.8s,\testimator rf's best error=80.4577,\tbest estimator lgbm's best error=58.8968\n",
      "[flaml.automl.logger: 11-12 16:20:06] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 16:20:07] {2391} INFO -  at 6.3s,\testimator lgbm's best error=58.7889,\tbest estimator lgbm's best error=58.7889\n",
      "[flaml.automl.logger: 11-12 16:20:07] {2218} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:07] {2391} INFO -  at 6.8s,\testimator extra_tree's best error=58.4890,\tbest estimator extra_tree's best error=58.4890\n",
      "[flaml.automl.logger: 11-12 16:20:07] {2218} INFO - iteration 20, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:09] {2391} INFO -  at 8.2s,\testimator rf's best error=80.4577,\tbest estimator extra_tree's best error=58.4890\n",
      "[flaml.automl.logger: 11-12 16:20:09] {2218} INFO - iteration 21, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:09] {2391} INFO -  at 8.5s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 16:20:09] {2218} INFO - iteration 22, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2391} INFO -  at 9.3s,\testimator rf's best error=67.6800,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2218} INFO - iteration 23, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2391} INFO -  at 9.8s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2218} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2391} INFO -  at 10.0s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 16:20:10] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:11] {2391} INFO -  at 10.5s,\testimator extra_tree's best error=57.3735,\tbest estimator extra_tree's best error=57.3735\n",
      "[flaml.automl.logger: 11-12 16:20:11] {2218} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 16:20:11] {2391} INFO -  at 10.8s,\testimator xgboost's best error=180.3067,\tbest estimator extra_tree's best error=57.3735\n",
      "[flaml.automl.logger: 11-12 16:20:11] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:12] {2391} INFO -  at 11.8s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:12] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:13] {2391} INFO -  at 12.3s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:13] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:14] {2391} INFO -  at 13.2s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:14] {2218} INFO - iteration 30, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:15] {2391} INFO -  at 14.1s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:15] {2218} INFO - iteration 31, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:16] {2391} INFO -  at 15.3s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:16] {2218} INFO - iteration 32, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:17] {2391} INFO -  at 16.2s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:17] {2218} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:18] {2391} INFO -  at 17.2s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:18] {2218} INFO - iteration 34, current learner rf\n",
      "[flaml.automl.logger: 11-12 16:20:20] {2391} INFO -  at 19.6s,\testimator rf's best error=58.5857,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:20] {2218} INFO - iteration 35, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:21] {2391} INFO -  at 20.2s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 16:20:21] {2218} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 16:20:22] {2391} INFO -  at 21.1s,\testimator extra_tree's best error=56.8065,\tbest estimator extra_tree's best error=56.8065\n",
      "[flaml.automl.logger: 11-12 16:20:22] {2218} INFO - iteration 37, current learner catboost\n",
      "[flaml.automl.logger: 11-12 16:20:25] {2391} INFO -  at 24.0s,\testimator catboost's best error=57.9966,\tbest estimator extra_tree's best error=56.8065\n",
      "[flaml.automl.logger: 11-12 16:20:25] {2218} INFO - iteration 38, current learner catboost\n",
      "[flaml.automl.logger: 11-12 16:20:30] {2391} INFO -  at 30.0s,\testimator catboost's best error=57.9966,\tbest estimator extra_tree's best error=56.8065\n",
      "[flaml.automl.logger: 11-12 16:20:30] {2218} INFO - iteration 39, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 16:20:31] {2391} INFO -  at 30.1s,\testimator xgb_limitdepth's best error=195.6529,\tbest estimator extra_tree's best error=56.8065\n",
      "[flaml.automl.logger: 11-12 16:20:31] {2493} INFO - selected model: ExtraTreesRegressor(max_features=0.7073789155024577, max_leaf_nodes=251,\n",
      "                    n_estimators=35, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 11-12 16:20:31] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 16:20:31] {1931} INFO - Time taken to find the best model: 21.148711919784546\n",
      "[flaml.automl.logger: 11-12 16:20:31] {1941} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "FLAML_TIME_BUDGET_A = 30*60\n",
    "FLAML_TIME_BUDGET_B_C = 15*60\n",
    "\n",
    "def trainFlamAutoML(letter, preprocessor, time_budget=60):\n",
    "    X, y = pre.general_read_flaml(letter)\n",
    "    X = pre.concatenate_dfs(X)\n",
    "    X_train, y_train,X_test, y_test = pre.train_test_split_may_june_july(X,y,letter)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    y_train = y_train[\"target\"]\n",
    "    y_test = y_test[\"target\"]\n",
    "    automl = AutoML()\n",
    "\n",
    "    automl_settings = {\n",
    "        \"time_budget\": time_budget,  # in seconds\n",
    "        \"metric\": 'mae',\n",
    "        \"task\": 'regression',\n",
    "        \"log_file_name\": f\"flaml_{letter}.log\",\n",
    "        \"seed\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "    automl.fit(X_train=X_train, y_train=y_train, X_val=X_test, y_val=y_test, **automl_settings)\n",
    "    return automl\n",
    "\n",
    "PREPROCESSORS = [\"quarters\"]\n",
    "\n",
    "for preprocessor in PREPROCESSORS:\n",
    "    flaml_preprocessor = pre.choose_transformer(preprocessor)\n",
    "    flaml_A = trainFlamAutoML(\"A\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_A)\n",
    "    flaml_B = trainFlamAutoML(\"B\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_B_C)\n",
    "    flaml_C = trainFlamAutoML(\"C\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_B_C)\n",
    "    \n",
    "post.makePredictionWithModelAndPreprocessor(flaml_A,flaml_B,flaml_C,flaml_preprocessor,f\"{FOLDER_NAME}/flaml_{preprocessor}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11851\n",
      "[LightGBM] [Info] Number of data points in the train set: 31373, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 578.107824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11725\n",
      "[LightGBM] [Info] Number of data points in the train set: 26551, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 83.214852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11924\n",
      "[LightGBM] [Info] Number of data points in the train set: 23564, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score 68.826784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 1ms/step\n",
      "68/68 [==============================] - 0s 801us/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "{'Catboost': 322.33210907842766, 'RandomForest': 322.23620489357415, 'LightGBM': 290.507321431308, 'MLP': 357.50508285208804, 'XGBoost': 347.18163421678827}\n",
      "{'Catboost': 60.75907869661851, 'RandomForest': 60.50625552132473, 'LightGBM': 56.638435643764, 'MLP': 61.8769138054954, 'XGBoost': 64.45669385192582}\n",
      "{'Catboost': 46.47025029979914, 'RandomForest': 48.69249792482573, 'LightGBM': 46.8981926817536, 'MLP': 48.39199246608083, 'XGBoost': 52.492248335391686}\n",
      "{'Catboost': 322.33210907842766, 'RandomForest': 322.23620489357415, 'LightGBM': 290.507321431308, 'MLP': 357.50508285208804, 'XGBoost': 347.18163421678827}\n",
      "Model score: 0.9815839319373033\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Catboost': 60.75907869661851, 'RandomForest': 60.50625552132473, 'LightGBM': 56.638435643764, 'MLP': 61.8769138054954, 'XGBoost': 64.45669385192582}\n",
      "Model score: 0.9833147987143214\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Catboost': 46.47025029979914, 'RandomForest': 48.69249792482573, 'LightGBM': 46.8981926817536, 'MLP': 48.39199246608083, 'XGBoost': 52.492248335391686}\n",
      "Model score: 0.9856222961325023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 2ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "18.785759422222224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import catboost as cb\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def create_model_mlp(num_features, params):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            num_features,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"normal\",\n",
    "            input_shape=(num_features,),\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(16, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "    model.add(layers.Dense(1, kernel_initializer=\"normal\", activation=\"linear\"))\n",
    "\n",
    "    optimizer = SGD(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "    )\n",
    "    model.compile(loss=\"mean_absolute_error\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def blob(letter):\n",
    "\n",
    "\n",
    "\n",
    "    A_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.07524503439954632, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'learning_rate': 0.21457171264708794, 'iterations': 1743, 'l2_leaf_reg': 3.2654906719939096, 'border_count': 125, 'random_strength': 4, 'subsample': 0.9647762909882722},\n",
    "    {'n_estimators': 728, 'max_depth': 32, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'},\n",
    "    {'lambda_l1': 0.046833503396687244, 'lambda_l2': 0.006793841288240755, 'num_leaves': 163, 'feature_fraction': 0.929721178994391, 'bagging_fraction': 0.9044770721877625, 'bagging_freq': 6, 'min_child_samples': 16},\n",
    "    {'learning_rate': 0.0011881356000584273, 'momentum': 0.007427232252863111},\n",
    "    {'lambda': 4.008350742341045, 'alpha': 0.5938349159130611, 'eta': 0.41872596427506, 'gamma': 0.3960437023698051, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.6306741879739037, 'colsample_bytree': 0.8160324662642783, 'colsample_bylevel': 0.6887672828499098, 'colsample_bynode': 0.6657703558661234}]\n",
    "\n",
    "    B_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.0478383929191963, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'learning_rate': 0.2914259300415878, 'iterations': 1482, 'l2_leaf_reg': 0.01173059472637936, 'border_count': 67, 'random_strength': 1},\n",
    "    {'n_estimators': 1812, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'},\n",
    "    {'lambda_l1': 0.3292151184937159, 'lambda_l2': 0.00017039816288479435, 'num_leaves': 121, 'feature_fraction': 0.7992531492630386, 'bagging_fraction': 0.9062024635089169, 'bagging_freq': 1, 'min_child_samples': 12},\n",
    "    {'learning_rate': 0.00040347911788473766, 'momentum': 0.39280565026198133},\n",
    "    {'lambda': 0.020066610743154346, 'alpha': 0.020699686769877438, 'eta': 0.38009204594957435, 'gamma': 0.019310696968323503, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.7467461954995296, 'colsample_bytree': 0.9341895261741568, 'colsample_bylevel': 0.6869510780278182, 'colsample_bynode': 0.3781649042157331}]\n",
    "\n",
    "    C_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.07122595559105349, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'learning_rate': 0.1428232280057889, 'iterations': 1231, 'l2_leaf_reg': 0.03177024578576441, 'border_count': 111, 'random_strength': 7, 'bagging_temperature': 0.5470894828054673},\n",
    "    {'n_estimators': 1188, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'},\n",
    "    {'lambda_l1': 0.010132697944440957, 'lambda_l2': 1.3131072935454758e-08, 'num_leaves': 46, 'feature_fraction': 0.8270943641777372, 'bagging_fraction': 0.6628880080664098, 'bagging_freq': 4, 'min_child_samples': 19},\n",
    "    {'learning_rate': 0.0005264021357158383, 'momentum': 0.005758505842856219},\n",
    "    {'lambda': 4.0646418744914525, 'alpha': 0.01032666426760746, 'eta': 0.5072545314266129, 'gamma': 0.45062371002987023, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.9508152944450243, 'colsample_bytree': 0.5386940634341901, 'colsample_bylevel': 0.5562947611307176, 'colsample_bynode': 0.649251076479528}]\n",
    "\n",
    "    ens_params = {\n",
    "        \"A\": A_params,\n",
    "        \"B\": B_params,\n",
    "        \"C\": C_params\n",
    "    }\n",
    "\n",
    "    df = ensPre.trim(ensPre.readDataSet(letter), [])\n",
    "    df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "    y = df[\"target\"]\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y, holdout_X, holdout_y  = ensPre.new_train_test_split(X, y, letter,True)\n",
    "     \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled=pd.DataFrame(X_scaled,columns=X.columns,index=X.index)\n",
    "    train_x_scaled, valid_x_scaled, train_y_scaled, valid_y_scaled,holdout_X_scaled, holdout_y_scaled  = ensPre.new_train_test_split(X_scaled, y,letter,True)\n",
    "\n",
    "    train_x=pd.concat([train_x,valid_x],axis=0)\n",
    "    train_y=pd.concat([train_y,valid_y],axis=0)\n",
    "\n",
    "    train_x_scaled=pd.concat([train_x_scaled,valid_x_scaled],axis=0)\n",
    "    train_y_scaled=pd.concat([train_y_scaled,valid_y_scaled],axis=0)\n",
    "\n",
    "\n",
    "    Models={}\n",
    "    \n",
    "    study0_model=cb.CatBoostRegressor(**ens_params[letter][0])\n",
    "    study0_model.fit(train_x,train_y,verbose=0)\n",
    "    Models[\"Catboost\"]=study0_model\n",
    "\n",
    "    study1_model=RandomForestRegressor(**ens_params[letter][1])\n",
    "    study1_model.fit(train_x,train_y)\n",
    "    Models[\"RandomForest\"]=study1_model\n",
    "\n",
    "    study2_model=lgb.train(ens_params[letter][2],lgb.Dataset(train_x, label=train_y))\n",
    "    Models[\"LightGBM\"]=study2_model\n",
    "    \n",
    "    study3_model=create_model_mlp(X.shape[1], ens_params[letter][3])\n",
    "    study3_model.fit(train_x_scaled,train_y_scaled,shuffle=True,\n",
    "        batch_size=16,\n",
    "        epochs=100,\n",
    "        verbose=False)\n",
    "    Models[\"MLP\"]=study3_model\n",
    "\n",
    "    study4_model=xgb.train(ens_params[letter][4], xgb.DMatrix(train_x_scaled, label=train_y_scaled), verbose_eval=False)\n",
    "    Models[\"XGBoost\"]=study4_model\n",
    "    Models[\"Scaler\"]=scaler\n",
    "    \n",
    "    return Models\n",
    "\n",
    "def predict_base_models(X,base_models):\n",
    "\n",
    "    predictions = pd.DataFrame()\n",
    "    scaler = base_models[\"Scaler\"]\n",
    "    \n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled=pd.DataFrame(X_scaled,columns=X.columns,index=X.index)\n",
    "\n",
    "    for model_name, model in base_models.items():\n",
    "        # Make predictions with the current model\n",
    "        if model_name==\"MLP\":\n",
    "            pred = model.predict(X_scaled)\n",
    "        elif model_name==\"XGBoost\":\n",
    "            pred = model.predict(xgb.DMatrix(X_scaled))\n",
    "        elif model_name==\"Scaler\":\n",
    "            continue\n",
    "        else:\n",
    "            pred = model.predict(X)\n",
    "        # Add the predictions to the DataFrame\n",
    "        predictions[model_name] = pred\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def prepare_data(letter):\n",
    "    df = ensPre.trim(ensPre.readDataSet(letter), [])\n",
    "    df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "    y = df[\"target\"]\n",
    "    train_x, valid_x, train_y, valid_y, holdout_X, holdout_y  = ensPre.new_train_test_split(X, y, letter,True)\n",
    "    return train_x, valid_x, train_y, valid_y, holdout_X, holdout_y\n",
    "\n",
    "\n",
    "A_base_models=blob(\"A\")\n",
    "B_base_models=blob(\"B\")\n",
    "C_base_models=blob(\"C\")\n",
    "\n",
    "\n",
    "A_train_x, A_valid_x, A_train_y, A_valid_y, A_holdout_X, A_holdout_y=prepare_data(\"A\")\n",
    "B_train_x, B_valid_x, B_train_y, B_valid_y, B_holdout_X, B_holdout_y=prepare_data(\"B\")\n",
    "C_train_x, C_valid_x, C_train_y, C_valid_y, C_holdout_X, C_holdout_y=prepare_data(\"C\")\n",
    "\n",
    "A_predictions_fitonval=(predict_base_models(A_holdout_X,A_base_models))\n",
    "B_predictions_fitonval=(predict_base_models(B_holdout_X,B_base_models))\n",
    "C_predictions_fitonval=(predict_base_models(C_holdout_X,C_base_models))\n",
    "\n",
    "A_predictions_fitonval[\"valdid\"]=A_holdout_y.values\n",
    "B_predictions_fitonval[\"valdid\"]=B_holdout_y.values\n",
    "C_predictions_fitonval[\"valdid\"]=C_holdout_y.values\n",
    " \n",
    "def calculate_mae(predictions_df, valid_column='valdid'):\n",
    "    mae_scores = {}\n",
    "\n",
    "    for model_name in predictions_df.columns:\n",
    "        if model_name != valid_column:\n",
    "            mae = mean_absolute_error(predictions_df[valid_column], predictions_df[model_name])\n",
    "            mae_scores[model_name] = mae\n",
    "\n",
    "    return mae_scores\n",
    "\n",
    "print(calculate_mae(A_predictions_fitonval))\n",
    "print(calculate_mae(B_predictions_fitonval))\n",
    "print(calculate_mae(C_predictions_fitonval))\n",
    "\n",
    "def trainmeta(letter):\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y,holdout_X, holdout_y  = prepare_data(letter)\n",
    "    train_x=pd.concat([train_x,valid_x,holdout_X],axis=0)\n",
    "    train_y=pd.concat([train_y,valid_y,holdout_y],axis=0)\n",
    "\n",
    "    # Replace 'A' with 'letter'\n",
    "    predictions_fitonval = globals()[f'{letter}_predictions_fitonval']\n",
    "    X_meta = predictions_fitonval[[\"Catboost\",\"RandomForest\",\"LightGBM\",\"MLP\",\"XGBoost\"]]\n",
    "    y_meta = holdout_y\n",
    "\n",
    "    mae_values = {}\n",
    "    for column in predictions_fitonval[[\"Catboost\",\"RandomForest\",\"LightGBM\",\"MLP\",\"XGBoost\"]]:\n",
    "    \n",
    "        mae = mean_absolute_error(y_meta, predictions_fitonval[column])\n",
    "        mae_values[column] = mae\n",
    "\n",
    "    print(mae_values)\n",
    "\n",
    "    # Create and train the Linear Regression model\n",
    "    meta_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    meta_model.fit(X_meta, y_meta)\n",
    "\n",
    "    # Score the model\n",
    "    score = meta_model.score(X_meta, y_meta)\n",
    "    print(f\"Model score: {score}\")\n",
    "\n",
    "    # Make predictions and add them to the DataFrame\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "# Get feature importances\n",
    "    importances = meta_model.feature_importances_\n",
    "\n",
    "    # Get the indices of the importances sorted by their values\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'Feature Importances for {letter}')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [X_meta.columns[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    return meta_model\n",
    "\n",
    "\n",
    "meta_A=trainmeta(\"A\")\n",
    "meta_B=trainmeta(\"B\")\n",
    "meta_C=trainmeta(\"C\")\n",
    "\n",
    "\n",
    "\n",
    "A_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"A\")))\n",
    "A_x_test=ensPre.readtest(\"A\")\n",
    "A_x_test=ensPre.preptest(A_x_test,[],A_stats)\n",
    "\n",
    "B_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"B\")))\n",
    "B_x_test=ensPre.readtest(\"B\")\n",
    "B_x_test=ensPre.preptest(B_x_test,[],B_stats)\n",
    "\n",
    "C_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"C\")))\n",
    "C_x_test=ensPre.readtest(\"C\")\n",
    "C_x_test=ensPre.preptest(C_x_test,[],C_stats)\n",
    "\n",
    "\n",
    "A_base_preds=(predict_base_models(A_x_test,A_base_models))\n",
    "B_base_preds=(predict_base_models(B_x_test,B_base_models))\n",
    "C_base_preds=(predict_base_models(C_x_test,C_base_models))\n",
    "\n",
    "A_y_pred=meta_A.predict(A_base_preds)\n",
    "B_y_pred=meta_B.predict(B_base_preds)\n",
    "C_y_pred=meta_C.predict(C_base_preds)\n",
    "\n",
    "A_y_pred=pd.DataFrame(A_y_pred, index=A_x_test.index, columns=['y_pred'])\n",
    "print(0.016*A_y_pred[\"y_pred\"].mean())\n",
    "A_y_pred[A_y_pred[\"y_pred\"]<0.0016*A_y_pred[\"y_pred\"].mean()]=0\n",
    "\n",
    "B_y_pred=pd.DataFrame(B_y_pred, index=B_x_test.index, columns=['y_pred'])\n",
    "B_y_pred[B_y_pred[\"y_pred\"]<0.0016*B_y_pred[\"y_pred\"].mean()]=0\n",
    "C_y_pred=pd.DataFrame(C_y_pred, index=C_x_test.index, columns=['y_pred'])\n",
    "C_y_pred[C_y_pred[\"y_pred\"]<0.0016*C_y_pred[\"y_pred\"].mean()]=0\n",
    "\n",
    "\n",
    "combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "combined_pred[\"y_pred\"] = combined_pred[\"y_pred\"].clip(lower=0)\n",
    "combined_pred.to_csv(\"combined_pred.csv\", index=True)\n",
    "combined_pred=(pd.read_csv(\"combined_pred.csv\"))\n",
    "\n",
    "# Add the \"Id\" column\n",
    "combined_pred.insert(0, \"id\", range(len(combined_pred)))\n",
    "combined_pred = combined_pred.rename(columns={combined_pred.columns[-1]: \"prediction\"})\n",
    "combined_pred[[\"id\", \"prediction\"]].to_csv(f\"{FOLDER_NAME}/Ensemble_prediction.csv\", index=False)\n",
    "os.remove(\"combined_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jallastacking the csvfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(FOLDER_NAME):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(FOLDER_NAME, filename)\n",
    "        dataframe_name = filename.split('.')[0]  # Get the name of the file without the extension\n",
    "        dataframes[dataframe_name] = pd.read_csv(file_path)\n",
    "\n",
    "data = dataframes[\"Ensemble_prediction\"][\"prediction\"]*0.2 + dataframes[\"DNN\"][\"prediction\"]*0.4 + dataframes[\"flaml_quarters\"][\"prediction\"]*0.4\n",
    "data = pd.DataFrame(data, columns=[\"prediction\"])\n",
    "data2 = dataframes[\"DNN\"][\"prediction\"]*0.5 + dataframes[\"flaml_quarters\"][\"prediction\"]*0.5\n",
    "data2 = pd.DataFrame(data2, columns = [\"prediction\"])\n",
    "data.index.name = \"id\"\n",
    "data2.index.name = \"id\"\n",
    "\n",
    "data.to_csv(\"Jallastack_vektet_pc1.csv\")\n",
    "data.to_csv(\"Jallastack_uten_ens_pc1.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
