{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "#General Preprocessing functions:\n",
    "class pre:\n",
    "    def clean_NaN(df):\n",
    "        df = df.copy()\n",
    "        df.dropna(subset=['target'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def remove_long_sequences(df, col_name, seq_len):\n",
    "        df = df.copy()\n",
    "        # Identify sequences of zeros\n",
    "        df['group'] = (df[col_name] != 0).cumsum()\n",
    "        df['group_count'] = df.groupby('group')[col_name].transform('count')\n",
    "        \n",
    "        # Create a mask to identify rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        mask = (df[col_name] == 0) & (df['group_count'] > seq_len) #& (df['is_in_shadow:idx'] < 1)\n",
    "        \n",
    "        # Remove rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        df_cleaned = df[~mask].drop(columns=['group', 'group_count'])\n",
    "        return df_cleaned.copy()\n",
    "\n",
    "\n",
    "    def remove_repeating_nonzero(df, col_name, repeat_count=5):\n",
    "        df = df.copy()\n",
    "        # create a mask to identify rows with repeating nonzero values in the target column\n",
    "        mask = ((df[col_name] != 0) & (df[col_name].shift(1) == df[col_name]))\n",
    "        # create a mask to identify rows with repeating nonzero values that occur more than repeat_count times\n",
    "        repeat_mask = mask & (mask.groupby((~mask).cumsum()).cumcount() >= repeat_count)\n",
    "        # create a mask to identify the complete sequence of repeating nonzero values\n",
    "        seq_mask = repeat_mask | repeat_mask.shift(-5)\n",
    "        # remove rows with repeating nonzero values that occur more than repeat_count times\n",
    "        df = df[~seq_mask]\n",
    "        return df\n",
    "\n",
    "    def clean(df):\n",
    "        df = df.copy()\n",
    "        df=pre.clean_NaN(df)\n",
    "        df=pre.remove_long_sequences(df, 'target', 60)\n",
    "        df=pre.remove_repeating_nonzero(df, 'target')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def encode(data, col, max_val):\n",
    "        data = data.copy()\n",
    "        data = data.copy()\n",
    "        data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "        data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "        return data\n",
    "\n",
    "    def create_time_features(df):\n",
    "        df = df.copy()\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        df[\"dayofyear\"]=df.index.dayofyear\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"week\"] = df.index.isocalendar().week\n",
    "\n",
    "        #zero indexing:\n",
    "        df[\"dayofyear\"]-=1\n",
    "        df[\"month\"]-=1\n",
    "        df[\"week\"]-=1\n",
    "\n",
    "\n",
    "        #Cycling the time features:\n",
    "        df = pre.encode(df, \"hour\", 24)\n",
    "        df = pre.encode(df, \"month\", 12)\n",
    "        df = pre.encode(df, \"week\", 53)\n",
    "        df = pre.encode(df, \"dayofyear\", 366)\n",
    "\n",
    "        df.drop(columns=[\"hour\", \"month\", \"week\", \"dayofyear\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df[\"mult1\"]=(1-df[\"is_in_shadow:idx\"])*df['direct_rad:W']\n",
    "        df[\"mult2\"]=(1-df[\"is_in_shadow:idx\"])*df['clear_sky_rad:W']\n",
    "        df[\"date_calc\"]=pd.to_datetime(df[\"date_calc\"])\n",
    "        df.index=pd.to_datetime(df.index)\n",
    "        df[\"uncertainty\"]=(df.index-df[\"date_calc\"]).apply(lambda x: x.total_seconds()/3600)\n",
    "        df[\"uncertainty\"].fillna(0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def create_features(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        df.dropna(subset=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "        df[\"total_solar_rad\"]=df[\"direct_rad:W\"]+df[\"diffuse_rad:W\"]\n",
    "        #df[\"clear_sky_%\"]=df[\"total_solar_rad\"]/df[\"clear_sky_rad:W\"]*100\n",
    "        #df[\"clear_sky_%\"].fillna(0, inplace=True)\n",
    "        df[\"spec humid\"]=df[\"absolute_humidity_2m:gm3\"]/df[\"air_density_2m:kgm3\"]\n",
    "        df[\"temp*total_rad\"]=df[\"t_1000hPa:K\"]*df[\"total_solar_rad\"]\n",
    "        df[\"wind_angle\"]=(np.arctan2(df[\"wind_speed_u_10m:ms\"],df[\"wind_speed_v_10m:ms\"]))*180/np.pi\n",
    "        #df[\"total_snow_depth\"] = df[\"snow_depth:cm\"] + df[\"fresh_snow_1h:cm\"]\n",
    "        #df[\"total_precip_5min\"] = df[\"precip_5min:mm\"] + df[\"snow_melt_10min:mm\"]\n",
    "        #df[\"total_precip_type\"] = df[\"precip_type_5min:idx\"] + df[\"snow_water:kgm2\"]\n",
    "        df[\"total_pressure\"] = df[\"pressure_50m:hPa\"] + df[\"pressure_100m:hPa\"]\n",
    "        df[\"total_sun_angle\"] = df[\"sun_azimuth:d\"] + df[\"sun_elevation:d\"]\n",
    "        df[\"solar intensity\"]=1361*np.cos(np.radians(90-df[\"sun_elevation:d\"]))\n",
    "        df[\"solar intensity\"].clip(lower=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def shift_target(df, target_col):\n",
    "        df = df.copy()\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Store the original indices\n",
    "        original_indices = df.index\n",
    "\n",
    "        # Reindex the DataFrame to include all 15-minute intervals\n",
    "        all_intervals = pd.date_range(start=df.index.min(), end=df.index.max(), freq='15T')\n",
    "        df = df.reindex(all_intervals)\n",
    "\n",
    "        # Shift the target variable by 1 period (15 minutes) forward and backward\n",
    "        df[target_col + '_shifted_forward'] = df[target_col].shift(-1)\n",
    "        df[target_col + '_shifted_backward'] = df[target_col].shift(1)\n",
    "\n",
    "        # Forward fill the missing values for the forward shift\n",
    "        df[target_col + '_shifted_forward'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill the missing values for the backward shift\n",
    "        df[target_col + '_shifted_backward'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Keep only the original indices\n",
    "        df = df.loc[original_indices]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def add_lagged_features(df):\n",
    "        df = df.copy()\n",
    "        features_to_lag = [ \"total_solar_rad\", \"temptotal_rad\", \"clear_sky_radW\", \"diffuse_radW\", \"direct_radW\",  \"total_cloud_coverp\", \"solarintensity\", \"total_sun_angle\", \"pressure_100mhPa\"]\n",
    "        \n",
    "        for feature in features_to_lag:\n",
    "            df = pre.shift_target(df, feature)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def general_read_flaml(letter):\n",
    "\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        #X_final = add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "\n",
    "    def general_read(letter):\n",
    "\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        X_final = pre.add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "    def readRawData(letter):\n",
    "        # read X_train_observed.parquet file for the current letter\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "\n",
    "\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "\n",
    "        # read train_targets.parquet file for the current letter\n",
    "        y = pd.read_parquet(f\"/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/TDT4173_Machine_Learning/{letter}/train_targets.parquet\")\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "\n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "\n",
    "\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        #removing november december and january\n",
    "        #Y = Y[(Y.index.month != 11) & (Y.index.month != 12) & (Y.index.month != 1)] \n",
    "\n",
    "        # Step 1: Keep only rows in X that are within an hour present in Y\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "        #Check length conditions\n",
    "        # print(f\"\\nExpected length of X_final: {4 * len(Y)}\")\n",
    "        # print(f\"Actual length of X_final: {len(X_final)}\")\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        # print(\"\\nHours with mismatched number of 15-min intervals:\")\n",
    "        # print(mismatch_hours)\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "        # if not missing_hours_in_x.empty:\n",
    "        #     print(\"\\nAdditional hours in Y without four 15-min intervals in X:\")\n",
    "        #     print(missing_hours_in_x)\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #Re-check length conditions\n",
    "        # print(f\"\\nAdjusted expected length of X_final: {4 * len(Y_clean)}\")\n",
    "        # print(f\"Actual length of X_final: {len(X_final)}\")\n",
    "\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final.drop(columns=['cloud_base_agl:m'], inplace=True)\n",
    "        X_final.drop(columns=['ceiling_height_agl:m'], inplace=True)\n",
    "        X_final.drop(columns=['snow_density:kgm3'], inplace=True)\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "\n",
    "\n",
    "        return X_final, Y_clean\n",
    "\n",
    "\n",
    "    def general_read_lstm(letter):\n",
    "\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        return X_final, Y_clean\n",
    "\n",
    "\n",
    "    def concatenate_dfs(df_list):\n",
    "        \"\"\"\n",
    "        Concatenates a list of DataFrames into a single DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df_list (list of pd.DataFrame): List of DataFrame objects to concatenate.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A single DataFrame containing all rows from the input DataFrames in the order they appear in the list.\n",
    "        \"\"\"\n",
    "        return pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "\n",
    "    class QuartersAsColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None):\n",
    "            # Ensure input is a DataFrame\n",
    "            X = X.copy()\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            #make sure index is datetime:\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "\n",
    "            original_index = X.index\n",
    "\n",
    "\n",
    "            X['hour'] = X.index.floor('H')\n",
    "            X['minute'] = X.index.minute\n",
    "\n",
    "            # Melt the DataFrame to long format\n",
    "            df_melted = pd.melt(X, id_vars=['hour', 'minute'], value_vars=X.columns[:-2]).copy()  # excluding 'hour' and 'minute'\n",
    "\n",
    "            # Create a multi-level column name combining variable and minute\n",
    "            df_melted['variable_minute'] = df_melted['variable'] + '_' + df_melted['minute'].astype(str) + 'min'\n",
    "\n",
    "            # Drop the 'variable_minute' column\n",
    "\n",
    "\n",
    "            # Pivot the data to get one row per hour and columns for each variable and minute\n",
    "            X = df_melted.pivot(index='hour', columns='variable_minute', values='value').copy()\n",
    "            #rename index to date_forecast:\n",
    "            X.index.rename(\"date_forecast\", inplace=True)\n",
    "\n",
    "\n",
    "            #drop irrelevant columns:\n",
    "            #hour_sin\thour_cos\tmonth_sin\tmonth_cos\tweek_sin\tweek_cos\tdayofyear_sin\tdayofyear_cos\tmult1\tmult2\tuncertainty\n",
    "\n",
    "\n",
    "            irrelevant_cols = [\"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\", \"week_sin\", \"week_cos\", \"dayofyear_sin\", \"dayofyear_cos\", \"uncertainty\"]\n",
    "            variantes = [\"_0min\", \"_15min\", \"_30min\", \"_45min\"]\n",
    "            for variant in variantes:\n",
    "                for col in irrelevant_cols:\n",
    "                    if variant == \"_0min\":\n",
    "                        #remove _0min from column name;\n",
    "                        X.rename(columns={col+variant:col}, inplace=True)\n",
    "                    else:\n",
    "                        X.drop(columns=[col+variant], inplace=True)\n",
    "            \n",
    "\n",
    "            reindex_map = original_index.floor('H').unique()\n",
    "            X = X.reindex(reindex_map)\n",
    "            X.index = reindex_map\n",
    "\n",
    "            #drop hour_\n",
    "\n",
    "            if \"object\" in X.dtypes.unique():\n",
    "                print(\"waring: object in QuarterAsColumnsTransformer\")\n",
    "                print(X.dtypes.unique())\n",
    "                for col in X.columns:\n",
    "                    print(col)\n",
    "\n",
    "            #X = X.select_dtypes(include=[np.number])\n",
    "            return X\n",
    "\n",
    "    class StatisticalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X, y=None):\n",
    "            X_copy = X.copy()\n",
    "            X_copy.index = pd.to_datetime(X_copy.index)\n",
    "            X_copy['hour'] = X_copy.index.floor('H')\n",
    "            \n",
    "            # Compute mean, std\n",
    "            aggregated = X_copy.groupby('hour').agg(['mean', 'std'])\n",
    "            \n",
    "            # Filter hours with exactly 4 data points\n",
    "            valid_hours = X_copy.groupby('hour').size()\n",
    "            valid_hours = valid_hours[valid_hours == 4].index\n",
    "            \n",
    "            X_final = aggregated.loc[valid_hours]\n",
    "            \n",
    "            # Flatten the multi-index to form new column names\n",
    "            X_final.columns = ['_'.join(col).strip() for col in X_final.columns.values]\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "            #drop minute_mean and minute_std if they exist:\n",
    "            if \"minute_mean\" in X_final.columns:\n",
    "                X_final.drop(columns=[\"minute_mean\", \"minute_std\"], inplace=True)\n",
    "            \n",
    "            X_final = X_final.select_dtypes(include=[np.number])\n",
    "            # print(X_final.dtypes.unique())\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "\n",
    "\n",
    "            return X_final\n",
    "        \n",
    "    class TrimmedMeanTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X, y=None):\n",
    "            # Ensure the dataframe's index is a datetime type\n",
    "            X = X.copy()\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "            \n",
    "            original_index = X.index\n",
    "\n",
    "            # Create a helper column 'hour_label' \n",
    "            X['hour_label'] = X.index.floor('H')\n",
    "            \n",
    "            # Compute the trimmed mean for each valid hour\n",
    "            def compute_trimmed_mean(group):\n",
    "                if group.shape[0] != 4:  # Only process groups of size 4\n",
    "                    return np.nan\n",
    "\n",
    "                # Exclude any datetime columns\n",
    "                numeric_cols = group.select_dtypes(include=[np.number])\n",
    "                \n",
    "                min_val = np.min(numeric_cols, axis=0)\n",
    "                max_val = np.max(numeric_cols, axis=0)\n",
    "                total = np.sum(numeric_cols, axis=0)\n",
    "                return (total - min_val - max_val) / 2  # Removing min and max\n",
    "\n",
    "            # Group and apply the function\n",
    "            X_trimmed_mean = X.groupby('hour_label').apply(compute_trimmed_mean)\n",
    "            \n",
    "            # Drop the helper column in the result as it's no longer needed\n",
    "            if 'hour_label' in X_trimmed_mean.columns:\n",
    "                X_trimmed_mean = X_trimmed_mean.drop(columns=['hour_label'])\n",
    "\n",
    "            # Filter hours with exactly 4 data points\n",
    "            valid_hours = X['hour_label'].value_counts()\n",
    "            valid_hours = valid_hours[valid_hours == 4].index\n",
    "            X_final = X_trimmed_mean[X_trimmed_mean.index.isin(valid_hours)]\n",
    "\n",
    "            reindex_map = original_index.floor('H').unique()\n",
    "            X_final = X_final.reindex(reindex_map)\n",
    "            X_final.index = reindex_map\n",
    "\n",
    "            X_final = X_final.select_dtypes(include=[np.number])\n",
    "            \n",
    "            return X_final\n",
    "\n",
    "    class HourMonthTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.encoding_map = {}\n",
    "            self.y_ = None  # To store y during fit\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"y cannot be None for fitting the encoder\")\n",
    "\n",
    "            # Store the target values for encoding later\n",
    "            self.y_ = y\n",
    "\n",
    "            try:\n",
    "                # Extract hour and month from the index and use y provided during fit\n",
    "                df = pd.DataFrame({'target': self.y_, 'hour': X.index.hour, 'month': X.index.month})\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "            # Compute mean target value for each hour of each month\n",
    "            self.encoding_map = df.groupby(['month', 'hour'])['target'].mean().to_dict()\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if self.y_ is None:\n",
    "                raise ValueError(\"The encoder has not been fitted with target values\")\n",
    "\n",
    "            # Extract hour and month from the index\n",
    "            X_transformed = X.copy()\n",
    "            X_transformed['hour'] = X.index.hour\n",
    "            X_transformed['month'] = X.index.month\n",
    "\n",
    "            # Map the mean target values\n",
    "            X_transformed['target_encoded'] = X_transformed.apply(\n",
    "                lambda row: self.encoding_map.get((row['month'], row['hour']), np.nan), axis=1)\n",
    "\n",
    "            # Optionally drop 'hour' and 'month' if they're not needed\n",
    "            X_transformed.drop(['hour', 'month'], axis=1, inplace=True)\n",
    "\n",
    "            # Check for object dtypes and print warning if any\n",
    "            if \"object\" in X_transformed.dtypes.values:\n",
    "                print(\"Warning: object dtype in HourMonthTargetEncoder\")\n",
    "                print(X_transformed.dtypes)\n",
    "\n",
    "            # Ensure that only numeric types are returned\n",
    "            X_transformed = X_transformed.select_dtypes(include=[np.number])\n",
    "\n",
    "            return X_transformed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_preprocessor(data, preprocessor_name):\n",
    "        data = data.copy()\n",
    "        # Assuming `pre.choose_transformer` returns a callable object that can be used to transform the data\n",
    "        preprocessor = pre.choose_transformer(preprocessor_name)\n",
    "        return preprocessor.transform(data)\n",
    "\n",
    "    #Preprocessing functions for the different models:\n",
    "\n",
    "\n",
    "    #LSTM preprocessing:\n",
    "    def train_val_split_diffrent_folds(X,y,letter,fold_number):\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "        if letter == \"A\":\n",
    "                assert fold_number in [0,1,2,3]\n",
    "                year = 2019 + fold_number\n",
    "        elif letter == \"B\":\n",
    "            assert fold_number in [0,1,2]\n",
    "            year = 2019 + fold_number\n",
    "        elif letter == \"C\":\n",
    "            assert fold_number in [0,1]\n",
    "            year = 2020 + fold_number\n",
    "\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_val = X[may_june_july_condition_X]\n",
    "        y_val = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_split_may_june_july(X, y,letter):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        return X_train, y_train, X_may_june_july, y_may_june_july\n",
    "\n",
    "    def train_val_blend(X, y,letter):\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2021\n",
    "\n",
    "\n",
    "        if letter == \"A\":\n",
    "            blend_year = 2021\n",
    "        elif letter == \"B\":\n",
    "            blend_year = 2020\n",
    "        elif letter == \"C\":\n",
    "            blend_year = 2020\n",
    "        \n",
    "        if letter == \"C\":\n",
    "            # Define conditions to move May and June of split_date's year from train to test\n",
    "            may_june_july_condition_X = ((X.index.month == 6) | (X.index.month == 5)) & ((X.index.year == year))\n",
    "            may_june_july_condition_y = ((y.index.month == 6) | (y.index.month == 5)) & ((y.index.year == year))\n",
    "\n",
    "        else:\n",
    "            # Define conditions to move May and June of split_date's year from train to test\n",
    "            may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "            may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "            \n",
    "        X_val = X[may_june_july_condition_X]\n",
    "        y_val = y[may_june_july_condition_y]\n",
    "\n",
    "        if letter == \"C\":\n",
    "            X_blend_condition = ((X.index.month == 7) | (X.index.month == 8)) & ((X.index.year == blend_year))\n",
    "            y_blend_condition = ((y.index.month == 7) | (y.index.month == 8)) & ((y.index.year == blend_year))\n",
    "        else:\n",
    "            X_blend_condition = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == blend_year))\n",
    "            y_blend_condition = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == blend_year))\n",
    "\n",
    "\n",
    "        X_blend = X[X_blend_condition]\n",
    "        y_blend = y[y_blend_condition]\n",
    "\n",
    "        # Remove the data from training set\n",
    "        X_train = X[~may_june_july_condition_X & ~X_blend_condition]\n",
    "        y_train = y[~may_june_july_condition_y & ~y_blend_condition]\n",
    "\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_blend, y_blend\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_split_on_specific_day_May_june(X, y, split_date):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        split_date = pd.Timestamp(split_date).normalize()\n",
    "\n",
    "        # Ensure split_date is a datetime object\n",
    "        if isinstance(split_date, str):\n",
    "            split_date = pd.Timestamp(split_date)\n",
    "\n",
    "        print(f\"Split date: {split_date}\")\n",
    "\n",
    "        # Split the data based on the provided date\n",
    "        X_train = X[X.index.normalize() < split_date]\n",
    "        y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "        X_test = X[X.index.normalize() >= split_date]\n",
    "        y_test = y[y.index.normalize() >= split_date]\n",
    "\n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_condition_X = ((X_train.index.month == 5) | (X_train.index.month == 6) | (X_train.index.month == 7)) & ((X_train.index.year == split_date.year))\n",
    "        may_june_condition_y = ((y_train.index.month == 5) | (y_train.index.month == 6) | (y_train.index.month == 7)) & ((y_train.index.year == split_date.year))\n",
    "        \n",
    "        X_may_june = X_train[may_june_condition_X]\n",
    "        y_may_june = y_train[may_june_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X_train[~may_june_condition_X]\n",
    "        y_train = y_train[~may_june_condition_y]\n",
    "\n",
    "        # Append May and June data to test set\n",
    "        X_test = pd.concat([X_may_june, X_test])\n",
    "        y_test = pd.concat([y_may_june, y_test])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def time_series_split(X, split_date = \"2022-10-29\"):\n",
    "        \n",
    "        if not isinstance(X.index, pd.DatetimeIndex):\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "        \n",
    "        split_date = pd.to_datetime(split_date)\n",
    "        \n",
    "        mask_val = (X.index >= split_date)\n",
    "        \n",
    "        split_year = split_date.year\n",
    "        mask_may_june_july = (X.index.month.isin([5, 6, 7])) & (X.index.year == split_year)\n",
    "        \n",
    "        mask_val = mask_val | mask_may_june_july\n",
    "\n",
    "        \n",
    "        test_fold = np.where(mask_val, 0, -1)\n",
    "        \n",
    "        \n",
    "        return PredefinedSplit(test_fold)\n",
    "\n",
    "    def split_df_on_alternate_days(x_df, y_df):\n",
    "        # Convert index to datetime if it's not already\n",
    "        x_df.index = pd.to_datetime(x_df.index)\n",
    "        y_df.index = pd.to_datetime(y_df.index)\n",
    "        \n",
    "        # Check if both dataframes are aligned\n",
    "        assert all(x_df.index == y_df.index), \"Indexes of x_df and y_df do not match!\"\n",
    "\n",
    "        # Extract day from the index\n",
    "        days = x_df.index.day\n",
    "\n",
    "        # Split into even and odd days\n",
    "        x_even_days = x_df[days % 2 == 0]\n",
    "        y_even_days = y_df[days % 2 == 0]\n",
    "\n",
    "        x_odd_days = x_df[days % 2 != 0]\n",
    "        y_odd_days = y_df[days % 2 != 0]\n",
    "\n",
    "        return x_even_days, y_even_days, x_odd_days, y_odd_days\n",
    "\n",
    "    def lstm_train_test_split(X, y,letter, split_date):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        split_date = pd.Timestamp(split_date).normalize()\n",
    "\n",
    "        if isinstance(split_date, str):\n",
    "            split_date = pd.Timestamp(split_date)\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "\n",
    "        X_train = X[X.index.normalize() < split_date]\n",
    "        y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "        X_test = X[X.index.normalize() >= split_date]\n",
    "        y_test = y[y.index.normalize() >= split_date]\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        # Append May and June data to test set\n",
    "        X_test = pd.concat([X_may_june_july, X_test])\n",
    "        y_test = pd.concat([y_may_june_july, y_test])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "    def remove_winter_months(df):\n",
    "        \"\"\"\n",
    "        Removes the winter months (December, January, February) from a DataFrame that\n",
    "        has a DateTime index.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame with DateTime index.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with winter months removed.\n",
    "        \"\"\"\n",
    "        # Ensure the index is a DateTimeIndex\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"DataFrame index must be a DateTimeIndex.\")\n",
    "\n",
    "        # Define condition to filter out the winter months\n",
    "        winter_condition = (df.index.month == 12) | (df.index.month == 1) | (df.index.month == 2) | (df.index.month == 11)\n",
    "\n",
    "        # Filter out the winter months\n",
    "        df_no_winter = df[~winter_condition]\n",
    "        return df_no_winter\n",
    "\n",
    "\n",
    "    def new_train_test_split(X, y,letter, split_date):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        split_date = pd.Timestamp(split_date).normalize()\n",
    "        print(f\"Split date: {split_date}\")\n",
    "\n",
    "        if isinstance(split_date, str):\n",
    "            split_date = pd.Timestamp(split_date)\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "\n",
    "        X_train = X[X.index.normalize() < split_date]\n",
    "        y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "        X_test = X[X.index.normalize() >= split_date]\n",
    "        y_test = y[y.index.normalize() >= split_date]\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        # Append May and June data to test set\n",
    "        X_test = pd.concat([X_may_june_july, X_test])\n",
    "        y_test = pd.concat([y_may_june_july, y_test])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "    def choose_scaler(scaler_string):\n",
    "        if scaler_string == \"minmax\":\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_string == \"standard\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_string == \"robust\":\n",
    "            return RobustScaler()\n",
    "\n",
    "    def choose_transformer(transformer_string):\n",
    "        if transformer_string == \"quarters\":\n",
    "            return pre.QuartersAsColumnsTransformer()\n",
    "        elif transformer_string == \"statistical\":\n",
    "            return pre.StatisticalFeaturesTransformer()\n",
    "        elif transformer_string == \"trimmedMean\":\n",
    "            return pre.TrimmedMeanTransformer()\n",
    "\n",
    "    def choose_encoder(encoder_boolian):\n",
    "        if encoder_boolian == True:\n",
    "            return pre.HourMonthTargetEncoder()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def generate_predefined_split(X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        This function takes in separate training and validation datasets, combines them,\n",
    "        and creates a PredefinedSplit object that can be used with sklearn's GridSearchCV\n",
    "        or other model selection utilities. This allows for specifying which samples are\n",
    "        used for training and which are used for validation.\n",
    "\n",
    "        Parameters:\n",
    "        X_train (array-like): Training features.\n",
    "        X_val (array-like): Validation features.\n",
    "        y_train (array-like): Training labels.\n",
    "        y_val (array-like): Validation labels.\n",
    "\n",
    "        Returns:\n",
    "        X (array-like): The combined dataset of features.\n",
    "        y (array-like): The combined dataset of labels.\n",
    "        split_index (PredefinedSplit): An instance of PredefinedSplit with the indices set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Combine the training and validation sets\n",
    "        X = np.concatenate((X_train, X_val), axis=0)\n",
    "        y = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "        # Generate the indices array where -1 indicates the sample is part of the training set,\n",
    "        # and 0 indicates the sample is part of the validation set.\n",
    "        train_indices = -1 * np.ones(len(X_train))\n",
    "        val_indices = 0 * np.ones(len(X_val))\n",
    "        test_fold = np.concatenate((train_indices, val_indices))\n",
    "\n",
    "        # Create the PredefinedSplit object\n",
    "        predefined_split = PredefinedSplit(test_fold)\n",
    "\n",
    "        return X, y, predefined_split\n",
    "\n",
    "    def printhei():\n",
    "        print(\"hei\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "#import a mean signed error function\n",
    "\n",
    "class post:\n",
    "    def calculate_hourly_mae_and_plot(predictions, actuals):\n",
    "        \"\"\"\n",
    "        Calculate the MAE for each hour of the day across multiple days, \n",
    "        and plot a histogram of these MAE values.\n",
    "        Assumes each day contains 24 consecutive hourly observations in order.\n",
    "\n",
    "        :param predictions: List or array of predictions.\n",
    "        :param actuals: List or array of actual values.\n",
    "        \"\"\"\n",
    "        if len(predictions) % 24 != 0 or len(actuals) % 24 != 0:\n",
    "            raise ValueError(\"The length of predictions and actuals should be a multiple of 24.\")\n",
    "\n",
    "        hourly_mae = []\n",
    "        num_days = len(predictions) // 24\n",
    "\n",
    "        for hour in range(24):\n",
    "            hourly_preds = predictions[hour::24]\n",
    "            hourly_acts = actuals[hour::24]\n",
    "            hourly_mae.append(mean_absolute_error(hourly_acts, hourly_preds))\n",
    "\n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(24), hourly_mae, color='skyblue')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.title('Hourly MAE of Predictions')\n",
    "        plt.xticks(range(24), [f\"{hour:02d}:00\" for hour in range(24)])\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def calculate_hourly_me_and_plot(predictions, actuals):\n",
    "        \"\"\"\n",
    "        Calculate the ME (Mean Error) for each hour of the day across multiple days, \n",
    "        and plot a histogram of these ME values.\n",
    "        Assumes each day contains 24 consecutive hourly observations in order.\n",
    "\n",
    "        :param predictions: List or array of predictions.\n",
    "        :param actuals: List or array of actual values.\n",
    "        \"\"\"\n",
    "        if len(predictions) % 24 != 0 or len(actuals) % 24 != 0:\n",
    "            raise ValueError(\"The length of predictions and actuals should be a multiple of 24.\")\n",
    "\n",
    "        hourly_me = []\n",
    "        num_days = len(predictions) // 24\n",
    "\n",
    "        for hour in range(24):\n",
    "            hourly_preds = predictions[hour::24]\n",
    "            hourly_acts = actuals[hour::24]\n",
    "            hourly_me.append(np.mean([a - p for a, p in zip(hourly_acts, hourly_preds)]))\n",
    "\n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(24), hourly_me, color='skyblue')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Mean Error')\n",
    "        plt.title('Hourly ME of Predictions')\n",
    "        plt.xticks(range(24), [f\"{hour:02d}:00\" for hour in range(24)])\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def readRawTest(letter):\n",
    "        df = pd.read_parquet(f\"/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/TDT4173_Machine_Learning/{letter}/X_test_estimated.parquet\")\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "\n",
    "    def readAndBasicPreprocess(letter):\n",
    "        X = post.readRawTest(letter)\n",
    "        X.drop(columns=['cloud_base_agl:m'], inplace=True)\n",
    "        X.drop(columns=['ceiling_height_agl:m'], inplace=True)\n",
    "        X.drop(columns=['snow_density:kgm3'], inplace=True)\n",
    "        X=pre.create_features(X)\n",
    "        X=pre.create_time_features(X)\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "        X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        X = pre.add_lagged_features(X)\n",
    "        return X\n",
    "\n",
    "    def makePrediction(A_model, B_model, C_model, filename):\n",
    "        A_x_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_y_pred=A_model.predict(A_x_test)\n",
    "        A_y_pred=pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_x_test= post.readAndBasicPreprocess(\"B\")\n",
    "        B_y_pred=B_model.predict(B_x_test)\n",
    "        B_y_pred=pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_x_test= post.readAndBasicPreprocess(\"C\")\n",
    "        C_y_pred=C_model.predict(C_x_test)\n",
    "        C_y_pred=pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    def makeEnsemblePrediction(A_xgb_model, A_xgb_processing, A_dnn_model, A_dnn_preprocessing, A_dnn_target_preprocessing, B_xgb_model, B_xgb_processing, B_dnn_model, B_dnn_preprocessing, B_dnn_target_preprocessing, C_xgb_model, C_xgb_processing, C_dnn_model,C_dnn_preprocessing, C_dnn_target_preprocessing, filename):\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "\n",
    "        A_X_test_xgb = A_xgb_processing.transform(A_X_test)\n",
    "        A_y_pred_xgb = A_xgb_model.predict(A_X_test_xgb)\n",
    "\n",
    "        A_X_test_dnn = pd.DataFrame(A_dnn_preprocessing.transform(A_X_test))\n",
    "        A_y_pred_dnn = A_dnn_model.predict(A_X_test_dnn)\n",
    "        A_y_pred_dnn = A_dnn_target_preprocessing.inverse_transform(A_y_pred_dnn).reshape(-1)\n",
    "\n",
    "        A_y_pred = (A_y_pred_xgb + A_y_pred_dnn) / 2\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "\n",
    "        B_X_test_xgb = B_xgb_processing.transform(B_X_test)\n",
    "        B_y_pred_xgb = B_xgb_model.predict(B_X_test_xgb)\n",
    "\n",
    "        B_X_test_dnn = pd.DataFrame(B_dnn_preprocessing.transform(B_X_test))\n",
    "        B_y_pred_dnn = B_dnn_model.predict(B_X_test_dnn)\n",
    "        B_y_pred_dnn = B_dnn_target_preprocessing.inverse_transform(B_y_pred_dnn).reshape(-1)\n",
    "\n",
    "        B_y_pred = (B_y_pred_xgb + B_y_pred_dnn) / 2\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "\n",
    "        C_X_test_xgb = B_xgb_processing.transform(C_X_test)\n",
    "        C_y_pred_xgb = B_xgb_model.predict(C_X_test_xgb)\n",
    "\n",
    "        C_X_test_dnn = pd.DataFrame(B_dnn_preprocessing.transform(C_X_test))\n",
    "        C_y_pred_dnn = B_dnn_model.predict(C_X_test_dnn)\n",
    "        C_y_pred_dnn = B_dnn_target_preprocessing.inverse_transform(C_y_pred_dnn).reshape(-1)\n",
    "\n",
    "        C_y_pred = (C_y_pred_xgb + C_y_pred_dnn) / 2\n",
    "\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "        \n",
    "\n",
    "    def make_dnn_prediction(A_model, A_preprocessing, A_target_scaling, B_model, B_preprocessing, B_target_scaling, C_model, C_preprocessing, C_target_scaling, filename):\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test_dnn = pd.DataFrame(A_preprocessing.transform(A_X_test))\n",
    "        A_y_pred_dnn = A_model.predict(A_X_test_dnn)\n",
    "        A_y_pred_dnn = A_target_scaling.inverse_transform(A_y_pred_dnn).reshape(-1)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred_dnn, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test_dnn = pd.DataFrame(B_preprocessing.transform(B_X_test))\n",
    "        B_y_pred_dnn = B_model.predict(B_X_test_dnn)\n",
    "        B_y_pred_dnn = B_target_scaling.inverse_transform(B_y_pred_dnn).reshape(-1)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred_dnn, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test_dnn = pd.DataFrame(C_preprocessing.transform(C_X_test))\n",
    "        C_y_pred_dnn = C_model.predict(C_X_test_dnn)\n",
    "        C_y_pred_dnn = C_target_scaling.inverse_transform(C_y_pred_dnn).reshape(-1)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred_dnn, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "    def make_xgb_prediction(A_model, A_preprocessing, B_model, B_preprocessing, C_model, C_preprocessing, filename):\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test_xgb = A_preprocessing.transform(A_X_test)\n",
    "\n",
    "        A_y_pred_xgb = A_model.predict(A_X_test_xgb)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred_xgb, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test_xgb = B_preprocessing.transform(B_X_test)\n",
    "\n",
    "        B_y_pred_xgb = B_model.predict(B_X_test_xgb)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred_xgb, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test_xgb = C_preprocessing.transform(C_X_test)\n",
    "\n",
    "        C_y_pred_xgb = C_model.predict(C_X_test_xgb)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred_xgb, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    def makeAutoMLPred(A_model, B_model, C_model, filename):\n",
    "        \"\"\"\n",
    "        Assumes QuarterAsColumn\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test = pre.QuartersAsColumnsTransformer().transform(A_X_test)\n",
    "        A_y_pred = A_model.predict(A_X_test)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test = pre.QuartersAsColumnsTransformer().transform(B_X_test)\n",
    "        B_y_pred = B_model.predict(B_X_test)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test = pre.QuartersAsColumnsTransformer().transform(C_X_test)\n",
    "        C_y_pred = C_model.predict(C_X_test)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.loc[(combined_pred.index % 24).isin([22, 23, 0]), \"prediction\"] = 0\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    def compute_mae(ser1, ser2):\n",
    "        \"\"\"Compute Mean Absolute Error between two Series.\"\"\"\n",
    "        return np.abs(ser1 - ser2).mean()\n",
    "\n",
    "    def plot_mae_grid(dataframes_dict):\n",
    "        \"\"\"Plot a grid of MAE values for a dictionary of DataFrames.\"\"\"\n",
    "        \n",
    "        labels = list(dataframes_dict.keys())\n",
    "        dataframes = list(dataframes_dict.values())\n",
    "        n = len(dataframes)\n",
    "        \n",
    "        mae_grid = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    mae_grid[i][j] = post.compute_mae(dataframes[i][\"prediction\"], dataframes[j][\"prediction\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        cax = ax.matshow(mae_grid, cmap=\"viridis\")\n",
    "        \n",
    "        ax.grid(False)\n",
    "        plt.xticks(range(n), labels, rotation=45)\n",
    "        plt.yticks(range(n), labels)\n",
    "        \n",
    "        # Add annotations\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                text = ax.text(j, i, f\"{mae_grid[i, j]:.2f}\",\n",
    "                            ha=\"center\", va=\"center\", color=\"w\" if mae_grid[i, j] > (mae_grid.max() / 2) else \"black\")\n",
    "        \n",
    "        plt.colorbar(cax)\n",
    "        plt.title('MAE Between DataFrames on \"prediction\" Column', pad=20)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def test():\n",
    "        print(\"hei\")\n",
    "\n",
    "    def makePredictionWithModelAndPreprocessor(A_model, B_model, C_model, preprocessor, filename):\n",
    "        \"\"\"\n",
    "        Assumes same preprocessing for all locations\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test = preprocessor.transform(A_X_test)\n",
    "        A_y_pred = A_model.predict(A_X_test)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test = preprocessor.transform(B_X_test)\n",
    "        B_y_pred = B_model.predict(B_X_test)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test = preprocessor.transform(C_X_test)\n",
    "        C_y_pred = C_model.predict(C_X_test)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "    def submission_vs_best_submission(filepath):\n",
    "        \"\"\"\n",
    "            mostly for debugging and testing. Checks if the submission is in the same ballpark as the best submission\n",
    "        \"\"\"\n",
    "\n",
    "        refrence = pd.read_csv(\"/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/TDT4173_Machine_Learning/Jallastacking/csvfiles/two_best_combined_zeroed_night_hours.csv\")\n",
    "\n",
    "        submission = pd.read_csv(filepath)\n",
    "\n",
    "        print(f\"MAE for location A: {mean_absolute_error(refrence['prediction'].iloc[0:720], submission['prediction'].iloc[0:720])}\")\n",
    "        print(f\"MAE for location B: {mean_absolute_error(refrence['prediction'].iloc[720:1440], submission['prediction'].iloc[720:1440])}\")\n",
    "        print(f\"MAE for location C: {mean_absolute_error(refrence['prediction'].iloc[1440:2160], submission['prediction'].iloc[1440:2160])}\")\n",
    "\n",
    "\n",
    "    def make_average_prediction(preds_dict,filename):\n",
    "        \"\"\"\n",
    "        Generates a prediction by taking the average of the predictions in preds_dict.\n",
    "        \"\"\"\n",
    "        lenght = len(preds_dict)\n",
    "        data = 0\n",
    "        for value in preds_dict.values():\n",
    "            data += value[\"prediction\"]\n",
    "        data = data / lenght\n",
    "        data = pd.DataFrame(data, columns=['prediction'])\n",
    "        data.index.name = \"id\"\n",
    "        data[\"prediction\"] = data['prediction'].apply(lambda x: 0 if x < 0.05 else x)\n",
    "        data.loc[(data.index % 24).isin([22, 23, 0]), \"prediction\"] = 0\n",
    "        data.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "    def make_lgbm_preprocessor_pred(A_model, A_scaler, A_preprocessor, B_model, B_scaler, B_preprocessor,C_model, C_scaler, C_preprocessor, filename):\n",
    "\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test = A_preprocessor.transform(A_X_test)\n",
    "        A_X_test = A_scaler.transform(A_X_test)\n",
    "        A_y_pred = A_model.predict(A_X_test)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test = B_preprocessor.transform(B_X_test)\n",
    "        B_X_test = B_scaler.transform(B_X_test)\n",
    "        B_y_pred = B_model.predict(B_X_test)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test = C_preprocessor.transform(C_X_test)\n",
    "        C_X_test = C_scaler.transform(C_X_test)\n",
    "        C_y_pred = C_model.predict(C_X_test)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    def make_one_location_pred(model, letter, preprocessor, filename):\n",
    "        if letter == \"A\":\n",
    "            index_range = range(0,720)\n",
    "        elif letter == \"B\":\n",
    "            index_range = range(720,1440)\n",
    "        elif letter == \"C\":\n",
    "            index_range = range(1440,2160)\n",
    "        \n",
    "\n",
    "        X_test = post.readAndBasicPreprocess(letter)\n",
    "        X_test = preprocessor.transform(X_test)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = pd.DataFrame(y_pred, index=index_range, columns=['prediction'])\n",
    "        y_pred[\"prediction\"] = y_pred[\"prediction\"].clip(lower=0)\n",
    "        y_pred.index.name = \"id\"\n",
    "        y_pred.to_csv(filename, index=True)\n",
    "\n",
    "    def mean_diffrent_summers(filepath,folds_dict, predictionfilename):\n",
    "        #FOLDS = {\"A\": 4, \"B\": 3, \"C\": 2}\n",
    "        d_A = 0\n",
    "        d_B = 0\n",
    "        d_C = 0\n",
    "\n",
    "        for letter in folds_dict.keys():\n",
    "            preds = []\n",
    "            folder_path = f\"{filepath}/{letter}\"\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    data = df[\"prediction\"]\n",
    "                    preds.append(data)\n",
    "            #mean the predictions\n",
    "            if letter == \"A\":\n",
    "                for i,pred in enumerate(preds):\n",
    "                    if i == 0:\n",
    "                        d_A = pred\n",
    "                    else:\n",
    "                        d_A += pred\n",
    "                d_A =d_A/len(preds)\n",
    "                #d_A = pd.DataFrame(d_A, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "            \n",
    "            elif letter == \"B\":\n",
    "                for i,pred in enumerate(preds):\n",
    "                    if i == 0:\n",
    "                        d_B = pred\n",
    "                    else:\n",
    "                        d_B += pred\n",
    "                #print(d_B)\n",
    "\n",
    "                d_B = d_B/len(preds)\n",
    "                #d_B = pd.DataFrame(d_B, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "            \n",
    "            elif letter == \"C\":\n",
    "                for i,pred in enumerate(preds):\n",
    "                    if i == 0:\n",
    "                        d_C = pred\n",
    "                    else:\n",
    "                        d_C += pred\n",
    "                d_C =d_C/len(preds)\n",
    "                #d_C = pd.DataFrame(d_C, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "            \n",
    "        combined_pred = pd.concat([d_A, d_B, d_C], axis=0)\n",
    "        #name the column prediction\n",
    "        combined_pred = pd.DataFrame(combined_pred, columns=['prediction'])\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        #reset index:\n",
    "        combined_pred.reset_index(inplace=True, drop=True)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.loc[(combined_pred.index % 24).isin([22, 23, 0]), \"prediction\"] = 0\n",
    "        combined_pred.to_csv(predictionfilename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 69\n",
    "\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "FOLDER_NAME = \"SHORT_NOTEBOOK1_CSV_FILES\"\n",
    "GLOBAL_VERBOSE = False\n",
    "PREPROCESSORS = [\"quarters\",\"statistical\", \"trimmedMean\"]\n",
    "LETTERS = [\"A\", \"B\", \"C\"]\n",
    "if not os.path.exists(FOLDER_NAME):\n",
    "    os.makedirs(FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining The DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#impoting lightning:\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "#import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as DataSet\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "SEED = 69\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class SolarForecastingDataset(DataSet):\n",
    "    def __init__(self, features_df, target_series):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with features and target labels.\n",
    "\n",
    "        :param features_df: DataFrame containing the features.\n",
    "        :param target_series: Series containing the target labels.\n",
    "        \"\"\"\n",
    "        self.features = features_df\n",
    "        self.targets = target_series\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extracting the features and the target label for the given index\n",
    "        feature_vector = self.features.iloc[index].values\n",
    "        target_label = self.targets.iloc[index]\n",
    "\n",
    "        return {\n",
    "            \"feature_vector\": torch.tensor(feature_vector, dtype=torch.float),\n",
    "            \"target_label\": torch.tensor(target_label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class SolarForecastingDatasetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_features_df, train_targets_series, test_features_df, test_targets_series, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_features_df = train_features_df\n",
    "        self.train_targets_series = train_targets_series\n",
    "        self.test_features_df = test_features_df\n",
    "        self.test_targets_series = test_targets_series\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SolarForecastingDataset(self.train_features_df, self.train_targets_series)\n",
    "        self.test_dataset = SolarForecastingDataset(self.test_features_df, self.test_targets_series)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,worker_init_fn=seed_worker, generator=g, shuffle=True,)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "class FullyConnectedDNN(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, dropout_prob=0.1):\n",
    "        super(FullyConnectedDNN, self).__init__()\n",
    "        # Create fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)):\n",
    "            in_features = input_size if i == 0 else layer_sizes[i - 1]\n",
    "            out_features = layer_sizes[i]\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "        self.output_layer = nn.Linear(layer_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def weighted_mae_loss(input, target, exponent=1, constant=1):\n",
    "    assert input.size() == target.size()\n",
    "\n",
    "    # Calculate the absolute error\n",
    "    absolute_errors = torch.abs(input - target)\n",
    "\n",
    "    # Apply exponential scaling with a constant\n",
    "    adjusted_target = target + constant\n",
    "    weighted_errors = absolute_errors * (adjusted_target ** exponent)\n",
    "\n",
    "    return weighted_errors.mean()\n",
    "\n",
    "class CustomMAELoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=1.0):\n",
    "        super(CustomMAELoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        errors = torch.abs(inputs - targets)  # Compute the absolute errors\n",
    "        capped_errors = torch.clamp(errors, max=10000)\n",
    "        loss = self.alpha * torch.pow(capped_errors, self.beta) \n",
    "        return torch.mean(loss)  # Return the mean loss\n",
    "\n",
    "\n",
    "class SolarPowerProductionPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, output_size, weight_decay=1e-5, dropout_prob=0.1, learning_rate=0.01, verbose=True, loss_exponent=1.0, loss_beta=1.0):\n",
    "        super().__init__()\n",
    "        self.model = FullyConnectedDNN(input_size, layer_sizes, output_size, dropout_prob=dropout_prob)\n",
    "        self.criterion = self.criterion = lambda input, target: weighted_mae_loss(input, target, exponent=loss_exponent, constant=1)\n",
    "        #self.criterion = CustomMAELoss(loss_alpha, loss_beta)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"train_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"val_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"test_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        # Save the best model path to the pl_module\n",
    "        pl_module.best_model_path = self.best_model_path\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "            predictions = model(features)[1]  \n",
    "            if not isinstance(predictions, torch.Tensor):\n",
    "                raise TypeError(\"Model output is not a tensor. Got type: {}\".format(type(predictions)))\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Check for tensor types before concatenation\n",
    "    if not all(isinstance(p, torch.Tensor) for p in all_predictions):\n",
    "        raise TypeError(\"Not all elements in predictions are tensors.\")\n",
    "\n",
    "    if not all(isinstance(l, torch.Tensor) for l in all_labels):\n",
    "        raise TypeError(\"Not all elements in labels are tensors.\")\n",
    "\n",
    "    all_predictions_tensor = torch.cat(all_predictions, dim=0)\n",
    "    all_labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    all_predictions_np = all_predictions_tensor.cpu().numpy()\n",
    "    all_labels_np = all_labels_tensor.cpu().numpy()\n",
    "    \n",
    "    return all_predictions_np, all_labels_np\n",
    "\n",
    "class HenrikDNN:\n",
    "\n",
    "    def __init__(self,n_features = None, layer_sizes = [100,50], output_size = 1, drop_out_prob = 0.1, learning_rate = 0.01, weight_decay = 1e-5, max_epochs = 100, paitience = 5, batch_size = 16, val_chack_interval = 1, pruning_callback = None, verbose = True, loss_expontent = 1):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.verbose = verbose\n",
    "        self.output_size = output_size\n",
    "        self.drop_out_prob = drop_out_prob\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_expontent = loss_expontent\n",
    "        self.paitience = paitience\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.val_chack_interval = val_chack_interval\n",
    "        self.pruning_callback = pruning_callback\n",
    "        SEED = 69\n",
    "        print(\"hei\")\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "        self.pl_model = SolarPowerProductionPredictor(self.n_features, self.layer_sizes, self.output_size, weight_decay=self.weight_decay, dropout_prob=self.drop_out_prob, learning_rate=self.learning_rate, verbose=self.verbose, loss_exponent=self.loss_expontent)\n",
    "\n",
    "        self.checkpoint_callback = CustomModelCheckpoint(\n",
    "            dirpath='HenrikDNN_checkpoints',\n",
    "            save_top_k=1,\n",
    "            verbose=self.verbose,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            filename='model-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "\n",
    "        self.early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.paitience\n",
    "        )\n",
    "\n",
    "\n",
    "        self.callbacks = [self.early_stopping_callback, self.checkpoint_callback]\n",
    "        if self.pruning_callback is not None:\n",
    "            self.callbacks.append(self.pruning_callback)\n",
    "        seed_everything(69, workers=True)\n",
    "\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=self.callbacks,\n",
    "            enable_progress_bar=self.verbose,\n",
    "            accelerator=\"cpu\",\n",
    "            check_val_every_n_epoch = 2,\n",
    "            deterministic=True\n",
    "\n",
    "            #val_check_interval=self.val_chack_interval\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train: Training df with datetime index. \n",
    "            y_train: Training df, with datetime index. Each row in y_train corresponds to four rows in X_train.\n",
    "    \n",
    "        \"\"\"\n",
    "        #print the seed:\n",
    "        SEED = 69\n",
    "        print(\"hei\")\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "        self.data_module = SolarForecastingDatasetDataModule(X_train, y_train, X_val, y_val, batch_size=self.batch_size)\n",
    "        self.trainer.fit(self.pl_model, self.data_module)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        trained_model = SolarPowerProductionPredictor.load_from_checkpoint(\n",
    "            self.pl_model.best_model_path,\n",
    "            input_size=self.n_features,\n",
    "            layer_sizes=self.layer_sizes,\n",
    "            output_size=self.output_size,\n",
    "            dropout_prob=self.drop_out_prob,\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            verbose=self.verbose,\n",
    "            loss_exponent=self.loss_expontent\n",
    "        )\n",
    "\n",
    "        X_dataloader = torch.utils.data.DataLoader(\n",
    "            SolarForecastingDataset(X, pd.Series(np.zeros(X.shape[0]))),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g\n",
    "        )\n",
    "\n",
    "        predictions, _ = get_predictions(trained_model, X_dataloader)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_on_specific_day_May_june(X, y, split_date):\n",
    "    \"\"\"\n",
    "    Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "    from training set to test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Quarter-hourly input data with DateTime index.\n",
    "    - y: Hourly target data with DateTime index.\n",
    "    - split_date: Date (string or datetime object) to split the data on.\n",
    "    \n",
    "    Returns:\n",
    "    X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    split_date = pd.Timestamp(split_date).normalize()\n",
    "\n",
    "    # Ensure split_date is a datetime object\n",
    "    if isinstance(split_date, str):\n",
    "        split_date = pd.Timestamp(split_date)\n",
    "\n",
    "    print(f\"Split date: {split_date}\")\n",
    "\n",
    "    # Split the data based on the provided date\n",
    "    X_train = X[X.index.normalize() < split_date]\n",
    "    y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "    X_test = X[X.index.normalize() >= split_date]\n",
    "    y_test = y[y.index.normalize() >= split_date]\n",
    "\n",
    "    # Define conditions to move May and June of split_date's year from train to test\n",
    "    may_june_condition_X = ((X_train.index.month == 5) | (X_train.index.month == 6) | (X_train.index.month == 7)) & (X_train.index.year == split_date.year)\n",
    "    may_june_condition_y = ((y_train.index.month == 5) | (y_train.index.month == 6) | (y_train.index.month == 7)) & (y_train.index.year == split_date.year)\n",
    "    \n",
    "    X_may_june = X_train[may_june_condition_X]\n",
    "    y_may_june = y_train[may_june_condition_y]\n",
    "\n",
    "    # Remove May and June data from training set\n",
    "    X_train = X_train[~may_june_condition_X]\n",
    "    y_train = y_train[~may_june_condition_y]\n",
    "\n",
    "    # Append May and June data to test set\n",
    "    X_test = pd.concat([X_may_june, X_test])\n",
    "    y_test = pd.concat([y_may_june, y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training A, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 69\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c0f81b03d34ead9e0d40b1ea89421f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad15bafdc94c49638d9b4ceb0038e77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80816e8c8d4c440f8bce19a58b50d312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 498: 'val_loss' reached 0.07809 (best 0.07809), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.08.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1bb71df5f648469a8b2122403ece9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location A: 329.5761192322475\n",
      "new best score for A: 329.5761192322475\n",
      "training B, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 69\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f65222c94e4e76948ea473d0bb3945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a460f4d03f4671a962074d3179a8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9987c3f66f954b489f9e4baef380b7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 386: 'val_loss' reached 0.12988 (best 0.12988), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.13-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bcbc054b914f929fe5e8bf7e72294c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 772: 'val_loss' reached 0.09252 (best 0.09252), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=03-val_loss=0.09.ckpt' as top 1\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location B: 71.50019573666992\n",
      "new best score for B: 71.50019573666992\n",
      "training C, iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 69\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "hei\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed2daab0ee446aea87922ddffe30972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532f89fe20b842eaa0b94ca8cf19b948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c794d4fff4b7987f1abe33af8d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 304: 'val_loss' reached 0.03928 (best 0.03928), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=01-val_loss=0.04-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bac36364114aeb910ca465a781d6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 608: 'val_loss' reached 0.03810 (best 0.03810), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=03-val_loss=0.04.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c9f25c4e2a4a07926cef79828e85f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 912: 'val_loss' reached 0.03732 (best 0.03732), saving model to '/Users/henrikhorpedal/Documents/Skolearbeid/Maskinlæring/Group Task/submission notebooks/MLC/HenrikDNN_checkpoints/model-epoch=05-val_loss=0.04.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4616f21a9807406aaef4994f82aeb85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location C: 57.72308643838329\n",
      "new best score for C: 57.72308643838329\n"
     ]
    }
   ],
   "source": [
    "def trainDNN(letter):\n",
    "    X, y = pre.general_read(letter)\n",
    "    X = pre.concatenate_dfs(X)\n",
    "    X_train, y_train,X_val, y_val = pre.train_test_split_may_june_july(X,y , letter)\n",
    "    y_train = y_train[\"target\"]\n",
    "    y_val = y_val[\"target\"]\n",
    "\n",
    "    if letter == \"A\":\n",
    "\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [119,101], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.0000969995972939842, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.9702228408589507, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 2.499126185711371e-8 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_preprocessor = 'quarters' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_encoder = True #<----- from opta hyper parameter tuning\n",
    "    \n",
    "    elif letter == \"B\":\n",
    "        \n",
    "        dnn_params = {\n",
    "            'layer_sizes': [200,180], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000018846485346070986, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.963895316469423, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 6.586949775384596e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "        \n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'statistical'\n",
    "        dnn_target_encoder = False\n",
    "    \n",
    "    elif letter == \"C\":\n",
    "        #52.31801357204807\tquarters\ttrue\t144\t131\t0.000060363753946263044\t-1.5104000124283146\t2.335809622586189e-7\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [144,131], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000060363753946263044, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': -1.5104000124283146, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': True,\n",
    "            'weight_decay': 2.335809622586189e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'quarters'\n",
    "        dnn_target_encoder = True\n",
    "\n",
    "    dnn_feature_scaler = pre.choose_scaler(dnn_feature_scaler)\n",
    "    dnn_target_scaler = pre.choose_scaler(dnn_target_scaler)\n",
    "    dnn_preprocessor = pre.choose_transformer(dnn_preprocessor)\n",
    "    dnn_target_encoder = pre.choose_encoder(dnn_target_encoder)\n",
    "    \n",
    "    dnn_preprocessing = Pipeline([\n",
    "        ('custom_transformer', dnn_preprocessor),\n",
    "        ('target_encoder', dnn_target_encoder), \n",
    "        ('feature_scaler', dnn_feature_scaler)\n",
    "    ])\n",
    "\n",
    "    dnn_target_preprocessing = Pipeline([\n",
    "        ('target_scaler', dnn_target_scaler)\n",
    "    ])\n",
    "\n",
    "    #fit the preprocessing:\n",
    "    dnn_preprocessing.fit(X_train, y_train)\n",
    "    dnn_target_preprocessing.fit(pd.DataFrame(y_train))\n",
    "\n",
    "    #transform the data:\n",
    "    X_train_dnn = pd.DataFrame((dnn_preprocessing.transform(X_train)))\n",
    "    y_train_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_train)))\n",
    "    X_val_dnn = pd.DataFrame(dnn_preprocessing.transform(X_val))\n",
    "    y_val_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_val)))\n",
    "\n",
    "    #fit the model:\n",
    "    dnn_model = HenrikDNN(n_features =X_train_dnn.shape[1] , **dnn_params)\n",
    "    dnn_model.train(X_train_dnn, y_train_dnn, X_val_dnn, y_val_dnn)\n",
    "    #predict:\n",
    "    dnn_pred = dnn_model.predict(X_val_dnn)\n",
    "    #scale back:\n",
    "    dnn_pred = dnn_target_preprocessing.inverse_transform(dnn_pred).reshape(-1)\n",
    "\n",
    "    print(f\"MAE DNN location {letter}: {mean_absolute_error(y_val, dnn_pred)}\")\n",
    "\n",
    "    return dnn_model, dnn_preprocessing, dnn_target_preprocessing, mean_absolute_error(y_val, dnn_pred)\n",
    "\n",
    "\n",
    "models = {\"A\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"B\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"C\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_iterations = 1\n",
    "\n",
    "\n",
    "for letter in [\"A\", \"B\", \"C\"]:\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"training {letter}, iteration {i+1} of {num_iterations}\")\n",
    "        model, preprocessor, target_preprocessor, score = trainDNN(letter)\n",
    "        if score < models[letter][\"best_score\"]:\n",
    "            print(f\"new best score for {letter}: {score}\")\n",
    "            models[letter][\"best_score\"] = score\n",
    "            models[letter][\"best_model\"] = model\n",
    "            models[letter][\"best_preprocessor\"] = preprocessor\n",
    "            models[letter][\"best_target_preprocessor\"] = target_preprocessor\n",
    "\n",
    "post.make_dnn_prediction(models[\"A\"][\"best_model\"],\n",
    "                         models[\"A\"][\"best_preprocessor\"],\n",
    "                         models[\"A\"][\"best_target_preprocessor\"],\n",
    "                         models[\"B\"][\"best_model\"],\n",
    "                         models[\"B\"][\"best_preprocessor\"],\n",
    "                         models[\"B\"][\"best_target_preprocessor\"],\n",
    "                         models[\"C\"][\"best_model\"],\n",
    "                         models[\"C\"][\"best_preprocessor\"],\n",
    "                         models[\"C\"][\"best_target_preprocessor\"],\n",
    "                         f\"{FOLDER_NAME}/DNN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
