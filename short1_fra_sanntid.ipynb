{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle: \"I love your NaN(s)\" - (public score:142.136697, jallasatck2.csv)\n",
    "\n",
    "Rory Fitzgerald, roryf (540995), \n",
    "\n",
    "Henrik Horpedal, henrhorp (564667)\n",
    "\n",
    "Peter Pham, phpham (527659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 69\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "FOLDER_NAME = \"SHORT_NOTEBOOK1_CSV_FILES\"\n",
    "GLOBAL_VERBOSE = False\n",
    "PREPROCESSORS = [\"quarters\",\"statistical\", \"trimmedMean\"]\n",
    "LETTERS = [\"A\", \"B\", \"C\"]\n",
    "if not os.path.exists(FOLDER_NAME):\n",
    "    os.makedirs(FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# using a class to mimic a namespace:\n",
    "class pre:\n",
    "\n",
    "    def clean_NaN(df):\n",
    "        df = df.copy()\n",
    "        df.dropna(subset=['target'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def remove_long_sequences(df, col_name, seq_len):\n",
    "        df = df.copy()\n",
    "        # Identify sequences of zeros\n",
    "        df['group'] = (df[col_name] != 0).cumsum()\n",
    "        df['group_count'] = df.groupby('group')[col_name].transform('count')\n",
    "        \n",
    "        # Create a mask to identify rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        mask = (df[col_name] == 0) & (df['group_count'] > seq_len) #& (df['is_in_shadow:idx'] < 1)\n",
    "        \n",
    "        # Remove rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        df_cleaned = df[~mask].drop(columns=['group', 'group_count'])\n",
    "        return df_cleaned.copy()\n",
    "\n",
    "\n",
    "    def remove_repeating_nonzero(df, col_name, repeat_count=5):\n",
    "        df = df.copy()\n",
    "        # create a mask to identify rows with repeating nonzero values in the target column\n",
    "        mask = ((df[col_name] != 0) & (df[col_name].shift(1) == df[col_name]))\n",
    "        # create a mask to identify rows with repeating nonzero values that occur more than repeat_count times\n",
    "        repeat_mask = mask & (mask.groupby((~mask).cumsum()).cumcount() >= repeat_count)\n",
    "        # create a mask to identify the complete sequence of repeating nonzero values\n",
    "        seq_mask = repeat_mask | repeat_mask.shift(-5)\n",
    "        # remove rows with repeating nonzero values that occur more than repeat_count times\n",
    "        df = df[~seq_mask]\n",
    "        return df\n",
    "\n",
    "    def clean(df):\n",
    "        df = df.copy()\n",
    "        df=pre.clean_NaN(df)\n",
    "        df=pre.remove_long_sequences(df, 'target', 60)\n",
    "        df=pre.remove_repeating_nonzero(df, 'target')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def encode(data, col, max_val):\n",
    "        data = data.copy()\n",
    "        data = data.copy()\n",
    "        data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "        data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "        return data\n",
    "\n",
    "    def create_time_features(df):\n",
    "        df = df.copy()\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        df[\"dayofyear\"]=df.index.dayofyear\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"week\"] = df.index.isocalendar().week\n",
    "\n",
    "        #zero indexing:\n",
    "        df[\"dayofyear\"]-=1\n",
    "        df[\"month\"]-=1\n",
    "        df[\"week\"]-=1\n",
    "\n",
    "\n",
    "        #Cycling the time features:\n",
    "        df = pre.encode(df, \"hour\", 24)\n",
    "        df = pre.encode(df, \"month\", 12)\n",
    "        df = pre.encode(df, \"week\", 53)\n",
    "        df = pre.encode(df, \"dayofyear\", 366)\n",
    "\n",
    "        df.drop(columns=[\"hour\", \"month\", \"week\", \"dayofyear\"], inplace=True)\n",
    "\n",
    "\n",
    "        df[\"mult1\"]=(1-df[\"is_in_shadow:idx\"])*df['direct_rad:W']\n",
    "        df[\"mult2\"]=(1-df[\"is_in_shadow:idx\"])*df['clear_sky_rad:W']\n",
    "        df[\"date_calc\"]=pd.to_datetime(df[\"date_calc\"])\n",
    "        df.index=pd.to_datetime(df.index)\n",
    "        df[\"uncertainty\"]=(df.index-df[\"date_calc\"]).apply(lambda x: x.total_seconds()/3600)\n",
    "        df[\"uncertainty\"].fillna(0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def create_features(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        df.dropna(subset=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "        df[\"total_solar_rad\"]=df[\"direct_rad:W\"]+df[\"diffuse_rad:W\"]\n",
    "        #df[\"clear_sky_%\"]=df[\"total_solar_rad\"]/df[\"clear_sky_rad:W\"]*100\n",
    "        #df[\"clear_sky_%\"].fillna(0, inplace=True)\n",
    "        df[\"spec humid\"]=df[\"absolute_humidity_2m:gm3\"]/df[\"air_density_2m:kgm3\"]\n",
    "        df[\"temp*total_rad\"]=df[\"t_1000hPa:K\"]*df[\"total_solar_rad\"]\n",
    "        df[\"wind_angle\"]=(np.arctan2(df[\"wind_speed_u_10m:ms\"],df[\"wind_speed_v_10m:ms\"]))*180/np.pi\n",
    "        #df[\"total_snow_depth\"] = df[\"snow_depth:cm\"] + df[\"fresh_snow_1h:cm\"]\n",
    "        #df[\"total_precip_5min\"] = df[\"precip_5min:mm\"] + df[\"snow_melt_10min:mm\"]\n",
    "        #df[\"total_precip_type\"] = df[\"precip_type_5min:idx\"] + df[\"snow_water:kgm2\"]\n",
    "        df[\"total_pressure\"] = df[\"pressure_50m:hPa\"] + df[\"pressure_100m:hPa\"]\n",
    "        df[\"total_sun_angle\"] = df[\"sun_azimuth:d\"] + df[\"sun_elevation:d\"]\n",
    "        df[\"solar intensity\"]=1361*np.cos(np.radians(90-df[\"sun_elevation:d\"]))\n",
    "        df[\"solar intensity\"].clip(lower=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def shift_target(df, target_col):\n",
    "        df = df.copy()\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Store the original indices\n",
    "        original_indices = df.index\n",
    "\n",
    "        # Reindex the DataFrame to include all 15-minute intervals\n",
    "        all_intervals = pd.date_range(start=df.index.min(), end=df.index.max(), freq='15T')\n",
    "        df = df.reindex(all_intervals)\n",
    "\n",
    "        # Shift the target variable by 1 period (15 minutes) forward and backward\n",
    "        df[target_col + '_shifted_forward'] = df[target_col].shift(-1)\n",
    "        df[target_col + '_shifted_backward'] = df[target_col].shift(1)\n",
    "\n",
    "        # Forward fill the missing values for the forward shift\n",
    "        df[target_col + '_shifted_forward'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Backward fill the missing values for the backward shift\n",
    "        df[target_col + '_shifted_backward'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Keep only the original indices\n",
    "        df = df.loc[original_indices]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def add_lagged_features(df):\n",
    "        df = df.copy()\n",
    "        features_to_lag = [ \"total_solar_rad\", \"temptotal_rad\", \"clear_sky_radW\", \"diffuse_radW\", \"direct_radW\",  \"total_cloud_coverp\", \"solarintensity\", \"total_sun_angle\", \"pressure_100mhPa\"] #these were the most important features, and therefore assumed to be the most important to lag \n",
    "        \n",
    "        for feature in features_to_lag:\n",
    "            df = pre.shift_target(df, feature)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    #experiments showed that flaml preformed slightly better with fewer features, no lagged features:\n",
    "    def general_read_flaml(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        #X_final = add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "\n",
    "    def general_read(letter):\n",
    "\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        y.index = pd.to_datetime(y.index) \n",
    "        \n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        # truncate y to match the index of df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        latest_y_time = y.index[-1]\n",
    "        latest_needed_df_time = latest_y_time + pd.Timedelta(minutes=45)\n",
    "        # Truncate y based on df\n",
    "        y = y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        # Ensure df has all needed entries from the start of y to 45 minutes after the end of y\n",
    "        df = df.truncate(before=y.index[0], after=latest_needed_df_time)\n",
    "        y.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "        X = df.copy()\n",
    "        Y = y.copy()\n",
    "        #drop nan rows in Y\n",
    "        Y = pre.clean(Y)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        Y.index = pd.to_datetime(Y.index)\n",
    "\n",
    "        X_filtered = X[X.index.floor('H').isin(Y.index)]\n",
    "\n",
    "        # Step 2: Ensure there are exactly four 15-min intervals for each hour\n",
    "        valid_indices = X_filtered.groupby(X_filtered.index.floor('H')).filter(lambda group: len(group) == 4).index\n",
    "\n",
    "        # Final filtered X\n",
    "        X_final = X[X.index.isin(valid_indices)]\n",
    "\n",
    "\n",
    "        #Troubleshooting: Find and print the hours with a mismatch\n",
    "        group_sizes = X_filtered.groupby(X_filtered.index.floor('H')).size()\n",
    "        mismatch_hours = group_sizes[group_sizes != 4]\n",
    "\n",
    "        #Additional troubleshooting: find hours in Y without four 15-min intervals in X\n",
    "        missing_hours_in_x = Y.index[~Y.index.isin(X_filtered.index.floor('H'))]\n",
    "\n",
    "\n",
    "        #Remove mismatched and missing hours from Y\n",
    "        all_issues = mismatch_hours.index.union(missing_hours_in_x)\n",
    "        Y_clean = Y[~Y.index.isin(all_issues)]\n",
    "\n",
    "        #dropping nan columns:\n",
    "        X_final = X_final.drop(columns=['cloud_base_agl:m'])\n",
    "        X_final = X_final.drop(columns=['ceiling_height_agl:m'])\n",
    "        X_final = X_final.drop(columns=['snow_density:kgm3'])\n",
    "\n",
    "        X_final = pre.create_features(X_final)\n",
    "        X_final = pre.create_time_features(X_final)\n",
    "        X_final.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "        X_final = X_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        Y_clean = Y_clean.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        X_final = pre.add_lagged_features(X_final)\n",
    "\n",
    "        # Split X_final into a list of 4-row DataFrames\n",
    "        X_grouped = [group for _, group in X_final.groupby(X_final.index.floor('H')) if len(group) == 4]\n",
    "        \n",
    "        # Ensure we only take the groups of X corresponding to Y_clean\n",
    "        X_list = [X_grouped[i] for i in range(len(Y_clean))]\n",
    "\n",
    "        return X_list, Y_clean\n",
    "\n",
    "\n",
    "    #the general read function returns a list of 4-row DataFrames, that was in order to have the same lenght of X and Y, therefore this function was needed.\n",
    "    def concatenate_dfs(df_list):\n",
    "        \"\"\"\n",
    "        Concatenates a list of DataFrames into a single DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df_list (list of pd.DataFrame): List of DataFrame objects to concatenate.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A single DataFrame containing all rows from the input DataFrames in the order they appear in the list.\n",
    "        \"\"\"\n",
    "        return pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "    #preprocessor which could be used in a pipeline:\n",
    "    class QuartersAsColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None):\n",
    "            # Ensure input is a DataFrame\n",
    "            X = X.copy()\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            #make sure index is datetime:\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "\n",
    "            original_index = X.index\n",
    "\n",
    "\n",
    "            X['hour'] = X.index.floor('H')\n",
    "            X['minute'] = X.index.minute\n",
    "\n",
    "            # Melt the DataFrame to long format\n",
    "            df_melted = pd.melt(X, id_vars=['hour', 'minute'], value_vars=X.columns[:-2]).copy()  # excluding 'hour' and 'minute'\n",
    "\n",
    "            # Create a multi-level column name combining variable and minute\n",
    "            df_melted['variable_minute'] = df_melted['variable'] + '_' + df_melted['minute'].astype(str) + 'min'\n",
    "\n",
    "            # Drop the 'variable_minute' column\n",
    "\n",
    "\n",
    "            # Pivot the data to get one row per hour and columns for each variable and minute\n",
    "            X = df_melted.pivot(index='hour', columns='variable_minute', values='value').copy()\n",
    "            #rename index to date_forecast:\n",
    "            X.index.rename(\"date_forecast\", inplace=True)\n",
    "\n",
    "\n",
    "            #drop irrelevant columns:\n",
    "            #hour_sin\thour_cos\tmonth_sin\tmonth_cos\tweek_sin\tweek_cos\tdayofyear_sin\tdayofyear_cos\tmult1\tmult2\tuncertainty\n",
    "\n",
    "\n",
    "            irrelevant_cols = [\"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\", \"week_sin\", \"week_cos\", \"dayofyear_sin\", \"dayofyear_cos\", \"uncertainty\"]\n",
    "            variantes = [\"_0min\", \"_15min\", \"_30min\", \"_45min\"]\n",
    "            for variant in variantes:\n",
    "                for col in irrelevant_cols:\n",
    "                    if variant == \"_0min\":\n",
    "                        #remove _0min from column name;\n",
    "                        X.rename(columns={col+variant:col}, inplace=True)\n",
    "                    else:\n",
    "                        X.drop(columns=[col+variant], inplace=True)\n",
    "            \n",
    "\n",
    "            reindex_map = original_index.floor('H').unique()\n",
    "            X = X.reindex(reindex_map)\n",
    "            X.index = reindex_map\n",
    "\n",
    "            #drop hour_\n",
    "\n",
    "            if \"object\" in X.dtypes.unique():\n",
    "                print(\"waring: object in QuarterAsColumnsTransformer\")\n",
    "                print(X.dtypes.unique())\n",
    "                for col in X.columns:\n",
    "                    print(col)\n",
    "\n",
    "            #X = X.select_dtypes(include=[np.number])\n",
    "            return X\n",
    "        \n",
    "    #preprocessor which could be used in a pipeline:\n",
    "    class StatisticalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X, y=None):\n",
    "            X_copy = X.copy()\n",
    "            X_copy.index = pd.to_datetime(X_copy.index)\n",
    "            X_copy['hour'] = X_copy.index.floor('H')\n",
    "            \n",
    "            # Compute mean, std\n",
    "            aggregated = X_copy.groupby('hour').agg(['mean', 'std'])\n",
    "            \n",
    "            # Filter hours with exactly 4 data points\n",
    "            valid_hours = X_copy.groupby('hour').size()\n",
    "            valid_hours = valid_hours[valid_hours == 4].index\n",
    "            \n",
    "            X_final = aggregated.loc[valid_hours]\n",
    "            \n",
    "            # Flatten the multi-index to form new column names\n",
    "            X_final.columns = ['_'.join(col).strip() for col in X_final.columns.values]\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "            #drop minute_mean and minute_std if they exist:\n",
    "            if \"minute_mean\" in X_final.columns:\n",
    "                X_final.drop(columns=[\"minute_mean\", \"minute_std\"], inplace=True)\n",
    "            \n",
    "            X_final = X_final.select_dtypes(include=[np.number])\n",
    "            # print(X_final.dtypes.unique())\n",
    "            # for col in X_final.columns:\n",
    "            #     print(col)\n",
    "\n",
    "\n",
    "            return X_final\n",
    "        \n",
    "    \n",
    "\n",
    "    class HourMonthTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.encoding_map = {}\n",
    "            self.y_ = None  # To store y during fit\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"y cannot be None for fitting the encoder\")\n",
    "\n",
    "            # Store the target values for encoding later\n",
    "            self.y_ = y\n",
    "\n",
    "            try:\n",
    "                # Extract hour and month from the index and use y provided during fit\n",
    "                df = pd.DataFrame({'target': self.y_, 'hour': X.index.hour, 'month': X.index.month})\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "            # Compute mean target value for each hour of each month\n",
    "            self.encoding_map = df.groupby(['month', 'hour'])['target'].mean().to_dict()\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            # Ensure X's index is a datetime index\n",
    "            if not isinstance(X.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Index of input X must be a pandas DatetimeIndex\")\n",
    "\n",
    "            if self.y_ is None:\n",
    "                raise ValueError(\"The encoder has not been fitted with target values\")\n",
    "\n",
    "            # Extract hour and month from the index\n",
    "            X_transformed = X.copy()\n",
    "            X_transformed['hour'] = X.index.hour\n",
    "            X_transformed['month'] = X.index.month\n",
    "\n",
    "            # Map the mean target values\n",
    "            X_transformed['target_encoded'] = X_transformed.apply(\n",
    "                lambda row: self.encoding_map.get((row['month'], row['hour']), np.nan), axis=1)\n",
    "\n",
    "            # Optionally drop 'hour' and 'month' if they're not needed\n",
    "            X_transformed.drop(['hour', 'month'], axis=1, inplace=True)\n",
    "\n",
    "            # Check for object dtypes and print warning if any\n",
    "            if \"object\" in X_transformed.dtypes.values:\n",
    "                print(\"Warning: object dtype in HourMonthTargetEncoder\")\n",
    "                print(X_transformed.dtypes)\n",
    "\n",
    "            # Ensure that only numeric types are returned\n",
    "            X_transformed = X_transformed.select_dtypes(include=[np.number])\n",
    "\n",
    "            return X_transformed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_preprocessor(data, preprocessor_name):\n",
    "        data = data.copy()\n",
    "        # Assuming `pre.choose_transformer` returns a callable object that can be used to transform the data\n",
    "        preprocessor = pre.choose_transformer(preprocessor_name)\n",
    "        return preprocessor.transform(data)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def train_test_split_may_june_july(X, y,letter):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        return X_train, y_train, X_may_june_july, y_may_june_july\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def new_train_test_split(X, y,letter, split_date):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        split_date = pd.Timestamp(split_date).normalize()\n",
    "        print(f\"Split date: {split_date}\")\n",
    "\n",
    "        if isinstance(split_date, str):\n",
    "            split_date = pd.Timestamp(split_date)\n",
    "        if letter == \"A\":\n",
    "            year = 2022\n",
    "        elif letter == \"B\":\n",
    "            year = 2019\n",
    "        elif letter == \"C\":\n",
    "            year = 2020\n",
    "\n",
    "        X_train = X[X.index.normalize() < split_date]\n",
    "        y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "        X_test = X[X.index.normalize() >= split_date]\n",
    "        y_test = y[y.index.normalize() >= split_date]\n",
    "        \n",
    "        # Define conditions to move May and June of split_date's year from train to test\n",
    "        may_june_july_condition_X = ((X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)) & ((X.index.year == year))\n",
    "        may_june_july_condition_y = ((y.index.month == 5) | (y.index.month == 6) | (y.index.month == 7)) & ((y.index.year == year))\n",
    "        \n",
    "        X_may_june_july = X[may_june_july_condition_X]\n",
    "        y_may_june_july = y[may_june_july_condition_y]\n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        X_train = X[~may_june_july_condition_X]\n",
    "        y_train = y[~may_june_july_condition_y]\n",
    "\n",
    "        # Append May and June data to test set\n",
    "        X_test = pd.concat([X_may_june_july, X_test])\n",
    "        y_test = pd.concat([y_may_june_july, y_test])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "    def choose_scaler(scaler_string):\n",
    "        if scaler_string == \"minmax\":\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_string == \"standard\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_string == \"robust\":\n",
    "            return RobustScaler()\n",
    "\n",
    "    def choose_transformer(transformer_string):\n",
    "        if transformer_string == \"quarters\":\n",
    "            return pre.QuartersAsColumnsTransformer()\n",
    "        elif transformer_string == \"statistical\":\n",
    "            return pre.StatisticalFeaturesTransformer()\n",
    "        elif transformer_string == \"trimmedMean\":\n",
    "            return pre.TrimmedMeanTransformer()\n",
    "\n",
    "    def choose_encoder(encoder_boolian):\n",
    "        if encoder_boolian == True:\n",
    "            return pre.HourMonthTargetEncoder()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def generate_predefined_split(X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        This function takes in separate training and validation datasets, combines them,\n",
    "        and creates a PredefinedSplit object that can be used with sklearn's GridSearchCV\n",
    "        or other model selection utilities. This allows for specifying which samples are\n",
    "        used for training and which are used for validation.\n",
    "\n",
    "        Parameters:\n",
    "        X_train (array-like): Training features.\n",
    "        X_val (array-like): Validation features.\n",
    "        y_train (array-like): Training labels.\n",
    "        y_val (array-like): Validation labels.\n",
    "\n",
    "        Returns:\n",
    "        X (array-like): The combined dataset of features.\n",
    "        y (array-like): The combined dataset of labels.\n",
    "        split_index (PredefinedSplit): An instance of PredefinedSplit with the indices set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Combine the training and validation sets\n",
    "        X = np.concatenate((X_train, X_val), axis=0)\n",
    "        y = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "        # Generate the indices array where -1 indicates the sample is part of the training set,\n",
    "        # and 0 indicates the sample is part of the validation set.\n",
    "        train_indices = -1 * np.ones(len(X_train))\n",
    "        val_indices = 0 * np.ones(len(X_val))\n",
    "        test_fold = np.concatenate((train_indices, val_indices))\n",
    "\n",
    "        # Create the PredefinedSplit object\n",
    "        predefined_split = PredefinedSplit(test_fold)\n",
    "\n",
    "        return X, y, predefined_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ensPre:\n",
    "    def preptest(df,features_list,stats_dict):\n",
    "        df.drop(columns=[\"ceiling_height_agl:m\",\"snow_density:kgm3\",\"cloud_base_agl:m\"],inplace=True)\n",
    "        df=ensPre.create_features(df)\n",
    "        #df=unleash_hell(df)\n",
    "        df=ensPre.create_time_features(df)\n",
    "        #df=df[features_list[:-1]]\n",
    "        df=ensPre.cyclical_features(df)\n",
    "        df=ensPre.add_stats_features(df, stats_dict)\n",
    "        df=ensPre.shift_target(df,\"total_solar_rad\")\n",
    "        df.dropna(inplace=True)\n",
    "        df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "        return df\n",
    "    def readDataSet(letter):\n",
    "        # read X_train_observed.parquet file for the current letter\n",
    "        df = pd.read_parquet(f\"{letter}/X_train_observed.parquet\")\n",
    "        \n",
    "        df2=pd.read_parquet(f\"{letter}/X_train_estimated.parquet\")\n",
    "        \n",
    "        # set the index to date_forecast and group by hourly frequency\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.groupby(df.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        df2.set_index(\"date_forecast\", inplace=True)\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "        df2 = df2.groupby(df2.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        y = pd.read_parquet(f\"{letter}/train_targets.parquet\")\n",
    "        y.set_index(\"time\", inplace=True)\n",
    "        y.index = pd.to_datetime(y.index)\n",
    "\n",
    "        df=pd.concat([df,df2],axis=0)\n",
    "\n",
    "        df = df.truncate(before=y.index[0], after=y.index[-1])\n",
    "        y=y.truncate(before=df.index[0], after=df.index[-1])\n",
    "        df=pd.concat([df,y],axis=1)\n",
    "        df.rename(columns={\"pv_measurement\":\"target\"},inplace=True)\n",
    "\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def readtest(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_test_estimated.parquet\")\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.groupby(df.index.date).resample('H').max().reset_index(level=0, drop=True)\n",
    "        return df\n",
    "    def cleanNaN(df):\n",
    "        df.dropna(subset=['target'], inplace=True)\n",
    "        return df\n",
    "    # define a function to remove rows with sequences longer than 50 zeros\n",
    "    #remove NaN first to avoid NaN 0 sequence not being removed\n",
    "    def remove_long_sequences(df, col_name, seq_len):\n",
    "        # Identify sequences of zeros\n",
    "        df['group'] = (df[col_name] != 0).cumsum()\n",
    "        df['group_count'] = df.groupby('group')[col_name].transform('count')\n",
    "        \n",
    "        # Create a mask to identify rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        mask = (df[col_name] == 0) & (df['group_count'] > seq_len) & (df['is_in_shadow:idx'] < 1)\n",
    "        \n",
    "        # Remove rows with sequences longer than seq_len and isshadow lower than 1\n",
    "        df_cleaned = df[~mask].drop(columns=['group', 'group_count'])\n",
    "        return df_cleaned.copy()\n",
    "    \n",
    "    def remove_repeating_nonzero(df, col_name, repeat_count=5):\n",
    "        # create a mask to identify rows with repeating nonzero values in the target column\n",
    "        mask = ((df[col_name] != 0) & (df[col_name].shift(1) == df[col_name]))\n",
    "        # create a mask to identify rows with repeating nonzero values that occur more than repeat_count times\n",
    "        repeat_mask = mask & (mask.groupby((~mask).cumsum()).cumcount() >= repeat_count)\n",
    "        # create a mask to identify the complete sequence of repeating nonzero values\n",
    "        seq_mask = repeat_mask | repeat_mask.shift(-5)\n",
    "        # remove rows with repeating nonzero values that occur more than repeat_count times\n",
    "        df = df[~seq_mask]\n",
    "        return df\n",
    "    def clean(df):\n",
    "        df=ensPre.cleanNaN(df)\n",
    "        df=ensPre.remove_long_sequences(df, 'target', 30)\n",
    "        df=ensPre.remove_repeating_nonzero(df, 'target')\n",
    "        #df=remove_outliers_hourly_and_month(df, 'target')\n",
    "        return df\n",
    "    def remove_outliers_hourly_and_month(df, target_col):\n",
    "        for month in range (1,13):\n",
    "            df_month = df[df.index.month == month]\n",
    "            for i in range(24):\n",
    "                df_hour=df_month[df_month.index.hour == i]\n",
    "                Q1 = df_hour[target_col].quantile(0.25)\n",
    "                Q3 = df_hour[target_col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df_hour_rem = df_hour[(df_hour[target_col] >= lower_bound) & (df_hour[target_col] <= upper_bound)]\n",
    "                if i==0:\n",
    "                    df_rem=df_hour_rem\n",
    "                else:\n",
    "                    df_rem=pd.concat([df_rem,df_hour_rem],axis=0)\n",
    "            if month==1:\n",
    "                df_rem_month=df_rem\n",
    "            else:\n",
    "                df_rem_month=pd.concat([df_rem_month,df_rem],axis=0)\n",
    "        df_rem_month.sort_index(inplace=True)\n",
    "        return df_rem_month\n",
    "    def calculate_hourly_monthly_means(df):\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Create new columns for the hour and the month\n",
    "        df['hour'] = df.index.hour\n",
    "        df['month'] = df.index.month\n",
    "\n",
    "        # Calculate the mean of the target variable for each hour during each month\n",
    "        stats = df.groupby(['month', 'hour'])['target'].agg(['mean', 'std'])\n",
    "\n",
    "        # Convert the DataFrame to a dictionary of dictionaries\n",
    "        stats_dict = stats.to_dict('index')\n",
    "\n",
    "        return stats_dict\n",
    "\n",
    "    def add_stats_features(df, stats_dict):\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        df['mean'] = df.apply(lambda row: stats_dict[(row['month'], row['hour'])]['mean'], axis=1)\n",
    "        df['std'] = df.apply(lambda row: stats_dict[(row['month'], row['hour'])]['std'], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cyclical_features(df):\n",
    "        df[\"hour_sin\"] = np.sin(df.index.hour*(2.*np.pi/24))\n",
    "        df[\"hour_cos\"] = np.cos(df.index.hour*(2.*np.pi/24))\n",
    "        df[\"dayofyear_sin\"] = np.sin(df.index.dayofyear*(2.*np.pi/365))\n",
    "        df[\"dayofyear_cos\"] = np.cos(df.index.dayofyear*(2.*np.pi/365))\n",
    "        return df.copy()\n",
    "        \n",
    "\n",
    "\n",
    "    def create_time_features(df):\n",
    "        df[\"hour\"]=df.index.hour\n",
    "        #df[\"week\"]=df.index.isocalendar().week\n",
    "        df[\"dayofyear\"]=df.index.dayofyear\n",
    "        df[\"month\"]=df.index.month\n",
    "        df[\"mult1\"]=(1-df[\"is_in_shadow:idx\"])*df['direct_rad:W']\n",
    "        df[\"mult2\"]=(1-df[\"is_in_shadow:idx\"])*df['clear_sky_rad:W']\n",
    "        df[\"date_calc\"]=pd.to_datetime(df[\"date_calc\"])\n",
    "        df.index=pd.to_datetime(df.index)\n",
    "        df[\"uncertainty\"]=(df.index-df[\"date_calc\"]).apply(lambda x: x.total_seconds()/3600)\n",
    "        df[\"uncertainty\"].fillna(0, inplace=True)\n",
    "        df.drop(columns=[\"date_calc\"],inplace=True)\n",
    "        return df.copy()\n",
    "\n",
    "    def trim(df,features_list):\n",
    "        df.drop(columns=[\"ceiling_height_agl:m\",\"snow_density:kgm3\",\"cloud_base_agl:m\"],inplace=True)\n",
    "        df=ensPre.clean(df)\n",
    "        df=ensPre.create_features(df)\n",
    "        #df=unleash_hell(df)\n",
    "        df=ensPre.create_time_features(df)\n",
    "        #df=df[features_list]\n",
    "        df=ensPre.cyclical_features(df)\n",
    "        stats_dict=ensPre.calculate_hourly_monthly_means(df)\n",
    "        df=ensPre.add_stats_features(df, stats_dict)\n",
    "        df=ensPre.shift_target(df,\"total_solar_rad\")\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "        #df = df[~df.index.month.isin([1, 2, 11, 12])]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_features(df):\n",
    "\n",
    "        df.dropna(subset=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "\n",
    "        df[\"total_solar_rad\"]=df[\"direct_rad:W\"]+df[\"diffuse_rad:W\"]\n",
    "\n",
    "        #df[\"clear_sky_%\"]=df[\"total_solar_rad\"]/df[\"clear_sky_rad:W\"]*100\n",
    "        #df[\"clear_sky_%\"].fillna(0, inplace=True)\n",
    "        df[\"spec humid\"]=df[\"absolute_humidity_2m:gm3\"]/df[\"air_density_2m:kgm3\"]\n",
    "        df[\"temp*total_rad\"]=df[\"t_1000hPa:K\"]*df[\"total_solar_rad\"]\n",
    "        df[\"wind_angle\"]=(np.arctan2(df[\"wind_speed_u_10m:ms\"],df[\"wind_speed_v_10m:ms\"]))*180/np.pi\n",
    "        #df[\"total_snow_depth\"] = df[\"snow_depth:cm\"] + df[\"fresh_snow_1h:cm\"]\n",
    "        #df[\"total_precip_5min\"] = df[\"precip_5min:mm\"] + df[\"snow_melt_10min:mm\"]\n",
    "        #df[\"total_precip_type\"] = df[\"precip_type_5min:idx\"] + df[\"snow_water:kgm2\"]\n",
    "        df[\"total_pressure\"] = df[\"pressure_50m:hPa\"] + df[\"pressure_100m:hPa\"]\n",
    "        df[\"total_sun_angle\"] = df[\"sun_azimuth:d\"] + df[\"sun_elevation:d\"]\n",
    "        df[\"solar intensity\"]=1361*np.cos(np.radians(90-df[\"sun_elevation:d\"]))\n",
    "        df[\"solar intensity\"].clip(lower=0, inplace=True)\n",
    "        df[\"cloud\"]=df[\"clear_sky_rad:W\"]-df[\"total_solar_rad\"]\n",
    "        df[\"cloud2\"]=df[\"clear_sky_rad:W\"]*(1-df[\"effective_cloud_cover:p\"])\n",
    "        df['temp*total_rad_squared'] = df['temp*total_rad']**2\n",
    "        df['total_solar_rad_squared']=df['total_solar_rad']**2\n",
    "        df['t_1000hPa:K_daystd']=df['t_1000hPa:K'].groupby(df.index.date).transform('std')\n",
    "        df['wind_angle_squared']=df['wind_angle']**2\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    def unleash_hell(df):\n",
    "        cols=df.columns\n",
    "        new_cols=[]\n",
    "        for col in cols:\n",
    "            if col==\"target\" or col==\"date_calc\":\n",
    "                continue\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('max').rename(f\"{col}_daymax\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('min').rename(f\"{col}_daymin\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('mean').rename(f\"{col}_daymean\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('std').rename(f\"{col}_daystd\"))\n",
    "            new_cols.append(df[col].groupby(df.index.date).transform('sum').rename(f\"{col}_daysum\"))\n",
    "            #new_cols.append(pd.Series(np.log(1+df[col].values + 1e-8), index=df.index, name=f\"{col}_log\")) # convert numpy array to pandas Series\n",
    "            new_cols.append((df[col]**2).rename(f\"{col}_squared\"))\n",
    "        \"\"\"\n",
    "        for col1, col2 in itertools.combinations(cols, 2):\n",
    "            new_cols.append((df[col1] + df[col2]).rename(f\"{col1}+{col2}\"))\n",
    "            new_cols.append((df[col1] - df[col2]).rename(f\"{col1}-{col2}\"))\n",
    "            new_cols.append((df[col1] * df[col2]).rename(f\"{col1}*{col2}\"))\n",
    "            new_cols.append((df[col1] / df[col2]).rename(f\"{col1}/{col2}\"))\n",
    "        \"\"\"\n",
    "        df = pd.concat([df] + new_cols, axis=1)\n",
    "        return df.copy()\n",
    "            \n",
    "    def shift_target(df, target_col):\n",
    "        # Ensure the DataFrame is indexed by date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Store the original indices\n",
    "        original_indices = df.index\n",
    "\n",
    "        # Reindex the DataFrame to include all hours\n",
    "        all_hours = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "        df = df.reindex(all_hours)\n",
    "\n",
    "        # Shift the target variable by 1 hour forward and backward\n",
    "        df[target_col + '_shifted_forward'] = df[target_col].shift(-1)\n",
    "        df[target_col + '_shifted_backward'] = df[target_col].shift(1)\n",
    "\n",
    "        # Forward fill the missing values for the forward shift\n",
    "        df[target_col + '_shifted_forward'].ffill(inplace=True)\n",
    "\n",
    "        # Backward fill the missing values for the backward shift\n",
    "        df[target_col + '_shifted_backward'].bfill(inplace=True)\n",
    "\n",
    "        # Keep only the original indices\n",
    "        df = df.loc[original_indices]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def new_train_test_split(X, y,letter,holdout=False):\n",
    "        \"\"\"\n",
    "        Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "        from training set to test set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Quarter-hourly input data with DateTime index.\n",
    "        - y: Hourly target data with DateTime index.\n",
    "        - split_date: Date (string or datetime object) to split the data on.\n",
    "        \n",
    "        Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        Len=len(X)\n",
    "        Lensum=len(X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)])\n",
    "        X=pd.concat([X,y],axis=1)\n",
    "    \n",
    "\n",
    "        # Convert index to DatetimeIndex object\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "\n",
    "        num_samples = int(0.05 * len(X))\n",
    "        fraction = num_samples / len(X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)])\n",
    "        X_test = X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)].sample(frac=fraction, random_state=69)\n",
    "\n",
    "        X = X.drop(X_test.index)\n",
    "\n",
    "        if holdout:\n",
    "            X_holdout=X[(X.index.month == 5) | (X.index.month == 6) | (X.index.month == 7)].sample(frac=fraction*2, random_state=69)\n",
    "            X_train = X.drop(X_holdout.index)\n",
    "            y_holdout=X_holdout[\"target\"]\n",
    "            X_holdout=X_holdout.drop(columns=[\"target\"])\n",
    "        else:\n",
    "            X_train = X\n",
    "            y_holdout = None\n",
    "            X_holdout = None\n",
    "\n",
    "        \n",
    "\n",
    "        # Remove May and June data from training set\n",
    "        y_train=X_train[\"target\"]\n",
    "        y_test=X_test[\"target\"]\n",
    "        X_train=X_train.drop(columns=[\"target\"])\n",
    "        X_test=X_test.drop(columns=[\"target\"])\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, X_holdout, y_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "\n",
    "class post:    \n",
    "    def readRawTest(letter):\n",
    "        df = pd.read_parquet(f\"{letter}/X_test_estimated.parquet\")\n",
    "        df.set_index(\"date_forecast\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "\n",
    "    def readAndBasicPreprocess(letter):\n",
    "        X = post.readRawTest(letter)\n",
    "        X.drop(columns=['cloud_base_agl:m'], inplace=True)\n",
    "        X.drop(columns=['ceiling_height_agl:m'], inplace=True)\n",
    "        X.drop(columns=['snow_density:kgm3'], inplace=True)\n",
    "        X=pre.create_features(X)\n",
    "        X=pre.create_time_features(X)\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "        X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        X = pre.add_lagged_features(X)\n",
    "        return X\n",
    "\n",
    "    def makePrediction(A_model, B_model, C_model, filename):\n",
    "        A_x_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_y_pred=A_model.predict(A_x_test)\n",
    "        A_y_pred=pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_x_test= post.readAndBasicPreprocess(\"B\")\n",
    "        B_y_pred=B_model.predict(B_x_test)\n",
    "        B_y_pred=pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_x_test= post.readAndBasicPreprocess(\"C\")\n",
    "        C_y_pred=C_model.predict(C_x_test)\n",
    "        C_y_pred=pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "    \n",
    "    def make_dnn_prediction(A_model, A_preprocessing, A_target_scaling, B_model, B_preprocessing, B_target_scaling, C_model, C_preprocessing, C_target_scaling, filename):\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test_dnn = pd.DataFrame(A_preprocessing.transform(A_X_test))\n",
    "        A_y_pred_dnn = A_model.predict(A_X_test_dnn)\n",
    "        A_y_pred_dnn = A_target_scaling.inverse_transform(A_y_pred_dnn).reshape(-1)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred_dnn, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test_dnn = pd.DataFrame(B_preprocessing.transform(B_X_test))\n",
    "        B_y_pred_dnn = B_model.predict(B_X_test_dnn)\n",
    "        B_y_pred_dnn = B_target_scaling.inverse_transform(B_y_pred_dnn).reshape(-1)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred_dnn, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test_dnn = pd.DataFrame(C_preprocessing.transform(C_X_test))\n",
    "        C_y_pred_dnn = C_model.predict(C_X_test_dnn)\n",
    "        C_y_pred_dnn = C_target_scaling.inverse_transform(C_y_pred_dnn).reshape(-1)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred_dnn, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "    def compute_mae(ser1, ser2):\n",
    "        \"\"\"Compute Mean Absolute Error between two Series.\"\"\"\n",
    "        return np.abs(ser1 - ser2).mean()\n",
    "\n",
    "    def plot_mae_grid(dataframes_dict):\n",
    "        \"\"\"Plot a grid of MAE values for a dictionary of DataFrames.\"\"\"\n",
    "        \n",
    "        labels = list(dataframes_dict.keys())\n",
    "        dataframes = list(dataframes_dict.values())\n",
    "        n = len(dataframes)\n",
    "        \n",
    "        mae_grid = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    mae_grid[i][j] = post.compute_mae(dataframes[i][\"prediction\"], dataframes[j][\"prediction\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        cax = ax.matshow(mae_grid, cmap=\"viridis\")\n",
    "        \n",
    "        ax.grid(False)\n",
    "        plt.xticks(range(n), labels, rotation=45)\n",
    "        plt.yticks(range(n), labels)\n",
    "        \n",
    "        # Add annotations\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                text = ax.text(j, i, f\"{mae_grid[i, j]:.2f}\",\n",
    "                            ha=\"center\", va=\"center\", color=\"w\" if mae_grid[i, j] > (mae_grid.max() / 2) else \"black\")\n",
    "        \n",
    "        plt.colorbar(cax)\n",
    "        plt.title('MAE Between DataFrames on \"prediction\" Column', pad=20)\n",
    "        plt.show()\n",
    "\n",
    "    def makePredictionWithModelAndPreprocessor(A_model, B_model, C_model, preprocessor, filename):\n",
    "        \"\"\"\n",
    "        Assumes same preprocessing for all locations\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A_X_test = post.readAndBasicPreprocess(\"A\")\n",
    "        A_X_test = preprocessor.transform(A_X_test)\n",
    "        A_y_pred = A_model.predict(A_X_test)\n",
    "        A_y_pred = pd.DataFrame(A_y_pred, index=range(0,720), columns=['prediction'])\n",
    "\n",
    "        B_X_test = post.readAndBasicPreprocess(\"B\")\n",
    "        B_X_test = preprocessor.transform(B_X_test)\n",
    "        B_y_pred = B_model.predict(B_X_test)\n",
    "        B_y_pred = pd.DataFrame(B_y_pred, index=range(720,1440), columns=['prediction'])\n",
    "\n",
    "        C_X_test = post.readAndBasicPreprocess(\"C\")\n",
    "        C_X_test = preprocessor.transform(C_X_test)\n",
    "        C_y_pred = C_model.predict(C_X_test)\n",
    "        C_y_pred = pd.DataFrame(C_y_pred, index=range(1440,2160), columns=['prediction'])\n",
    "\n",
    "        combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "        combined_pred[\"prediction\"] = combined_pred[\"prediction\"].clip(lower=0)\n",
    "        combined_pred.index.name = \"id\"\n",
    "        combined_pred.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "    def make_average_prediction(preds_dict,filename):\n",
    "        \"\"\"\n",
    "        Generates a prediction by taking the average of the predictions in preds_dict.\n",
    "        \"\"\"\n",
    "        lenght = len(preds_dict)\n",
    "        data = 0\n",
    "        for value in preds_dict.values():\n",
    "            data += value[\"prediction\"]\n",
    "        data = data / lenght\n",
    "        data = pd.DataFrame(data, columns=['prediction'])\n",
    "        data.index.name = \"id\"\n",
    "        data[\"prediction\"] = data['prediction'].apply(lambda x: 0 if x < 0.1 else x)\n",
    "        data.loc[(data.index % 24).isin([22, 23, 0]), \"prediction\"] = 0\n",
    "        data.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining The DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#impoting lightning:\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "#import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as DataSet\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "SEED = 69\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class SolarForecastingDataset(DataSet):\n",
    "    def __init__(self, features_df, target_series):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with features and target labels.\n",
    "\n",
    "        :param features_df: DataFrame containing the features.\n",
    "        :param target_series: Series containing the target labels.\n",
    "        \"\"\"\n",
    "        self.features = features_df\n",
    "        self.targets = target_series\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extracting the features and the target label for the given index\n",
    "        feature_vector = self.features.iloc[index].values\n",
    "        target_label = self.targets.iloc[index]\n",
    "\n",
    "        return {\n",
    "            \"feature_vector\": torch.tensor(feature_vector, dtype=torch.float),\n",
    "            \"target_label\": torch.tensor(target_label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class SolarForecastingDatasetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_features_df, train_targets_series, test_features_df, test_targets_series, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_features_df = train_features_df\n",
    "        self.train_targets_series = train_targets_series\n",
    "        self.test_features_df = test_features_df\n",
    "        self.test_targets_series = test_targets_series\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SolarForecastingDataset(self.train_features_df, self.train_targets_series)\n",
    "        self.test_dataset = SolarForecastingDataset(self.test_features_df, self.test_targets_series)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,worker_init_fn=seed_worker, generator=g, shuffle=True,)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "class FullyConnectedDNN(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, dropout_prob=0.1):\n",
    "        super(FullyConnectedDNN, self).__init__()\n",
    "        # Create fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)):\n",
    "            in_features = input_size if i == 0 else layer_sizes[i - 1]\n",
    "            out_features = layer_sizes[i]\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "        self.output_layer = nn.Linear(layer_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def weighted_mae_loss(input, target, exponent=1, constant=1):\n",
    "    assert input.size() == target.size()\n",
    "\n",
    "    # Calculate the absolute error\n",
    "    absolute_errors = torch.abs(input - target)\n",
    "\n",
    "    # Apply exponential scaling with a constant\n",
    "    adjusted_target = target + constant\n",
    "    weighted_errors = absolute_errors * (adjusted_target ** exponent)\n",
    "\n",
    "    return weighted_errors.mean()\n",
    "\n",
    "\n",
    "class SolarPowerProductionPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, output_size, weight_decay=1e-5, dropout_prob=0.1, learning_rate=0.01, verbose=True, loss_exponent=1.0, loss_beta=1.0):\n",
    "        super().__init__()\n",
    "        self.model = FullyConnectedDNN(input_size, layer_sizes, output_size, dropout_prob=dropout_prob)\n",
    "        self.criterion = self.criterion = lambda input, target: weighted_mae_loss(input, target, exponent=loss_exponent, constant=1)\n",
    "        #self.criterion = CustomMAELoss(loss_alpha, loss_beta)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"train_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"val_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "        loss, outputs = self(features, labels) \n",
    "        self.log(\"test_loss\", loss, prog_bar=self.verbose, logger=False)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        # Save the best model path to the pl_module\n",
    "        pl_module.best_model_path = self.best_model_path\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[\"feature_vector\"], batch[\"target_label\"]\n",
    "            predictions = model(features)[1]  \n",
    "            if not isinstance(predictions, torch.Tensor):\n",
    "                raise TypeError(\"Model output is not a tensor. Got type: {}\".format(type(predictions)))\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Check for tensor types before concatenation\n",
    "    if not all(isinstance(p, torch.Tensor) for p in all_predictions):\n",
    "        raise TypeError(\"Not all elements in predictions are tensors.\")\n",
    "\n",
    "    if not all(isinstance(l, torch.Tensor) for l in all_labels):\n",
    "        raise TypeError(\"Not all elements in labels are tensors.\")\n",
    "\n",
    "    all_predictions_tensor = torch.cat(all_predictions, dim=0)\n",
    "    all_labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    all_predictions_np = all_predictions_tensor.cpu().numpy()\n",
    "    all_labels_np = all_labels_tensor.cpu().numpy()\n",
    "    \n",
    "    return all_predictions_np, all_labels_np\n",
    "\n",
    "class HenrikDNN:\n",
    "\n",
    "    def __init__(self,n_features = None, layer_sizes = [100,50], output_size = 1, drop_out_prob = 0.1, learning_rate = 0.01, weight_decay = 1e-5, max_epochs = 100, paitience = 5, batch_size = 16, val_chack_interval = 1, pruning_callback = None, verbose = True, loss_expontent = 1):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.verbose = verbose\n",
    "        self.output_size = output_size\n",
    "        self.drop_out_prob = drop_out_prob\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_expontent = loss_expontent\n",
    "        self.paitience = paitience\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.val_chack_interval = val_chack_interval\n",
    "        self.pruning_callback = pruning_callback\n",
    "        SEED = 69\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "\n",
    "        self.pl_model = SolarPowerProductionPredictor(self.n_features, self.layer_sizes, self.output_size, weight_decay=self.weight_decay, dropout_prob=self.drop_out_prob, learning_rate=self.learning_rate, verbose=self.verbose, loss_exponent=self.loss_expontent)\n",
    "\n",
    "        self.checkpoint_callback = CustomModelCheckpoint(\n",
    "            dirpath='HenrikDNN_checkpoints',\n",
    "            save_top_k=1,\n",
    "            verbose=self.verbose,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            filename='model-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "\n",
    "        self.early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.paitience\n",
    "        )\n",
    "\n",
    "\n",
    "        self.callbacks = [self.early_stopping_callback, self.checkpoint_callback]\n",
    "        if self.pruning_callback is not None:\n",
    "            self.callbacks.append(self.pruning_callback)\n",
    "        seed_everything(69, workers=True)\n",
    "\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=self.callbacks,\n",
    "            enable_progress_bar=self.verbose,\n",
    "            accelerator=\"cpu\",\n",
    "            check_val_every_n_epoch = 2,\n",
    "            deterministic=True\n",
    "\n",
    "            #val_check_interval=self.val_chack_interval\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train: Training df with datetime index. \n",
    "            y_train: Training df, with datetime index. Each row in y_train corresponds to four rows in X_train.\n",
    "    \n",
    "        \"\"\"\n",
    "        #print the seed:\n",
    "        SEED = 69\n",
    "        print(\"hei\")\n",
    "        torch.manual_seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "\n",
    "        self.data_module = SolarForecastingDatasetDataModule(X_train, y_train, X_val, y_val, batch_size=self.batch_size)\n",
    "        self.trainer.fit(self.pl_model, self.data_module)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        trained_model = SolarPowerProductionPredictor.load_from_checkpoint(\n",
    "            self.pl_model.best_model_path,\n",
    "            input_size=self.n_features,\n",
    "            layer_sizes=self.layer_sizes,\n",
    "            output_size=self.output_size,\n",
    "            dropout_prob=self.drop_out_prob,\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            verbose=self.verbose,\n",
    "            loss_exponent=self.loss_expontent\n",
    "        )\n",
    "\n",
    "        X_dataloader = torch.utils.data.DataLoader(\n",
    "            SolarForecastingDataset(X, pd.Series(np.zeros(X.shape[0]))),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g\n",
    "        )\n",
    "\n",
    "        predictions, _ = get_predictions(trained_model, X_dataloader)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_on_specific_day_May_june(X, y, split_date):\n",
    "    \"\"\"\n",
    "    Splits the data based on a given date. Additionally, moves May, June and July data of split_date's year\n",
    "    from training set to test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Quarter-hourly input data with DateTime index.\n",
    "    - y: Hourly target data with DateTime index.\n",
    "    - split_date: Date (string or datetime object) to split the data on.\n",
    "    \n",
    "    Returns:\n",
    "    X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    split_date = pd.Timestamp(split_date).normalize()\n",
    "\n",
    "    # Ensure split_date is a datetime object\n",
    "    if isinstance(split_date, str):\n",
    "        split_date = pd.Timestamp(split_date)\n",
    "\n",
    "    print(f\"Split date: {split_date}\")\n",
    "\n",
    "    # Split the data based on the provided date\n",
    "    X_train = X[X.index.normalize() < split_date]\n",
    "    y_train = y[y.index.normalize() < split_date]\n",
    "\n",
    "    X_test = X[X.index.normalize() >= split_date]\n",
    "    y_test = y[y.index.normalize() >= split_date]\n",
    "\n",
    "    # Define conditions to move May and June of split_date's year from train to test\n",
    "    may_june_condition_X = ((X_train.index.month == 5) | (X_train.index.month == 6) | (X_train.index.month == 7)) & (X_train.index.year == split_date.year)\n",
    "    may_june_condition_y = ((y_train.index.month == 5) | (y_train.index.month == 6) | (y_train.index.month == 7)) & (y_train.index.year == split_date.year)\n",
    "    \n",
    "    X_may_june = X_train[may_june_condition_X]\n",
    "    y_may_june = y_train[may_june_condition_y]\n",
    "\n",
    "    # Remove May and June data from training set\n",
    "    X_train = X_train[~may_june_condition_X]\n",
    "    y_train = y_train[~may_june_condition_y]\n",
    "\n",
    "    # Append May and June data to test set\n",
    "    X_test = pd.concat([X_may_june, X_test])\n",
    "    y_test = pd.concat([y_may_june, y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training A, iteration 1 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 17:56:29.448479: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-12 17:56:29.476150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 17:56:29.891322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE DNN location A: 286.5414860651279\n",
      "new best score for A: 286.5414860651279\n",
      "training A, iteration 2 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 288.68237908470246\n",
      "training A, iteration 3 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 289.1929559876062\n",
      "training A, iteration 4 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 289.4235650342097\n",
      "training A, iteration 5 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 290.3333862642418\n",
      "training A, iteration 6 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 287.4641432328177\n",
      "training A, iteration 7 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 46.4 K\n",
      "--------------------------------------------\n",
      "46.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location A: 293.1902655576716\n",
      "training B, iteration 1 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 66.26056107655843\n",
      "new best score for B: 66.26056107655843\n",
      "training B, iteration 2 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 66.18475673490728\n",
      "new best score for B: 66.18475673490728\n",
      "training B, iteration 3 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 65.80722993227957\n",
      "new best score for B: 65.80722993227957\n",
      "training B, iteration 4 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 66.13701551589087\n",
      "training B, iteration 5 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 66.09520563131474\n",
      "training B, iteration 6 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 66.20781233880216\n",
      "training B, iteration 7 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 67.8 K\n",
      "--------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location B: 65.70001195192403\n",
      "new best score for B: 65.70001195192403\n",
      "training C, iteration 1 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 52.05439660056946\n",
      "new best score for C: 52.05439660056946\n",
      "training C, iteration 2 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 53.571651111596466\n",
      "training C, iteration 3 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 52.84903817735908\n",
      "training C, iteration 4 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 53.383526802383685\n",
      "training C, iteration 5 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 54.76077917599066\n",
      "training C, iteration 6 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 52.49614753744029\n",
      "training C, iteration 7 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory HenrikDNN_checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FullyConnectedDNN | 60.5 K\n",
      "--------------------------------------------\n",
      "60.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 K    Total params\n",
      "0.242     Total estimated model params size (MB)\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/student/Downloads/test_short_notebok/sui/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "MAE DNN location C: 52.6386769562155\n"
     ]
    }
   ],
   "source": [
    "def trainDNN(letter):\n",
    "    X, y = pre.general_read(letter)\n",
    "    X = pre.concatenate_dfs(X)\n",
    "    X_train, y_train,X_val, y_val = pre.train_test_split_may_june_july(X,y , letter)\n",
    "    y_train = y_train[\"target\"]\n",
    "    y_val = y_val[\"target\"]\n",
    "\n",
    "    if letter == \"A\":\n",
    "\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [119,101], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.0000969995972939842, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.9702228408589507, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': False,\n",
    "            'weight_decay': 2.499126185711371e-8 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_scaler = 'minmax' #<----- from opta hyper parameter tuning\n",
    "        dnn_preprocessor = 'quarters' #<----- from opta hyper parameter tuning\n",
    "        dnn_target_encoder = True #<----- from opta hyper parameter tuning\n",
    "    \n",
    "    elif letter == \"B\":\n",
    "        \n",
    "        dnn_params = {\n",
    "            'layer_sizes': [200,180], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000018846485346070986, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': 0.963895316469423, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': False,\n",
    "            'weight_decay': 6.586949775384596e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "        \n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'statistical'\n",
    "        dnn_target_encoder = False\n",
    "    \n",
    "    elif letter == \"C\":\n",
    "        #52.31801357204807\tquarters\ttrue\t144\t131\t0.000060363753946263044\t-1.5104000124283146\t2.335809622586189e-7\n",
    "        dnn_params = {\n",
    "            'layer_sizes': [144,131], #<----- from opta hyper parameter tuning\n",
    "            'drop_out_prob': 0.03, #<----- from opta hyper parameter tuning\n",
    "            'learning_rate': 0.000060363753946263044, #<----- from opta hyper parameter tuning\n",
    "            'loss_expontent': -1.5104000124283146, #<----- from opta hyper parameter tuning\n",
    "            'max_epochs': 300, #\n",
    "            'paitience': 15, #\n",
    "            'batch_size': 128, #\n",
    "            'val_chack_interval': 0.5, #\n",
    "            'verbose': False,\n",
    "            'weight_decay': 2.335809622586189e-7 #<----- from opta hyper parameter tuning\n",
    "        }\n",
    "\n",
    "        dnn_feature_scaler = 'minmax'\n",
    "        dnn_target_scaler = 'minmax'\n",
    "        dnn_preprocessor = 'quarters'\n",
    "        dnn_target_encoder = True\n",
    "\n",
    "    dnn_feature_scaler = pre.choose_scaler(dnn_feature_scaler)\n",
    "    dnn_target_scaler = pre.choose_scaler(dnn_target_scaler)\n",
    "    dnn_preprocessor = pre.choose_transformer(dnn_preprocessor)\n",
    "    dnn_target_encoder = pre.choose_encoder(dnn_target_encoder)\n",
    "    \n",
    "    dnn_preprocessing = Pipeline([\n",
    "        ('custom_transformer', dnn_preprocessor),\n",
    "        ('target_encoder', dnn_target_encoder), \n",
    "        ('feature_scaler', dnn_feature_scaler)\n",
    "    ])\n",
    "\n",
    "    dnn_target_preprocessing = Pipeline([\n",
    "        ('target_scaler', dnn_target_scaler)\n",
    "    ])\n",
    "\n",
    "    #fit the preprocessing:\n",
    "    dnn_preprocessing.fit(X_train, y_train)\n",
    "    dnn_target_preprocessing.fit(pd.DataFrame(y_train))\n",
    "\n",
    "    #transform the data:\n",
    "    X_train_dnn = pd.DataFrame((dnn_preprocessing.transform(X_train)))\n",
    "    y_train_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_train)))\n",
    "    X_val_dnn = pd.DataFrame(dnn_preprocessing.transform(X_val))\n",
    "    y_val_dnn = pd.DataFrame(dnn_target_preprocessing.transform(pd.DataFrame(y_val)))\n",
    "\n",
    "    #fit the model:\n",
    "    dnn_model = HenrikDNN(n_features =X_train_dnn.shape[1] , **dnn_params)\n",
    "    dnn_model.train(X_train_dnn, y_train_dnn, X_val_dnn, y_val_dnn)\n",
    "    #predict:\n",
    "    dnn_pred = dnn_model.predict(X_val_dnn)\n",
    "    #scale back:\n",
    "    dnn_pred = dnn_target_preprocessing.inverse_transform(dnn_pred).reshape(-1)\n",
    "\n",
    "    print(f\"MAE DNN location {letter}: {mean_absolute_error(y_val, dnn_pred)}\")\n",
    "\n",
    "    return dnn_model, dnn_preprocessing, dnn_target_preprocessing, mean_absolute_error(y_val, dnn_pred)\n",
    "\n",
    "\n",
    "models = {\"A\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"B\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None},\n",
    "          \"C\": \n",
    "          {\"best_score\": 10000, \"best_model\": None, \"best_preprocessor\": None, \"best_target_preprocessor\": None}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_iterations = 7\n",
    "\n",
    "\n",
    "for letter in [\"A\", \"B\", \"C\"]:\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"training {letter}, iteration {i+1} of {num_iterations}\")\n",
    "        model, preprocessor, target_preprocessor, score = trainDNN(letter)\n",
    "        if score < models[letter][\"best_score\"]:\n",
    "            print(f\"new best score for {letter}: {score}\")\n",
    "            models[letter][\"best_score\"] = score\n",
    "            models[letter][\"best_model\"] = model\n",
    "            models[letter][\"best_preprocessor\"] = preprocessor\n",
    "            models[letter][\"best_target_preprocessor\"] = target_preprocessor\n",
    "\n",
    "post.make_dnn_prediction(models[\"A\"][\"best_model\"],\n",
    "                         models[\"A\"][\"best_preprocessor\"],\n",
    "                         models[\"A\"][\"best_target_preprocessor\"],\n",
    "                         models[\"B\"][\"best_model\"],\n",
    "                         models[\"B\"][\"best_preprocessor\"],\n",
    "                         models[\"B\"][\"best_target_preprocessor\"],\n",
    "                         models[\"C\"][\"best_model\"],\n",
    "                         models[\"C\"][\"best_preprocessor\"],\n",
    "                         models[\"C\"][\"best_target_preprocessor\"],\n",
    "                         f\"{FOLDER_NAME}/DNN.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask's AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-12 20:04:39] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 20:04:39] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 20:04:39] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 20:04:39] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 20:04:39] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2344} INFO - Estimated sufficient time budget=2267s. Estimated necessary time budget=19s.\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2391} INFO -  at 1.3s,\testimator lgbm's best error=721.2925,\tbest estimator lgbm's best error=721.2925\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2391} INFO -  at 1.5s,\testimator lgbm's best error=647.8535,\tbest estimator lgbm's best error=647.8535\n",
      "[flaml.automl.logger: 11-12 20:04:39] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 1.7s,\testimator lgbm's best error=647.8535,\tbest estimator lgbm's best error=647.8535\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 1.8s,\testimator lgbm's best error=635.6789,\tbest estimator lgbm's best error=635.6789\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 2.0s,\testimator lgbm's best error=472.8438,\tbest estimator lgbm's best error=472.8438\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 2.2s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 2.4s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2391} INFO -  at 2.6s,\testimator lgbm's best error=363.8583,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 20:04:40] {2218} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2391} INFO -  at 2.7s,\testimator xgboost's best error=715.6926,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2218} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2391} INFO -  at 2.9s,\testimator xgboost's best error=645.5103,\tbest estimator lgbm's best error=363.8583\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2218} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2391} INFO -  at 3.1s,\testimator lgbm's best error=346.0072,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2218} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2391} INFO -  at 3.3s,\testimator extra_tree's best error=421.1442,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 20:04:41] {2218} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:42] {2391} INFO -  at 3.7s,\testimator extra_tree's best error=408.7660,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 20:04:42] {2218} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:04:42] {2391} INFO -  at 4.6s,\testimator rf's best error=433.5866,\tbest estimator lgbm's best error=346.0072\n",
      "[flaml.automl.logger: 11-12 20:04:42] {2218} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2391} INFO -  at 4.9s,\testimator lgbm's best error=335.2427,\tbest estimator lgbm's best error=335.2427\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2218} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2391} INFO -  at 5.1s,\testimator lgbm's best error=335.2427,\tbest estimator lgbm's best error=335.2427\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2218} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2391} INFO -  at 5.3s,\testimator lgbm's best error=334.5931,\tbest estimator lgbm's best error=334.5931\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2218} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2391} INFO -  at 5.4s,\testimator xgboost's best error=645.5103,\tbest estimator lgbm's best error=334.5931\n",
      "[flaml.automl.logger: 11-12 20:04:43] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2391} INFO -  at 5.6s,\testimator lgbm's best error=321.4361,\tbest estimator lgbm's best error=321.4361\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2218} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2391} INFO -  at 6.0s,\testimator lgbm's best error=316.5987,\tbest estimator lgbm's best error=316.5987\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2218} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2391} INFO -  at 6.2s,\testimator lgbm's best error=316.5987,\tbest estimator lgbm's best error=316.5987\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2218} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2391} INFO -  at 6.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:44] {2218} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:45] {2391} INFO -  at 6.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:45] {2218} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:45] {2391} INFO -  at 7.5s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:45] {2218} INFO - iteration 24, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:04:47] {2391} INFO -  at 9.1s,\testimator rf's best error=433.5866,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:47] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:47] {2391} INFO -  at 9.4s,\testimator extra_tree's best error=384.1186,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:47] {2218} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:50] {2391} INFO -  at 11.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:50] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:50] {2391} INFO -  at 12.3s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:50] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2391} INFO -  at 12.7s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2391} INFO -  at 13.1s,\testimator extra_tree's best error=351.5905,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2218} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2391} INFO -  at 13.3s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:51] {2218} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:52] {2391} INFO -  at 13.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:52] {2218} INFO - iteration 32, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:04:53] {2391} INFO -  at 15.1s,\testimator rf's best error=376.4654,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:53] {2218} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2391} INFO -  at 15.9s,\testimator extra_tree's best error=335.0027,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2218} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2391} INFO -  at 16.1s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2218} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2391} INFO -  at 16.3s,\testimator xgboost's best error=645.5103,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:54] {2218} INFO - iteration 36, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:04:56] {2391} INFO -  at 18.0s,\testimator catboost's best error=320.4814,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:56] {2218} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:04:56] {2391} INFO -  at 18.5s,\testimator extra_tree's best error=329.5534,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:56] {2218} INFO - iteration 38, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:04:59] {2391} INFO -  at 21.6s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:04:59] {2218} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:00] {2391} INFO -  at 22.0s,\testimator xgboost's best error=363.8470,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:00] {2218} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:01] {2391} INFO -  at 22.7s,\testimator xgboost's best error=363.8470,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:01] {2218} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:01] {2391} INFO -  at 23.0s,\testimator xgboost's best error=346.0393,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:01] {2218} INFO - iteration 42, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2391} INFO -  at 23.8s,\testimator extra_tree's best error=329.5534,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2218} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2391} INFO -  at 24.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2218} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2391} INFO -  at 24.4s,\testimator xgboost's best error=346.0393,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:02] {2218} INFO - iteration 45, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:03] {2391} INFO -  at 24.6s,\testimator extra_tree's best error=329.5534,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:03] {2218} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:03] {2391} INFO -  at 25.2s,\testimator xgboost's best error=346.0393,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:03] {2218} INFO - iteration 47, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2391} INFO -  at 26.7s,\testimator rf's best error=348.9441,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2218} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2391} INFO -  at 26.9s,\testimator xgboost's best error=346.0393,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2218} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2391} INFO -  at 27.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:05] {2218} INFO - iteration 50, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:06] {2391} INFO -  at 28.5s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:06] {2218} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:07] {2391} INFO -  at 29.4s,\testimator extra_tree's best error=325.3105,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:07] {2218} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:09] {2391} INFO -  at 31.3s,\testimator rf's best error=348.9441,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:09] {2218} INFO - iteration 53, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:10] {2391} INFO -  at 32.6s,\testimator rf's best error=348.9441,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:10] {2218} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:11] {2391} INFO -  at 33.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:11] {2218} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:11] {2391} INFO -  at 33.5s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:11] {2218} INFO - iteration 56, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:12] {2391} INFO -  at 34.1s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:12] {2218} INFO - iteration 57, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:15] {2391} INFO -  at 37.0s,\testimator rf's best error=339.4236,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:15] {2218} INFO - iteration 58, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:17] {2391} INFO -  at 38.7s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:17] {2218} INFO - iteration 59, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:18] {2391} INFO -  at 39.8s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:18] {2218} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:18] {2391} INFO -  at 40.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:18] {2218} INFO - iteration 61, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:19] {2391} INFO -  at 41.0s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:19] {2218} INFO - iteration 62, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:19] {2391} INFO -  at 41.5s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:19] {2218} INFO - iteration 63, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:20] {2391} INFO -  at 42.0s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:20] {2218} INFO - iteration 64, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:23] {2391} INFO -  at 44.9s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:23] {2218} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:23] {2391} INFO -  at 45.1s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:23] {2218} INFO - iteration 66, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:25] {2391} INFO -  at 47.1s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:25] {2218} INFO - iteration 67, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:25] {2391} INFO -  at 47.3s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:25] {2218} INFO - iteration 68, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:27] {2391} INFO -  at 48.9s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:27] {2218} INFO - iteration 69, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:28] {2391} INFO -  at 49.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:28] {2218} INFO - iteration 70, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:31] {2391} INFO -  at 53.6s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:31] {2218} INFO - iteration 71, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:32] {2391} INFO -  at 54.0s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:32] {2218} INFO - iteration 72, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:05:39] {2391} INFO -  at 60.6s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:39] {2218} INFO - iteration 73, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:40] {2391} INFO -  at 62.0s,\testimator extra_tree's best error=322.7558,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:40] {2218} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:41] {2391} INFO -  at 63.1s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:41] {2218} INFO - iteration 75, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:42] {2391} INFO -  at 63.9s,\testimator extra_tree's best error=322.7558,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:42] {2218} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:44] {2391} INFO -  at 65.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:44] {2218} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:44] {2391} INFO -  at 65.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:44] {2218} INFO - iteration 78, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:45] {2391} INFO -  at 67.4s,\testimator extra_tree's best error=322.7558,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:45] {2218} INFO - iteration 79, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:47] {2391} INFO -  at 69.6s,\testimator rf's best error=332.6824,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:47] {2218} INFO - iteration 80, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:05:49] {2391} INFO -  at 70.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:49] {2218} INFO - iteration 81, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:05:50] {2391} INFO -  at 71.9s,\testimator extra_tree's best error=321.4442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:50] {2218} INFO - iteration 82, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:05:53] {2391} INFO -  at 74.8s,\testimator rf's best error=332.6824,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:53] {2218} INFO - iteration 83, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:05:59] {2391} INFO -  at 81.1s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:05:59] {2218} INFO - iteration 84, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:03] {2391} INFO -  at 85.2s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:03] {2218} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:06:04] {2391} INFO -  at 86.3s,\testimator extra_tree's best error=321.4442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:04] {2218} INFO - iteration 86, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:08] {2391} INFO -  at 90.5s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:08] {2218} INFO - iteration 87, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:16] {2391} INFO -  at 97.8s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:16] {2218} INFO - iteration 88, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:16] {2391} INFO -  at 98.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:16] {2218} INFO - iteration 89, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:18] {2391} INFO -  at 99.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:18] {2218} INFO - iteration 90, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:06:19] {2391} INFO -  at 100.8s,\testimator extra_tree's best error=321.4442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:19] {2218} INFO - iteration 91, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:06:20] {2391} INFO -  at 101.7s,\testimator rf's best error=332.6824,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:20] {2218} INFO - iteration 92, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:06:20] {2391} INFO -  at 102.6s,\testimator extra_tree's best error=321.4442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:20] {2218} INFO - iteration 93, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:25] {2391} INFO -  at 106.9s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:25] {2218} INFO - iteration 94, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:27] {2391} INFO -  at 109.0s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:27] {2218} INFO - iteration 95, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:27] {2391} INFO -  at 109.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:27] {2218} INFO - iteration 96, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:06:29] {2391} INFO -  at 110.9s,\testimator extra_tree's best error=319.3442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:29] {2218} INFO - iteration 97, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:06:32] {2391} INFO -  at 114.5s,\testimator rf's best error=326.3850,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:32] {2218} INFO - iteration 98, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:06:36] {2391} INFO -  at 117.9s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:36] {2218} INFO - iteration 99, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:06:41] {2391} INFO -  at 123.2s,\testimator rf's best error=325.1574,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:41] {2218} INFO - iteration 100, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:42] {2391} INFO -  at 124.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:42] {2218} INFO - iteration 101, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2391} INFO -  at 124.8s,\testimator xgb_limitdepth's best error=333.3793,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2218} INFO - iteration 102, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2391} INFO -  at 125.2s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2218} INFO - iteration 103, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2391} INFO -  at 125.4s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2218} INFO - iteration 104, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2391} INFO -  at 125.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:43] {2218} INFO - iteration 105, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:44] {2391} INFO -  at 125.8s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:44] {2218} INFO - iteration 106, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2391} INFO -  at 127.6s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2218} INFO - iteration 107, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2391} INFO -  at 128.3s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2218} INFO - iteration 108, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2391} INFO -  at 128.5s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:46] {2218} INFO - iteration 109, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:06:48] {2391} INFO -  at 129.7s,\testimator extra_tree's best error=319.3442,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:48] {2218} INFO - iteration 110, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:48] {2391} INFO -  at 129.9s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:48] {2218} INFO - iteration 111, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2391} INFO -  at 130.8s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2218} INFO - iteration 112, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2391} INFO -  at 131.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2218} INFO - iteration 113, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2391} INFO -  at 131.4s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:49] {2218} INFO - iteration 114, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:50] {2391} INFO -  at 132.6s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:50] {2218} INFO - iteration 115, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:51] {2391} INFO -  at 133.0s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:51] {2218} INFO - iteration 116, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:51] {2391} INFO -  at 133.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:51] {2218} INFO - iteration 117, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:52] {2391} INFO -  at 134.3s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:52] {2218} INFO - iteration 118, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:52] {2391} INFO -  at 134.6s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:52] {2218} INFO - iteration 119, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2391} INFO -  at 135.7s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2218} INFO - iteration 120, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2391} INFO -  at 136.1s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2218} INFO - iteration 121, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2391} INFO -  at 136.3s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2218} INFO - iteration 122, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2391} INFO -  at 136.6s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:54] {2218} INFO - iteration 123, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:55] {2391} INFO -  at 137.0s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:55] {2218} INFO - iteration 124, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:06:57] {2391} INFO -  at 138.7s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:57] {2218} INFO - iteration 125, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:57] {2391} INFO -  at 138.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:57] {2218} INFO - iteration 126, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:58] {2391} INFO -  at 139.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:58] {2218} INFO - iteration 127, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:59] {2391} INFO -  at 140.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:59] {2218} INFO - iteration 128, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:06:59] {2391} INFO -  at 140.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:06:59] {2218} INFO - iteration 129, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:07:02] {2391} INFO -  at 144.1s,\testimator rf's best error=325.1574,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:02] {2218} INFO - iteration 130, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:02] {2391} INFO -  at 144.3s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:02] {2218} INFO - iteration 131, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:07:05] {2391} INFO -  at 147.3s,\testimator extra_tree's best error=318.5533,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:05] {2218} INFO - iteration 132, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:07:07] {2391} INFO -  at 149.1s,\testimator extra_tree's best error=318.5533,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:07] {2218} INFO - iteration 133, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:07] {2391} INFO -  at 149.4s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:07] {2218} INFO - iteration 134, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:08] {2391} INFO -  at 150.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:08] {2218} INFO - iteration 135, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2391} INFO -  at 152.7s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2218} INFO - iteration 136, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2391} INFO -  at 153.3s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2218} INFO - iteration 137, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2391} INFO -  at 153.6s,\testimator xgb_limitdepth's best error=320.6676,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:11] {2218} INFO - iteration 138, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:07:17] {2391} INFO -  at 159.3s,\testimator extra_tree's best error=318.4289,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:17] {2218} INFO - iteration 139, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:17] {2391} INFO -  at 159.5s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:17] {2218} INFO - iteration 140, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2391} INFO -  at 160.0s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2218} INFO - iteration 141, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2391} INFO -  at 160.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2218} INFO - iteration 142, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2391} INFO -  at 160.6s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:18] {2218} INFO - iteration 143, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:19] {2391} INFO -  at 160.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:19] {2218} INFO - iteration 144, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:19] {2391} INFO -  at 161.2s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:19] {2218} INFO - iteration 145, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:21] {2391} INFO -  at 163.4s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:21] {2218} INFO - iteration 146, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:07:24] {2391} INFO -  at 165.9s,\testimator catboost's best error=316.2325,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:24] {2218} INFO - iteration 147, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:24] {2391} INFO -  at 166.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:24] {2218} INFO - iteration 148, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:25] {2391} INFO -  at 167.3s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:25] {2218} INFO - iteration 149, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:26] {2391} INFO -  at 167.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:26] {2218} INFO - iteration 150, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:26] {2391} INFO -  at 168.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:26] {2218} INFO - iteration 151, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2391} INFO -  at 168.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2218} INFO - iteration 152, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2391} INFO -  at 169.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2218} INFO - iteration 153, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2391} INFO -  at 169.5s,\testimator xgboost's best error=331.9281,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:27] {2218} INFO - iteration 154, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:07:38] {2391} INFO -  at 180.6s,\testimator catboost's best error=315.9494,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:38] {2218} INFO - iteration 155, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:39] {2391} INFO -  at 181.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:39] {2218} INFO - iteration 156, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:39] {2391} INFO -  at 181.5s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:39] {2218} INFO - iteration 157, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:40] {2391} INFO -  at 182.0s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:40] {2218} INFO - iteration 158, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2391} INFO -  at 182.9s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2218} INFO - iteration 159, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2391} INFO -  at 183.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2218} INFO - iteration 160, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2391} INFO -  at 183.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:41] {2218} INFO - iteration 161, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:43] {2391} INFO -  at 184.7s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:43] {2218} INFO - iteration 162, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:43] {2391} INFO -  at 185.1s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:43] {2218} INFO - iteration 163, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:44] {2391} INFO -  at 186.0s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:44] {2218} INFO - iteration 164, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:07:44] {2391} INFO -  at 186.6s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:44] {2218} INFO - iteration 165, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:47] {2391} INFO -  at 189.0s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:47] {2218} INFO - iteration 166, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:47] {2391} INFO -  at 189.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:47] {2218} INFO - iteration 167, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:48] {2391} INFO -  at 189.8s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:48] {2218} INFO - iteration 168, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:49] {2391} INFO -  at 190.7s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:49] {2218} INFO - iteration 169, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2391} INFO -  at 191.8s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2218} INFO - iteration 170, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2391} INFO -  at 192.3s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2218} INFO - iteration 171, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2391} INFO -  at 192.6s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:50] {2218} INFO - iteration 172, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:07:51] {2391} INFO -  at 193.2s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:51] {2218} INFO - iteration 173, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:54] {2391} INFO -  at 196.1s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:54] {2218} INFO - iteration 174, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:54] {2391} INFO -  at 196.5s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:54] {2218} INFO - iteration 175, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:07:57] {2391} INFO -  at 198.9s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:07:57] {2218} INFO - iteration 176, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:08:02] {2391} INFO -  at 204.5s,\testimator rf's best error=325.1574,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:02] {2218} INFO - iteration 177, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:04] {2391} INFO -  at 205.9s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:04] {2218} INFO - iteration 178, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:04] {2391} INFO -  at 206.4s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:04] {2218} INFO - iteration 179, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:08] {2391} INFO -  at 210.4s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:08] {2218} INFO - iteration 180, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:09] {2391} INFO -  at 210.7s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:09] {2218} INFO - iteration 181, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:09] {2391} INFO -  at 211.3s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:09] {2218} INFO - iteration 182, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:10] {2391} INFO -  at 212.6s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:10] {2218} INFO - iteration 183, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:11] {2391} INFO -  at 213.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:11] {2218} INFO - iteration 184, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:12] {2391} INFO -  at 213.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:12] {2218} INFO - iteration 185, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:08:15] {2391} INFO -  at 217.6s,\testimator rf's best error=325.1574,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:15] {2218} INFO - iteration 186, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:16] {2391} INFO -  at 218.0s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:16] {2218} INFO - iteration 187, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:16] {2391} INFO -  at 218.3s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:16] {2218} INFO - iteration 188, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:19] {2391} INFO -  at 221.6s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:19] {2218} INFO - iteration 189, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:20] {2391} INFO -  at 222.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:20] {2218} INFO - iteration 190, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:20] {2391} INFO -  at 222.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:20] {2218} INFO - iteration 191, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:21] {2391} INFO -  at 222.9s,\testimator xgboost's best error=318.3222,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:21] {2218} INFO - iteration 192, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:22] {2391} INFO -  at 223.9s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:22] {2218} INFO - iteration 193, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2391} INFO -  at 225.8s,\testimator xgboost's best error=317.2078,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2218} INFO - iteration 194, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2391} INFO -  at 226.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2218} INFO - iteration 195, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2391} INFO -  at 226.3s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2218} INFO - iteration 196, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2391} INFO -  at 226.4s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:24] {2218} INFO - iteration 197, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:28] {2391} INFO -  at 229.7s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:28] {2218} INFO - iteration 198, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:29] {2391} INFO -  at 231.3s,\testimator xgboost's best error=317.2078,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:29] {2218} INFO - iteration 199, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:08:31] {2391} INFO -  at 233.6s,\testimator xgboost's best error=317.2078,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:31] {2218} INFO - iteration 200, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:32] {2391} INFO -  at 234.2s,\testimator xgb_limitdepth's best error=319.3298,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:32] {2218} INFO - iteration 201, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:33] {2391} INFO -  at 234.7s,\testimator xgb_limitdepth's best error=316.5932,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:33] {2218} INFO - iteration 202, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:08:33] {2391} INFO -  at 235.0s,\testimator lgbm's best error=314.4265,\tbest estimator lgbm's best error=314.4265\n",
      "[flaml.automl.logger: 11-12 20:08:33] {2218} INFO - iteration 203, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:34] {2391} INFO -  at 236.6s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:34] {2218} INFO - iteration 204, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:35] {2391} INFO -  at 237.1s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:35] {2218} INFO - iteration 205, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:38] {2391} INFO -  at 240.0s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:38] {2218} INFO - iteration 206, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:39] {2391} INFO -  at 240.8s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:39] {2218} INFO - iteration 207, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:48] {2391} INFO -  at 250.2s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:48] {2218} INFO - iteration 208, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:48] {2391} INFO -  at 250.6s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:48] {2218} INFO - iteration 209, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:49] {2391} INFO -  at 251.6s,\testimator xgb_limitdepth's best error=314.0676,\tbest estimator xgb_limitdepth's best error=314.0676\n",
      "[flaml.automl.logger: 11-12 20:08:49] {2218} INFO - iteration 210, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:52] {2391} INFO -  at 253.8s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:08:52] {2218} INFO - iteration 211, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:08:52] {2391} INFO -  at 254.5s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:08:52] {2218} INFO - iteration 212, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:00] {2391} INFO -  at 261.9s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:00] {2218} INFO - iteration 213, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:01] {2391} INFO -  at 263.2s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:01] {2218} INFO - iteration 214, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:05] {2391} INFO -  at 267.1s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:05] {2218} INFO - iteration 215, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:07] {2391} INFO -  at 269.1s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:07] {2218} INFO - iteration 216, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:09] {2391} INFO -  at 271.5s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:09] {2218} INFO - iteration 217, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:10] {2391} INFO -  at 272.0s,\testimator xgb_limitdepth's best error=311.6356,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:10] {2218} INFO - iteration 218, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2391} INFO -  at 275.6s,\testimator rf's best error=325.1574,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2218} INFO - iteration 219, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2391} INFO -  at 276.1s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2218} INFO - iteration 220, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2391} INFO -  at 276.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=311.6356\n",
      "[flaml.automl.logger: 11-12 20:09:14] {2218} INFO - iteration 221, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:09:27] {2391} INFO -  at 288.9s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:09:27] {2218} INFO - iteration 222, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:09:28] {2391} INFO -  at 289.7s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:09:28] {2218} INFO - iteration 223, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:10:02] {2391} INFO -  at 324.3s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:02] {2218} INFO - iteration 224, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:10:07] {2391} INFO -  at 329.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:07] {2218} INFO - iteration 225, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:10:08] {2391} INFO -  at 329.8s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:08] {2218} INFO - iteration 226, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:10:09] {2391} INFO -  at 330.9s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:09] {2218} INFO - iteration 227, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:10:11] {2391} INFO -  at 333.4s,\testimator extra_tree's best error=318.4289,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:11] {2218} INFO - iteration 228, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:10:12] {2391} INFO -  at 334.1s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:12] {2218} INFO - iteration 229, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:10:32] {2391} INFO -  at 354.2s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:32] {2218} INFO - iteration 230, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:10:33] {2391} INFO -  at 354.8s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:33] {2218} INFO - iteration 231, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:10:40] {2391} INFO -  at 362.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:40] {2218} INFO - iteration 232, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:10:59] {2391} INFO -  at 380.8s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:10:59] {2218} INFO - iteration 233, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:11:07] {2391} INFO -  at 389.2s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:11:07] {2218} INFO - iteration 234, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:11:51] {2391} INFO -  at 433.0s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:11:51] {2218} INFO - iteration 235, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:11:55] {2391} INFO -  at 436.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:11:55] {2218} INFO - iteration 236, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:12:07] {2391} INFO -  at 448.9s,\testimator catboost's best error=315.2590,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:07] {2218} INFO - iteration 237, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:12:14] {2391} INFO -  at 455.9s,\testimator catboost's best error=314.5636,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:14] {2218} INFO - iteration 238, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:12:29] {2391} INFO -  at 471.1s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:29] {2218} INFO - iteration 239, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:12:41] {2391} INFO -  at 483.4s,\testimator catboost's best error=314.5636,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:41] {2218} INFO - iteration 240, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:12:51] {2391} INFO -  at 493.0s,\testimator catboost's best error=312.8698,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:51] {2218} INFO - iteration 241, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:12:58] {2391} INFO -  at 499.9s,\testimator catboost's best error=312.8698,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:58] {2218} INFO - iteration 242, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:12:58] {2391} INFO -  at 500.2s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:12:58] {2218} INFO - iteration 243, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:13:30] {2391} INFO -  at 531.8s,\testimator catboost's best error=312.8698,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:13:30] {2218} INFO - iteration 244, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:13:36] {2391} INFO -  at 537.9s,\testimator catboost's best error=312.8698,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:13:36] {2218} INFO - iteration 245, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:13:47] {2391} INFO -  at 549.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:13:47] {2218} INFO - iteration 246, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:13:53] {2391} INFO -  at 554.8s,\testimator rf's best error=322.5125,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:13:53] {2218} INFO - iteration 247, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:13:58] {2391} INFO -  at 560.3s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:13:58] {2218} INFO - iteration 248, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:14:06] {2391} INFO -  at 567.8s,\testimator catboost's best error=312.8698,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:06] {2218} INFO - iteration 249, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:14:06] {2391} INFO -  at 568.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:06] {2218} INFO - iteration 250, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:14:34] {2391} INFO -  at 596.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:34] {2218} INFO - iteration 251, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:14:50] {2391} INFO -  at 611.9s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:50] {2218} INFO - iteration 252, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:14:50] {2391} INFO -  at 612.2s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:50] {2218} INFO - iteration 253, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:14:54] {2391} INFO -  at 616.1s,\testimator rf's best error=322.5125,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:14:54] {2218} INFO - iteration 254, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:15:02] {2391} INFO -  at 623.8s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:15:02] {2218} INFO - iteration 255, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:15:06] {2391} INFO -  at 627.7s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:15:06] {2218} INFO - iteration 256, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:15:06] {2391} INFO -  at 628.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:15:06] {2218} INFO - iteration 257, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:15:44] {2391} INFO -  at 666.1s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:15:44] {2218} INFO - iteration 258, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:15:51] {2391} INFO -  at 673.1s,\testimator rf's best error=322.2414,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:15:51] {2218} INFO - iteration 259, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:16:18] {2391} INFO -  at 700.0s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:18] {2218} INFO - iteration 260, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:16:22] {2391} INFO -  at 704.2s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:22] {2218} INFO - iteration 261, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:16:28] {2391} INFO -  at 710.6s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:28] {2218} INFO - iteration 262, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:16:37] {2391} INFO -  at 718.7s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:37] {2218} INFO - iteration 263, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:16:37] {2391} INFO -  at 719.5s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:37] {2218} INFO - iteration 264, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:16:38] {2391} INFO -  at 720.2s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:38] {2218} INFO - iteration 265, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:16:47] {2391} INFO -  at 729.1s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:16:47] {2218} INFO - iteration 266, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:17:00] {2391} INFO -  at 742.5s,\testimator extra_tree's best error=317.7185,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:17:00] {2218} INFO - iteration 267, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:17:38] {2391} INFO -  at 780.1s,\testimator xgb_limitdepth's best error=310.5988,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:17:38] {2218} INFO - iteration 268, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:17:46] {2391} INFO -  at 788.2s,\testimator extra_tree's best error=317.7185,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:17:46] {2218} INFO - iteration 269, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:18:04] {2391} INFO -  at 806.6s,\testimator extra_tree's best error=317.0293,\tbest estimator xgb_limitdepth's best error=310.5988\n",
      "[flaml.automl.logger: 11-12 20:18:04] {2218} INFO - iteration 270, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:18:09] {2391} INFO -  at 810.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:09] {2218} INFO - iteration 271, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:18:09] {2391} INFO -  at 811.6s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:10] {2218} INFO - iteration 272, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:18:18] {2391} INFO -  at 820.4s,\testimator extra_tree's best error=317.0293,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:18] {2218} INFO - iteration 273, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:18:23] {2391} INFO -  at 824.7s,\testimator rf's best error=322.2414,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:23] {2218} INFO - iteration 274, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:18:23] {2391} INFO -  at 825.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:23] {2218} INFO - iteration 275, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:18:39] {2391} INFO -  at 841.2s,\testimator rf's best error=322.2414,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:39] {2218} INFO - iteration 276, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:18:39] {2391} INFO -  at 841.4s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:39] {2218} INFO - iteration 277, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:18:41] {2391} INFO -  at 843.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:41] {2218} INFO - iteration 278, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:18:45] {2391} INFO -  at 847.4s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:45] {2218} INFO - iteration 279, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:18:50] {2391} INFO -  at 851.9s,\testimator rf's best error=322.2414,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:50] {2218} INFO - iteration 280, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:18:52] {2391} INFO -  at 853.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:18:52] {2218} INFO - iteration 281, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:19:34] {2391} INFO -  at 895.9s,\testimator extra_tree's best error=316.5789,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:34] {2218} INFO - iteration 282, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:19:34] {2391} INFO -  at 896.6s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:34] {2218} INFO - iteration 283, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:19:49] {2391} INFO -  at 911.0s,\testimator rf's best error=322.2414,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:49] {2218} INFO - iteration 284, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:19:49] {2391} INFO -  at 911.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:49] {2218} INFO - iteration 285, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:19:58] {2391} INFO -  at 920.4s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:58] {2218} INFO - iteration 286, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:19:59] {2391} INFO -  at 920.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:19:59] {2218} INFO - iteration 287, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:20:44] {2391} INFO -  at 966.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:20:44] {2218} INFO - iteration 288, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:22:54] {2391} INFO -  at 1096.4s,\testimator extra_tree's best error=316.0479,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:22:54] {2218} INFO - iteration 289, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:22:55] {2391} INFO -  at 1096.7s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:22:55] {2218} INFO - iteration 290, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:22:55] {2391} INFO -  at 1097.4s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:22:55] {2218} INFO - iteration 291, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:22:59] {2391} INFO -  at 1101.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:22:59] {2218} INFO - iteration 292, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:23:00] {2391} INFO -  at 1102.0s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:00] {2218} INFO - iteration 293, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:04] {2391} INFO -  at 1106.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:04] {2218} INFO - iteration 294, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:23:06] {2391} INFO -  at 1108.4s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:06] {2218} INFO - iteration 295, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:08] {2391} INFO -  at 1110.6s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:08] {2218} INFO - iteration 296, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:23:24] {2391} INFO -  at 1126.2s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:24] {2218} INFO - iteration 297, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:32] {2391} INFO -  at 1133.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:32] {2218} INFO - iteration 298, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:23:33] {2391} INFO -  at 1135.5s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:33] {2218} INFO - iteration 299, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2391} INFO -  at 1142.7s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2218} INFO - iteration 300, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2391} INFO -  at 1143.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2218} INFO - iteration 301, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2391} INFO -  at 1143.6s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:41] {2218} INFO - iteration 302, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:23:42] {2391} INFO -  at 1144.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:42] {2218} INFO - iteration 303, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:23:42] {2391} INFO -  at 1144.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:42] {2218} INFO - iteration 304, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:53] {2391} INFO -  at 1155.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:53] {2218} INFO - iteration 305, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:23:53] {2391} INFO -  at 1155.5s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:53] {2218} INFO - iteration 306, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:55] {2391} INFO -  at 1157.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:55] {2218} INFO - iteration 307, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:23:59] {2391} INFO -  at 1160.7s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:23:59] {2218} INFO - iteration 308, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:24:03] {2391} INFO -  at 1165.6s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:03] {2218} INFO - iteration 309, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:24:12] {2391} INFO -  at 1174.5s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:12] {2218} INFO - iteration 310, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:24:13] {2391} INFO -  at 1175.0s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:13] {2218} INFO - iteration 311, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:24:21] {2391} INFO -  at 1183.3s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:21] {2218} INFO - iteration 312, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:24:21] {2391} INFO -  at 1183.6s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:21] {2218} INFO - iteration 313, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:24:22] {2391} INFO -  at 1184.5s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:22] {2218} INFO - iteration 314, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:24:23] {2391} INFO -  at 1185.0s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:23] {2218} INFO - iteration 315, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:24:24] {2391} INFO -  at 1186.1s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:24] {2218} INFO - iteration 316, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:24:24] {2391} INFO -  at 1186.5s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:24:24] {2218} INFO - iteration 317, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:25:17] {2391} INFO -  at 1239.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:17] {2218} INFO - iteration 318, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:17] {2391} INFO -  at 1239.5s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:17] {2218} INFO - iteration 319, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:19] {2391} INFO -  at 1241.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:19] {2218} INFO - iteration 320, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:19] {2391} INFO -  at 1241.5s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:19] {2218} INFO - iteration 321, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:25:26] {2391} INFO -  at 1247.8s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:26] {2218} INFO - iteration 322, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:25:46] {2391} INFO -  at 1268.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:46] {2218} INFO - iteration 323, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:47] {2391} INFO -  at 1268.8s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:47] {2218} INFO - iteration 324, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:47] {2391} INFO -  at 1269.1s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:47] {2218} INFO - iteration 325, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:25:48] {2391} INFO -  at 1270.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:48] {2218} INFO - iteration 326, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:49] {2391} INFO -  at 1270.8s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:49] {2218} INFO - iteration 327, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:25:50] {2391} INFO -  at 1272.1s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:50] {2218} INFO - iteration 328, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:50] {2391} INFO -  at 1272.3s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:50] {2218} INFO - iteration 329, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:25:51] {2391} INFO -  at 1272.8s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:51] {2218} INFO - iteration 330, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:25:53] {2391} INFO -  at 1275.2s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:53] {2218} INFO - iteration 331, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:25:59] {2391} INFO -  at 1281.6s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:25:59] {2218} INFO - iteration 332, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:26:00] {2391} INFO -  at 1281.9s,\testimator lgbm's best error=314.4265,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:26:00] {2218} INFO - iteration 333, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:26:38] {2391} INFO -  at 1319.8s,\testimator extra_tree's best error=315.4868,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:26:38] {2218} INFO - iteration 334, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:26:39] {2391} INFO -  at 1320.8s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:26:39] {2218} INFO - iteration 335, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:27:03] {2391} INFO -  at 1344.9s,\testimator extra_tree's best error=315.4868,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:03] {2218} INFO - iteration 336, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:27:04] {2391} INFO -  at 1345.8s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:04] {2218} INFO - iteration 337, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:05] {2391} INFO -  at 1346.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:05] {2218} INFO - iteration 338, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:27:05] {2391} INFO -  at 1347.6s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:05] {2218} INFO - iteration 339, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:27:09] {2391} INFO -  at 1351.2s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:09] {2218} INFO - iteration 340, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:34] {2391} INFO -  at 1375.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:34] {2218} INFO - iteration 341, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:27:35] {2391} INFO -  at 1376.8s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:35] {2218} INFO - iteration 342, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:27:35] {2391} INFO -  at 1377.1s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:35] {2218} INFO - iteration 343, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:48] {2391} INFO -  at 1389.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:48] {2218} INFO - iteration 344, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:49] {2391} INFO -  at 1391.3s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:49] {2218} INFO - iteration 345, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:27:50] {2391} INFO -  at 1391.9s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:50] {2218} INFO - iteration 346, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:53] {2391} INFO -  at 1394.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:53] {2218} INFO - iteration 347, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:27:59] {2391} INFO -  at 1401.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:27:59] {2218} INFO - iteration 348, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:28:04] {2391} INFO -  at 1405.8s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:28:04] {2218} INFO - iteration 349, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:28:07] {2391} INFO -  at 1409.3s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:28:07] {2218} INFO - iteration 350, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:28:09] {2391} INFO -  at 1411.0s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:28:09] {2218} INFO - iteration 351, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:28:26] {2391} INFO -  at 1428.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:28:26] {2218} INFO - iteration 352, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:28:27] {2391} INFO -  at 1429.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:28:27] {2218} INFO - iteration 353, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:29:16] {2391} INFO -  at 1478.5s,\testimator extra_tree's best error=315.4868,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:16] {2218} INFO - iteration 354, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:29:24] {2391} INFO -  at 1485.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:24] {2218} INFO - iteration 355, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:29:24] {2391} INFO -  at 1486.6s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:24] {2218} INFO - iteration 356, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:29:27] {2391} INFO -  at 1488.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:27] {2218} INFO - iteration 357, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:29:28] {2391} INFO -  at 1490.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:28] {2218} INFO - iteration 358, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:29:29] {2391} INFO -  at 1490.8s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:29] {2218} INFO - iteration 359, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:29:29] {2391} INFO -  at 1491.1s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:29:29] {2218} INFO - iteration 360, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:30:21] {2391} INFO -  at 1542.9s,\testimator extra_tree's best error=315.4868,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:30:21] {2218} INFO - iteration 361, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:30:24] {2391} INFO -  at 1545.9s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:30:24] {2218} INFO - iteration 362, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:30:56] {2391} INFO -  at 1578.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:30:56] {2218} INFO - iteration 363, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:31:11] {2391} INFO -  at 1592.7s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:11] {2218} INFO - iteration 364, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:16] {2391} INFO -  at 1598.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:16] {2218} INFO - iteration 365, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:28] {2391} INFO -  at 1610.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:28] {2218} INFO - iteration 366, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:31:28] {2391} INFO -  at 1610.5s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:28] {2218} INFO - iteration 367, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:31:30] {2391} INFO -  at 1612.2s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:30] {2218} INFO - iteration 368, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:33] {2391} INFO -  at 1615.5s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:33] {2218} INFO - iteration 369, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:31:41] {2391} INFO -  at 1622.6s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:41] {2218} INFO - iteration 370, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:45] {2391} INFO -  at 1627.6s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:45] {2218} INFO - iteration 371, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:57] {2391} INFO -  at 1639.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:57] {2218} INFO - iteration 372, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:31:59] {2391} INFO -  at 1640.7s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:31:59] {2218} INFO - iteration 373, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:32:02] {2391} INFO -  at 1643.8s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:02] {2218} INFO - iteration 374, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:32:03] {2391} INFO -  at 1644.9s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:03] {2218} INFO - iteration 375, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:04] {2391} INFO -  at 1645.7s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:04] {2218} INFO - iteration 376, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:04] {2391} INFO -  at 1646.3s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:04] {2218} INFO - iteration 377, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:05] {2391} INFO -  at 1647.2s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:05] {2218} INFO - iteration 378, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:06] {2391} INFO -  at 1647.7s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:06] {2218} INFO - iteration 379, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:32:08] {2391} INFO -  at 1650.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:08] {2218} INFO - iteration 380, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:08] {2391} INFO -  at 1650.6s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:08] {2218} INFO - iteration 381, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:11] {2391} INFO -  at 1653.3s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:11] {2218} INFO - iteration 382, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:32:17] {2391} INFO -  at 1659.3s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:17] {2218} INFO - iteration 383, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:32:21] {2391} INFO -  at 1663.0s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:21] {2218} INFO - iteration 384, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:32:25] {2391} INFO -  at 1666.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:25] {2218} INFO - iteration 385, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:32:25] {2391} INFO -  at 1667.2s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:25] {2218} INFO - iteration 386, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:32:35] {2391} INFO -  at 1677.0s,\testimator catboost's best error=312.4900,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:32:35] {2218} INFO - iteration 387, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:33:00] {2391} INFO -  at 1702.0s,\testimator extra_tree's best error=315.4868,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:00] {2218} INFO - iteration 388, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:33:10] {2391} INFO -  at 1712.2s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:10] {2218} INFO - iteration 389, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:33:14] {2391} INFO -  at 1715.9s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:14] {2218} INFO - iteration 390, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:33:14] {2391} INFO -  at 1716.5s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:14] {2218} INFO - iteration 391, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:33:16] {2391} INFO -  at 1718.0s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:16] {2218} INFO - iteration 392, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:33:18] {2391} INFO -  at 1719.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:18] {2218} INFO - iteration 393, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:33:25] {2391} INFO -  at 1727.0s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:25] {2218} INFO - iteration 394, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:33:26] {2391} INFO -  at 1727.7s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:26] {2218} INFO - iteration 395, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:33:26] {2391} INFO -  at 1728.3s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:26] {2218} INFO - iteration 396, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:33:27] {2391} INFO -  at 1729.1s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:33:27] {2218} INFO - iteration 397, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:34:07] {2391} INFO -  at 1769.4s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:07] {2218} INFO - iteration 398, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:08] {2391} INFO -  at 1770.1s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:08] {2218} INFO - iteration 399, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:09] {2391} INFO -  at 1770.7s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:09] {2218} INFO - iteration 400, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:09] {2391} INFO -  at 1771.6s,\testimator xgboost's best error=317.2078,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:09] {2218} INFO - iteration 401, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:34:10] {2391} INFO -  at 1772.1s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:10] {2218} INFO - iteration 402, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:16] {2391} INFO -  at 1777.7s,\testimator lgbm's best error=312.8840,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:16] {2218} INFO - iteration 403, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:34:16] {2391} INFO -  at 1778.6s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:16] {2218} INFO - iteration 404, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:20] {2391} INFO -  at 1782.6s,\testimator xgboost's best error=316.5309,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:20] {2218} INFO - iteration 405, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:21] {2391} INFO -  at 1783.1s,\testimator xgboost's best error=316.5309,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:21] {2218} INFO - iteration 406, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:34:38] {2391} INFO -  at 1799.9s,\testimator xgb_limitdepth's best error=309.1906,\tbest estimator xgb_limitdepth's best error=309.1906\n",
      "[flaml.automl.logger: 11-12 20:34:38] {2493} INFO - selected model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
      "             colsample_bylevel=0.8739309022559869, colsample_bynode=None,\n",
      "             colsample_bytree=0.8148494129779046, device=None,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.017836142105572647, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "             min_child_weight=0.07451980054061333, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=589,\n",
      "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
      "[flaml.automl.logger: 11-12 20:34:38] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 20:34:38] {1931} INFO - Time taken to find the best model: 810.8299508094788\n",
      "[flaml.automl.logger: 11-12 20:34:57] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 20:34:57] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 20:34:57] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 20:34:57] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 20:34:57] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2344} INFO - Estimated sufficient time budget=1456s. Estimated necessary time budget=12s.\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.0s,\testimator lgbm's best error=173.3460,\tbest estimator lgbm's best error=173.3460\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.1s,\testimator lgbm's best error=155.8314,\tbest estimator lgbm's best error=155.8314\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.3s,\testimator lgbm's best error=155.8314,\tbest estimator lgbm's best error=155.8314\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.4s,\testimator lgbm's best error=152.3728,\tbest estimator lgbm's best error=152.3728\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.5s,\testimator xgboost's best error=173.6956,\tbest estimator lgbm's best error=152.3728\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2391} INFO -  at 1.7s,\testimator lgbm's best error=111.8476,\tbest estimator lgbm's best error=111.8476\n",
      "[flaml.automl.logger: 11-12 20:34:57] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 1.8s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 2.0s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 2.1s,\testimator lgbm's best error=78.0171,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 2.2s,\testimator xgboost's best error=156.1478,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 2.4s,\testimator extra_tree's best error=83.8868,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2391} INFO -  at 2.5s,\testimator xgboost's best error=156.1478,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:58] {2218} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2391} INFO -  at 2.8s,\testimator extra_tree's best error=83.3841,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2218} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2391} INFO -  at 3.6s,\testimator rf's best error=89.5367,\tbest estimator lgbm's best error=78.0171\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2218} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2391} INFO -  at 3.7s,\testimator lgbm's best error=71.9364,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:34:59] {2218} INFO - iteration 15, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.0s,\testimator rf's best error=88.7012,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.1s,\testimator lgbm's best error=71.9364,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.3s,\testimator xgboost's best error=156.1478,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.4s,\testimator lgbm's best error=71.9364,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.6s,\testimator lgbm's best error=71.9364,\tbest estimator lgbm's best error=71.9364\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2391} INFO -  at 5.7s,\testimator lgbm's best error=70.6582,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:01] {2218} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2391} INFO -  at 5.9s,\testimator lgbm's best error=70.6582,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2218} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2391} INFO -  at 6.1s,\testimator lgbm's best error=70.6582,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2218} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2391} INFO -  at 6.3s,\testimator lgbm's best error=70.6582,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2218} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2391} INFO -  at 6.4s,\testimator lgbm's best error=70.6582,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2391} INFO -  at 6.7s,\testimator extra_tree's best error=74.1033,\tbest estimator lgbm's best error=70.6582\n",
      "[flaml.automl.logger: 11-12 20:35:02] {2218} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2391} INFO -  at 6.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2391} INFO -  at 7.3s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2391} INFO -  at 7.5s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:03] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2391} INFO -  at 8.0s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2218} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2391} INFO -  at 8.2s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2218} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2391} INFO -  at 8.4s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:04] {2218} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2391} INFO -  at 8.8s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2218} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2391} INFO -  at 9.0s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2218} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2391} INFO -  at 9.2s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2218} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2391} INFO -  at 9.5s,\testimator xgboost's best error=80.8189,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:05] {2218} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2391} INFO -  at 9.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2218} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2391} INFO -  at 10.5s,\testimator xgboost's best error=80.8189,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2218} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2391} INFO -  at 10.7s,\testimator xgboost's best error=75.8379,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:06] {2218} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2391} INFO -  at 10.8s,\testimator xgboost's best error=75.8379,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2218} INFO - iteration 40, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2391} INFO -  at 11.0s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2218} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2391} INFO -  at 11.5s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2218} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2391} INFO -  at 11.7s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:07] {2218} INFO - iteration 43, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:35:09] {2391} INFO -  at 13.4s,\testimator catboost's best error=71.2566,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:09] {2218} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2391} INFO -  at 13.8s,\testimator xgboost's best error=75.8379,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2218} INFO - iteration 45, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2391} INFO -  at 14.2s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2218} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2391} INFO -  at 14.4s,\testimator xgboost's best error=75.8379,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:10] {2218} INFO - iteration 47, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:35:11] {2391} INFO -  at 15.7s,\testimator catboost's best error=71.1061,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:11] {2218} INFO - iteration 48, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2391} INFO -  at 15.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2218} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2391} INFO -  at 16.1s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2218} INFO - iteration 50, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2391} INFO -  at 16.5s,\testimator xgboost's best error=74.4525,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:12] {2218} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2391} INFO -  at 16.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2218} INFO - iteration 52, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2391} INFO -  at 17.1s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2218} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2391} INFO -  at 17.3s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2218} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2391} INFO -  at 17.4s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2218} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2391} INFO -  at 17.6s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:13] {2218} INFO - iteration 56, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2391} INFO -  at 17.8s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2218} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2391} INFO -  at 18.0s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2218} INFO - iteration 58, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2391} INFO -  at 18.2s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2218} INFO - iteration 59, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2391} INFO -  at 18.7s,\testimator xgboost's best error=74.4525,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:14] {2218} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2391} INFO -  at 18.8s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2218} INFO - iteration 61, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2391} INFO -  at 19.3s,\testimator xgboost's best error=74.4525,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2218} INFO - iteration 62, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2391} INFO -  at 19.5s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:15] {2218} INFO - iteration 63, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:17] {2391} INFO -  at 21.6s,\testimator xgboost's best error=73.1234,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:17] {2218} INFO - iteration 64, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:18] {2391} INFO -  at 21.9s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:18] {2218} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:18] {2391} INFO -  at 22.3s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:18] {2218} INFO - iteration 66, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:35:19] {2391} INFO -  at 23.7s,\testimator catboost's best error=71.1061,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:19] {2218} INFO - iteration 67, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:20] {2391} INFO -  at 24.0s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:20] {2218} INFO - iteration 68, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:21] {2391} INFO -  at 25.0s,\testimator rf's best error=74.4666,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:21] {2218} INFO - iteration 69, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:21] {2391} INFO -  at 25.2s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:21] {2218} INFO - iteration 70, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:22] {2391} INFO -  at 26.3s,\testimator rf's best error=72.7419,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:22] {2218} INFO - iteration 71, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:22] {2391} INFO -  at 26.8s,\testimator xgboost's best error=73.1234,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:22] {2218} INFO - iteration 72, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:24] {2391} INFO -  at 28.2s,\testimator rf's best error=72.7419,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:24] {2218} INFO - iteration 73, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:24] {2391} INFO -  at 28.5s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:24] {2218} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2391} INFO -  at 29.8s,\testimator xgboost's best error=73.1234,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2218} INFO - iteration 75, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2391} INFO -  at 30.2s,\testimator extra_tree's best error=70.0520,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2218} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2391} INFO -  at 30.5s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2218} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2391} INFO -  at 30.7s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0520\n",
      "[flaml.automl.logger: 11-12 20:35:26] {2218} INFO - iteration 78, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:27] {2391} INFO -  at 30.9s,\testimator extra_tree's best error=70.0201,\tbest estimator extra_tree's best error=70.0201\n",
      "[flaml.automl.logger: 11-12 20:35:27] {2218} INFO - iteration 79, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2391} INFO -  at 31.9s,\testimator rf's best error=72.1834,\tbest estimator extra_tree's best error=70.0201\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2218} INFO - iteration 80, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2391} INFO -  at 32.3s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=70.0201\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2218} INFO - iteration 81, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2391} INFO -  at 32.6s,\testimator extra_tree's best error=69.6817,\tbest estimator extra_tree's best error=69.6817\n",
      "[flaml.automl.logger: 11-12 20:35:28] {2218} INFO - iteration 82, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2391} INFO -  at 32.9s,\testimator extra_tree's best error=69.6817,\tbest estimator extra_tree's best error=69.6817\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2218} INFO - iteration 83, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2391} INFO -  at 33.3s,\testimator extra_tree's best error=69.6817,\tbest estimator extra_tree's best error=69.6817\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2218} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2391} INFO -  at 33.7s,\testimator extra_tree's best error=69.6817,\tbest estimator extra_tree's best error=69.6817\n",
      "[flaml.automl.logger: 11-12 20:35:29] {2218} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:30] {2391} INFO -  at 34.2s,\testimator extra_tree's best error=69.6525,\tbest estimator extra_tree's best error=69.6525\n",
      "[flaml.automl.logger: 11-12 20:35:30] {2218} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:30] {2391} INFO -  at 34.5s,\testimator extra_tree's best error=69.5768,\tbest estimator extra_tree's best error=69.5768\n",
      "[flaml.automl.logger: 11-12 20:35:30] {2218} INFO - iteration 87, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:31] {2391} INFO -  at 35.0s,\testimator extra_tree's best error=69.5768,\tbest estimator extra_tree's best error=69.5768\n",
      "[flaml.automl.logger: 11-12 20:35:31] {2218} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:31] {2391} INFO -  at 35.4s,\testimator extra_tree's best error=69.5768,\tbest estimator extra_tree's best error=69.5768\n",
      "[flaml.automl.logger: 11-12 20:35:31] {2218} INFO - iteration 89, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:32] {2391} INFO -  at 35.9s,\testimator extra_tree's best error=69.5768,\tbest estimator extra_tree's best error=69.5768\n",
      "[flaml.automl.logger: 11-12 20:35:32] {2218} INFO - iteration 90, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:32] {2391} INFO -  at 36.4s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:32] {2218} INFO - iteration 91, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:33] {2391} INFO -  at 36.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:33] {2218} INFO - iteration 92, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:33] {2391} INFO -  at 37.4s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:33] {2218} INFO - iteration 93, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:34] {2391} INFO -  at 37.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:34] {2218} INFO - iteration 94, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:34] {2391} INFO -  at 38.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:34] {2218} INFO - iteration 95, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:36] {2391} INFO -  at 40.3s,\testimator rf's best error=71.4177,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:36] {2218} INFO - iteration 96, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:36] {2391} INFO -  at 40.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:36] {2218} INFO - iteration 97, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:37] {2391} INFO -  at 41.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:37] {2218} INFO - iteration 98, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:38] {2391} INFO -  at 41.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:38] {2218} INFO - iteration 99, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:35:42] {2391} INFO -  at 45.8s,\testimator xgboost's best error=73.1234,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:42] {2218} INFO - iteration 100, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:43] {2391} INFO -  at 47.2s,\testimator rf's best error=71.4177,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:43] {2218} INFO - iteration 101, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2391} INFO -  at 47.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2218} INFO - iteration 102, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2391} INFO -  at 48.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2218} INFO - iteration 103, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2391} INFO -  at 48.5s,\testimator lgbm's best error=70.6582,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:44] {2218} INFO - iteration 104, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:45] {2391} INFO -  at 48.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:45] {2218} INFO - iteration 105, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:47] {2391} INFO -  at 51.4s,\testimator rf's best error=71.4177,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:47] {2218} INFO - iteration 106, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:48] {2391} INFO -  at 52.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:48] {2218} INFO - iteration 107, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:48] {2391} INFO -  at 52.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:48] {2218} INFO - iteration 108, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:35:51] {2391} INFO -  at 55.1s,\testimator catboost's best error=71.1061,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:51] {2218} INFO - iteration 109, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:51] {2391} INFO -  at 55.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:51] {2218} INFO - iteration 110, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:52] {2391} INFO -  at 56.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:52] {2218} INFO - iteration 111, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:52] {2391} INFO -  at 56.5s,\testimator lgbm's best error=70.2960,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:52] {2218} INFO - iteration 112, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:53] {2391} INFO -  at 57.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:53] {2218} INFO - iteration 113, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:53] {2391} INFO -  at 57.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:53] {2218} INFO - iteration 114, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:54] {2391} INFO -  at 58.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:54] {2218} INFO - iteration 115, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:35:55] {2391} INFO -  at 59.5s,\testimator rf's best error=71.4177,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:55] {2218} INFO - iteration 116, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:35:55] {2391} INFO -  at 59.6s,\testimator lgbm's best error=70.2960,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:55] {2218} INFO - iteration 117, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2391} INFO -  at 60.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2218} INFO - iteration 118, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2391} INFO -  at 60.5s,\testimator xgb_limitdepth's best error=74.3243,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2218} INFO - iteration 119, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2391} INFO -  at 60.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:56] {2218} INFO - iteration 120, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2391} INFO -  at 61.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2218} INFO - iteration 121, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2391} INFO -  at 61.5s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2218} INFO - iteration 122, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2391} INFO -  at 61.6s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:57] {2218} INFO - iteration 123, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:59] {2391} INFO -  at 63.1s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:59] {2218} INFO - iteration 124, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:35:59] {2391} INFO -  at 63.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:35:59] {2218} INFO - iteration 125, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:36:03] {2391} INFO -  at 66.8s,\testimator rf's best error=70.7076,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:03] {2218} INFO - iteration 126, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:04] {2391} INFO -  at 67.8s,\testimator lgbm's best error=70.2960,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:04] {2218} INFO - iteration 127, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:04] {2391} INFO -  at 68.2s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:04] {2218} INFO - iteration 128, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2391} INFO -  at 68.9s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2218} INFO - iteration 129, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2391} INFO -  at 69.3s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2218} INFO - iteration 130, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2391} INFO -  at 69.4s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:05] {2218} INFO - iteration 131, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2391} INFO -  at 70.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2218} INFO - iteration 132, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2391} INFO -  at 70.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2218} INFO - iteration 133, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2391} INFO -  at 70.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:06] {2218} INFO - iteration 134, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:07] {2391} INFO -  at 71.0s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:07] {2218} INFO - iteration 135, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:36:11] {2391} INFO -  at 74.8s,\testimator rf's best error=70.4979,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:11] {2218} INFO - iteration 136, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:11] {2391} INFO -  at 75.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:11] {2218} INFO - iteration 137, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:12] {2391} INFO -  at 76.1s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:12] {2218} INFO - iteration 138, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:12] {2391} INFO -  at 76.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:12] {2218} INFO - iteration 139, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:13] {2391} INFO -  at 77.3s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:13] {2218} INFO - iteration 140, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:13] {2391} INFO -  at 77.5s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:13] {2218} INFO - iteration 141, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:14] {2391} INFO -  at 78.4s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:14] {2218} INFO - iteration 142, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:15] {2391} INFO -  at 79.4s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:15] {2218} INFO - iteration 143, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:36:39] {2391} INFO -  at 103.6s,\testimator xgboost's best error=72.4405,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:39] {2218} INFO - iteration 144, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:40] {2391} INFO -  at 103.9s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:40] {2218} INFO - iteration 145, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:40] {2391} INFO -  at 104.2s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:40] {2218} INFO - iteration 146, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:41] {2391} INFO -  at 104.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:41] {2218} INFO - iteration 147, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:41] {2391} INFO -  at 105.0s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:41] {2218} INFO - iteration 148, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:42] {2391} INFO -  at 106.3s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:42] {2218} INFO - iteration 149, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:42] {2391} INFO -  at 106.6s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:42] {2218} INFO - iteration 150, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:43] {2391} INFO -  at 107.1s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:43] {2218} INFO - iteration 151, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:43] {2391} INFO -  at 107.4s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:43] {2218} INFO - iteration 152, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:36:46] {2391} INFO -  at 109.8s,\testimator rf's best error=70.4979,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:46] {2218} INFO - iteration 153, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:46] {2391} INFO -  at 110.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:46] {2218} INFO - iteration 154, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:47] {2391} INFO -  at 111.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:47] {2218} INFO - iteration 155, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:47] {2391} INFO -  at 111.6s,\testimator lgbm's best error=69.8087,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:47] {2218} INFO - iteration 156, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:48] {2391} INFO -  at 112.2s,\testimator lgbm's best error=69.5091,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:48] {2218} INFO - iteration 157, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:48] {2391} INFO -  at 112.4s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:48] {2218} INFO - iteration 158, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:49] {2391} INFO -  at 112.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:49] {2218} INFO - iteration 159, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:50] {2391} INFO -  at 114.1s,\testimator lgbm's best error=69.2009,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:50] {2218} INFO - iteration 160, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:51] {2391} INFO -  at 115.2s,\testimator lgbm's best error=69.2009,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:51] {2218} INFO - iteration 161, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:52] {2391} INFO -  at 115.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:52] {2218} INFO - iteration 162, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:54] {2391} INFO -  at 118.6s,\testimator lgbm's best error=69.2009,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:54] {2218} INFO - iteration 163, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2391} INFO -  at 119.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2218} INFO - iteration 164, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2391} INFO -  at 119.3s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2218} INFO - iteration 165, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2391} INFO -  at 119.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:55] {2218} INFO - iteration 166, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:36:56] {2391} INFO -  at 120.5s,\testimator lgbm's best error=69.2009,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:56] {2218} INFO - iteration 167, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:57] {2391} INFO -  at 121.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:57] {2218} INFO - iteration 168, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:36:57] {2391} INFO -  at 121.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:57] {2218} INFO - iteration 169, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:36:59] {2391} INFO -  at 123.0s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:36:59] {2218} INFO - iteration 170, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:01] {2391} INFO -  at 125.4s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:01] {2218} INFO - iteration 171, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:04] {2391} INFO -  at 127.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:04] {2218} INFO - iteration 172, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:07] {2391} INFO -  at 131.3s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:07] {2218} INFO - iteration 173, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:08] {2391} INFO -  at 131.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:08] {2218} INFO - iteration 174, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:08] {2391} INFO -  at 132.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:08] {2218} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:09] {2391} INFO -  at 132.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:09] {2218} INFO - iteration 176, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:09] {2391} INFO -  at 133.7s,\testimator catboost's best error=71.1061,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:09] {2218} INFO - iteration 177, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:10] {2391} INFO -  at 133.8s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:10] {2218} INFO - iteration 178, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:10] {2391} INFO -  at 134.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:10] {2218} INFO - iteration 179, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2391} INFO -  at 134.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2218} INFO - iteration 180, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2391} INFO -  at 135.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2218} INFO - iteration 181, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2391} INFO -  at 135.5s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:11] {2218} INFO - iteration 182, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:12] {2391} INFO -  at 136.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:12] {2218} INFO - iteration 183, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:37:16] {2391} INFO -  at 140.1s,\testimator rf's best error=70.4979,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:16] {2218} INFO - iteration 184, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:19] {2391} INFO -  at 142.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:19] {2218} INFO - iteration 185, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:19] {2391} INFO -  at 143.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:19] {2218} INFO - iteration 186, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:20] {2391} INFO -  at 143.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:20] {2218} INFO - iteration 187, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:37:23] {2391} INFO -  at 146.8s,\testimator rf's best error=70.3506,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:23] {2218} INFO - iteration 188, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:23] {2391} INFO -  at 147.3s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:23] {2218} INFO - iteration 189, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:26] {2391} INFO -  at 150.3s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:26] {2218} INFO - iteration 190, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:29] {2391} INFO -  at 152.9s,\testimator catboost's best error=71.1061,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:29] {2218} INFO - iteration 191, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:29] {2391} INFO -  at 153.2s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:29] {2218} INFO - iteration 192, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:37:32] {2391} INFO -  at 156.5s,\testimator rf's best error=70.3506,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:32] {2218} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:33] {2391} INFO -  at 156.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:33] {2218} INFO - iteration 194, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:33] {2391} INFO -  at 157.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:33] {2218} INFO - iteration 195, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:34] {2391} INFO -  at 158.0s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:34] {2218} INFO - iteration 196, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:35] {2391} INFO -  at 159.0s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:35] {2218} INFO - iteration 197, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:39] {2391} INFO -  at 163.2s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:39] {2218} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:39] {2391} INFO -  at 163.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:39] {2218} INFO - iteration 199, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:40] {2391} INFO -  at 163.9s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:40] {2218} INFO - iteration 200, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:40] {2391} INFO -  at 164.2s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:40] {2218} INFO - iteration 201, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:41] {2391} INFO -  at 164.8s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:41] {2218} INFO - iteration 202, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:42] {2391} INFO -  at 166.2s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:42] {2218} INFO - iteration 203, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2391} INFO -  at 167.9s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2218} INFO - iteration 204, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2391} INFO -  at 168.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2218} INFO - iteration 205, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2391} INFO -  at 168.7s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:44] {2218} INFO - iteration 206, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:45] {2391} INFO -  at 169.0s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:45] {2218} INFO - iteration 207, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:45] {2391} INFO -  at 169.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:45] {2218} INFO - iteration 208, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:46] {2391} INFO -  at 169.9s,\testimator xgb_limitdepth's best error=71.9602,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:46] {2218} INFO - iteration 209, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:46] {2391} INFO -  at 170.4s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:46] {2218} INFO - iteration 210, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:48] {2391} INFO -  at 172.3s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:48] {2218} INFO - iteration 211, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:48] {2391} INFO -  at 172.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:48] {2218} INFO - iteration 212, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:49] {2391} INFO -  at 173.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:49] {2218} INFO - iteration 213, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:49] {2391} INFO -  at 173.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:49] {2218} INFO - iteration 214, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:50] {2391} INFO -  at 174.4s,\testimator xgb_limitdepth's best error=71.2665,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:50] {2218} INFO - iteration 215, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:37:50] {2391} INFO -  at 174.7s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:50] {2218} INFO - iteration 216, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:37:51] {2391} INFO -  at 175.5s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:51] {2218} INFO - iteration 217, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:37:52] {2391} INFO -  at 176.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:52] {2218} INFO - iteration 218, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:55] {2391} INFO -  at 179.3s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:55] {2218} INFO - iteration 219, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:37:57] {2391} INFO -  at 181.0s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:37:57] {2218} INFO - iteration 220, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:38:04] {2391} INFO -  at 187.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:04] {2218} INFO - iteration 221, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:06] {2391} INFO -  at 189.8s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:06] {2218} INFO - iteration 222, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:38:06] {2391} INFO -  at 190.6s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:06] {2218} INFO - iteration 223, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:07] {2391} INFO -  at 191.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:07] {2218} INFO - iteration 224, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:07] {2391} INFO -  at 191.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:07] {2218} INFO - iteration 225, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:38:10] {2391} INFO -  at 193.9s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:10] {2218} INFO - iteration 226, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:13] {2391} INFO -  at 196.9s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:13] {2218} INFO - iteration 227, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:15] {2391} INFO -  at 199.2s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:15] {2218} INFO - iteration 228, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:19] {2391} INFO -  at 203.7s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:19] {2218} INFO - iteration 229, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:20] {2391} INFO -  at 204.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:20] {2218} INFO - iteration 230, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:22] {2391} INFO -  at 206.0s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:22] {2218} INFO - iteration 231, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:22] {2391} INFO -  at 206.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:22] {2218} INFO - iteration 232, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:24] {2391} INFO -  at 208.1s,\testimator catboost's best error=70.1624,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:24] {2218} INFO - iteration 233, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:24] {2391} INFO -  at 208.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:24] {2218} INFO - iteration 234, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:25] {2391} INFO -  at 209.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:25] {2218} INFO - iteration 235, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:25] {2391} INFO -  at 209.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:25] {2218} INFO - iteration 236, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:27] {2391} INFO -  at 211.6s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:27] {2218} INFO - iteration 237, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:30] {2391} INFO -  at 214.5s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:30] {2218} INFO - iteration 238, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:31] {2391} INFO -  at 215.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:31] {2218} INFO - iteration 239, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:37] {2391} INFO -  at 220.9s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:37] {2218} INFO - iteration 240, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:38] {2391} INFO -  at 222.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:38] {2218} INFO - iteration 241, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:38:40] {2391} INFO -  at 224.5s,\testimator xgboost's best error=72.4405,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:40] {2218} INFO - iteration 242, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:38:43] {2391} INFO -  at 227.0s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:43] {2218} INFO - iteration 243, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:38:49] {2391} INFO -  at 232.9s,\testimator xgboost's best error=71.7493,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:49] {2218} INFO - iteration 244, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:38:50] {2391} INFO -  at 234.1s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:50] {2218} INFO - iteration 245, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:50] {2391} INFO -  at 234.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:50] {2218} INFO - iteration 246, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:38:52] {2391} INFO -  at 236.5s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:52] {2218} INFO - iteration 247, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:38:53] {2391} INFO -  at 237.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:38:53] {2218} INFO - iteration 248, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:39:17] {2391} INFO -  at 261.1s,\testimator xgboost's best error=71.7493,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:17] {2218} INFO - iteration 249, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:39:22] {2391} INFO -  at 266.3s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:22] {2218} INFO - iteration 250, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:23] {2391} INFO -  at 266.9s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:23] {2218} INFO - iteration 251, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:23] {2391} INFO -  at 267.4s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:23] {2218} INFO - iteration 252, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:39:32] {2391} INFO -  at 275.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:32] {2218} INFO - iteration 253, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:39:36] {2391} INFO -  at 280.6s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:36] {2218} INFO - iteration 254, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:37] {2391} INFO -  at 281.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:37] {2218} INFO - iteration 255, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:37] {2391} INFO -  at 281.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:37] {2218} INFO - iteration 256, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:39:39] {2391} INFO -  at 282.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:39] {2218} INFO - iteration 257, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:39:39] {2391} INFO -  at 283.3s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:39] {2218} INFO - iteration 258, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:39:41] {2391} INFO -  at 284.9s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:41] {2218} INFO - iteration 259, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:41] {2391} INFO -  at 285.4s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:41] {2218} INFO - iteration 260, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:42] {2391} INFO -  at 286.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:42] {2218} INFO - iteration 261, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:42] {2391} INFO -  at 286.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:42] {2218} INFO - iteration 262, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:39:46] {2391} INFO -  at 289.9s,\testimator xgboost's best error=71.7493,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:46] {2218} INFO - iteration 263, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:46] {2391} INFO -  at 290.5s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:46] {2218} INFO - iteration 264, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:39:48] {2391} INFO -  at 292.0s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:48] {2218} INFO - iteration 265, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:48] {2391} INFO -  at 292.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:48] {2218} INFO - iteration 266, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:49] {2391} INFO -  at 293.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:49] {2218} INFO - iteration 267, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:49] {2391} INFO -  at 293.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:49] {2218} INFO - iteration 268, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:50] {2391} INFO -  at 294.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:50] {2218} INFO - iteration 269, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:50] {2391} INFO -  at 294.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:50] {2218} INFO - iteration 270, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:51] {2391} INFO -  at 295.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:51] {2218} INFO - iteration 271, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:39:58] {2391} INFO -  at 301.8s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:58] {2218} INFO - iteration 272, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:39:58] {2391} INFO -  at 302.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:39:58] {2218} INFO - iteration 273, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:06] {2391} INFO -  at 310.4s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:06] {2218} INFO - iteration 274, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:07] {2391} INFO -  at 311.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:07] {2218} INFO - iteration 275, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:11] {2391} INFO -  at 315.0s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:11] {2218} INFO - iteration 276, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:17] {2391} INFO -  at 321.2s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:17] {2218} INFO - iteration 277, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:40:19] {2391} INFO -  at 323.2s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:19] {2218} INFO - iteration 278, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:19] {2391} INFO -  at 323.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:19] {2218} INFO - iteration 279, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:21] {2391} INFO -  at 325.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:21] {2218} INFO - iteration 280, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:22] {2391} INFO -  at 326.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:22] {2218} INFO - iteration 281, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:22] {2391} INFO -  at 326.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:22] {2218} INFO - iteration 282, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:40:27] {2391} INFO -  at 331.1s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:27] {2218} INFO - iteration 283, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:40:28] {2391} INFO -  at 332.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:28] {2218} INFO - iteration 284, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:40:30] {2391} INFO -  at 334.6s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:30] {2218} INFO - iteration 285, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:31] {2391} INFO -  at 335.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:31] {2218} INFO - iteration 286, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:31] {2391} INFO -  at 335.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:31] {2218} INFO - iteration 287, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:32] {2391} INFO -  at 336.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:32] {2218} INFO - iteration 288, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:32] {2391} INFO -  at 336.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:32] {2218} INFO - iteration 289, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:40:37] {2391} INFO -  at 341.1s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:37] {2218} INFO - iteration 290, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:40:43] {2391} INFO -  at 347.2s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:43] {2218} INFO - iteration 291, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:43] {2391} INFO -  at 347.7s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:43] {2218} INFO - iteration 292, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:40:54] {2391} INFO -  at 358.0s,\testimator xgboost's best error=71.1814,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:54] {2218} INFO - iteration 293, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:54] {2391} INFO -  at 358.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:54] {2218} INFO - iteration 294, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:55] {2391} INFO -  at 359.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:55] {2218} INFO - iteration 295, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:40:55] {2391} INFO -  at 359.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:40:55] {2218} INFO - iteration 296, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:41:06] {2391} INFO -  at 370.6s,\testimator xgboost's best error=71.0327,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:06] {2218} INFO - iteration 297, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:07] {2391} INFO -  at 371.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:07] {2218} INFO - iteration 298, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:07] {2391} INFO -  at 371.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:07] {2218} INFO - iteration 299, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:41:11] {2391} INFO -  at 375.4s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:11] {2218} INFO - iteration 300, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:41:13] {2391} INFO -  at 377.3s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:13] {2218} INFO - iteration 301, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:41:13] {2391} INFO -  at 377.7s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:13] {2218} INFO - iteration 302, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:41:21] {2391} INFO -  at 385.1s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:21] {2218} INFO - iteration 303, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:41:37] {2391} INFO -  at 401.7s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:37] {2218} INFO - iteration 304, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:38] {2391} INFO -  at 402.2s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:38] {2218} INFO - iteration 305, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:41:41] {2391} INFO -  at 405.1s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:41] {2218} INFO - iteration 306, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:41] {2391} INFO -  at 405.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:41] {2218} INFO - iteration 307, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:42] {2391} INFO -  at 406.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:42] {2218} INFO - iteration 308, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:42] {2391} INFO -  at 406.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:42] {2218} INFO - iteration 309, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:41:43] {2391} INFO -  at 407.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:43] {2218} INFO - iteration 310, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:41:46] {2391} INFO -  at 410.2s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:41:46] {2218} INFO - iteration 311, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:42:15] {2391} INFO -  at 439.5s,\testimator xgboost's best error=70.9721,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:15] {2218} INFO - iteration 312, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:19] {2391} INFO -  at 443.1s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:19] {2218} INFO - iteration 313, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:21] {2391} INFO -  at 445.4s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:21] {2218} INFO - iteration 314, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:22] {2391} INFO -  at 446.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:22] {2218} INFO - iteration 315, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:24] {2391} INFO -  at 448.7s,\testimator rf's best error=69.6418,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:24] {2218} INFO - iteration 316, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:25] {2391} INFO -  at 449.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:25] {2218} INFO - iteration 317, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:26] {2391} INFO -  at 449.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:26] {2218} INFO - iteration 318, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:29] {2391} INFO -  at 453.3s,\testimator rf's best error=69.4455,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:29] {2218} INFO - iteration 319, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:42:32] {2391} INFO -  at 455.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:32] {2218} INFO - iteration 320, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:38] {2391} INFO -  at 462.5s,\testimator rf's best error=69.4455,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:38] {2218} INFO - iteration 321, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:39] {2391} INFO -  at 463.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:39] {2218} INFO - iteration 322, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:39] {2391} INFO -  at 463.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:39] {2218} INFO - iteration 323, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:41] {2391} INFO -  at 465.4s,\testimator rf's best error=69.4455,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:41] {2218} INFO - iteration 324, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:43] {2391} INFO -  at 467.5s,\testimator rf's best error=69.4455,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:43] {2218} INFO - iteration 325, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:44] {2391} INFO -  at 468.0s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:44] {2218} INFO - iteration 326, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:44] {2391} INFO -  at 468.6s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:44] {2218} INFO - iteration 327, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:45] {2391} INFO -  at 469.1s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:45] {2218} INFO - iteration 328, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:50] {2391} INFO -  at 474.1s,\testimator rf's best error=69.4455,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:50] {2218} INFO - iteration 329, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:42:52] {2391} INFO -  at 476.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:52] {2218} INFO - iteration 330, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:53] {2391} INFO -  at 477.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:53] {2218} INFO - iteration 331, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:53] {2391} INFO -  at 477.8s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:53] {2218} INFO - iteration 332, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:42:54] {2391} INFO -  at 478.7s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:54] {2218} INFO - iteration 333, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:55] {2391} INFO -  at 479.3s,\testimator extra_tree's best error=68.9260,\tbest estimator extra_tree's best error=68.9260\n",
      "[flaml.automl.logger: 11-12 20:42:55] {2218} INFO - iteration 334, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:56] {2391} INFO -  at 479.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:42:56] {2218} INFO - iteration 335, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:56] {2391} INFO -  at 480.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:42:56] {2218} INFO - iteration 336, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:42:58] {2391} INFO -  at 482.3s,\testimator rf's best error=69.4437,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:42:58] {2218} INFO - iteration 337, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:42:59] {2391} INFO -  at 482.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:42:59] {2218} INFO - iteration 338, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:43:11] {2391} INFO -  at 495.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:11] {2218} INFO - iteration 339, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:12] {2391} INFO -  at 496.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:12] {2218} INFO - iteration 340, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:13] {2391} INFO -  at 496.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:13] {2218} INFO - iteration 341, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:43:19] {2391} INFO -  at 503.6s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:19] {2218} INFO - iteration 342, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:20] {2391} INFO -  at 504.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:20] {2218} INFO - iteration 343, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:20] {2391} INFO -  at 504.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:20] {2218} INFO - iteration 344, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:21] {2391} INFO -  at 505.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:21] {2218} INFO - iteration 345, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:21] {2391} INFO -  at 505.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:21] {2218} INFO - iteration 346, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:22] {2391} INFO -  at 506.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:22] {2218} INFO - iteration 347, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:23] {2391} INFO -  at 506.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:23] {2218} INFO - iteration 348, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:23] {2391} INFO -  at 507.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:23] {2218} INFO - iteration 349, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:24] {2391} INFO -  at 507.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:24] {2218} INFO - iteration 350, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:43:24] {2391} INFO -  at 508.5s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:24] {2218} INFO - iteration 351, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:25] {2391} INFO -  at 509.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:25] {2218} INFO - iteration 352, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:25] {2391} INFO -  at 509.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:25] {2218} INFO - iteration 353, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:26] {2391} INFO -  at 510.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:26] {2218} INFO - iteration 354, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:26] {2391} INFO -  at 510.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:26] {2218} INFO - iteration 355, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:43:36] {2391} INFO -  at 519.9s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:36] {2218} INFO - iteration 356, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:36] {2391} INFO -  at 520.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:36] {2218} INFO - iteration 357, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:37] {2391} INFO -  at 521.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:37] {2218} INFO - iteration 358, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:37] {2391} INFO -  at 521.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:37] {2218} INFO - iteration 359, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:38] {2391} INFO -  at 522.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:38] {2218} INFO - iteration 360, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:38] {2391} INFO -  at 522.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:38] {2218} INFO - iteration 361, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:39] {2391} INFO -  at 523.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:39] {2218} INFO - iteration 362, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:39] {2391} INFO -  at 523.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:39] {2218} INFO - iteration 363, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:40] {2391} INFO -  at 524.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:40] {2218} INFO - iteration 364, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:40] {2391} INFO -  at 524.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:40] {2218} INFO - iteration 365, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:43:41] {2391} INFO -  at 524.9s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:41] {2218} INFO - iteration 366, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:41] {2391} INFO -  at 525.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:41] {2218} INFO - iteration 367, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:43:44] {2391} INFO -  at 528.5s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:44] {2218} INFO - iteration 368, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:45] {2391} INFO -  at 529.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:45] {2218} INFO - iteration 369, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:45] {2391} INFO -  at 529.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:45] {2218} INFO - iteration 370, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:46] {2391} INFO -  at 530.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:46] {2218} INFO - iteration 371, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:46] {2391} INFO -  at 530.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:46] {2218} INFO - iteration 372, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:43:47] {2391} INFO -  at 531.8s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:47] {2218} INFO - iteration 373, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:48] {2391} INFO -  at 532.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:48] {2218} INFO - iteration 374, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:49] {2391} INFO -  at 532.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:49] {2218} INFO - iteration 375, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:43:57] {2391} INFO -  at 540.9s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:57] {2218} INFO - iteration 376, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:43:57] {2391} INFO -  at 541.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:43:57] {2218} INFO - iteration 377, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:02] {2391} INFO -  at 546.2s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:02] {2218} INFO - iteration 378, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:44:06] {2391} INFO -  at 550.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:06] {2218} INFO - iteration 379, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:07] {2391} INFO -  at 551.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:07] {2218} INFO - iteration 380, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:08] {2391} INFO -  at 552.6s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:08] {2218} INFO - iteration 381, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:09] {2391} INFO -  at 553.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:09] {2218} INFO - iteration 382, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:09] {2391} INFO -  at 553.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:09] {2218} INFO - iteration 383, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:10] {2391} INFO -  at 554.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:10] {2218} INFO - iteration 384, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:11] {2391} INFO -  at 554.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:11] {2218} INFO - iteration 385, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2391} INFO -  at 559.8s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2218} INFO - iteration 386, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2391} INFO -  at 560.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2218} INFO - iteration 387, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2391} INFO -  at 560.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:16] {2218} INFO - iteration 388, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:17] {2391} INFO -  at 561.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:17] {2218} INFO - iteration 389, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:44:22] {2391} INFO -  at 566.5s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:22] {2218} INFO - iteration 390, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:23] {2391} INFO -  at 567.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:23] {2218} INFO - iteration 391, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:23] {2391} INFO -  at 567.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:23] {2218} INFO - iteration 392, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:24] {2391} INFO -  at 568.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:24] {2218} INFO - iteration 393, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:24] {2391} INFO -  at 568.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:24] {2218} INFO - iteration 394, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:30] {2391} INFO -  at 574.7s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:30] {2218} INFO - iteration 395, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:31] {2391} INFO -  at 575.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:31] {2218} INFO - iteration 396, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:44:42] {2391} INFO -  at 586.4s,\testimator xgboost's best error=70.9721,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:42] {2218} INFO - iteration 397, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:43] {2391} INFO -  at 587.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:43] {2218} INFO - iteration 398, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:43] {2391} INFO -  at 587.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:43] {2218} INFO - iteration 399, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:45] {2391} INFO -  at 589.0s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:45] {2218} INFO - iteration 400, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:45] {2391} INFO -  at 589.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:45] {2218} INFO - iteration 401, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:46] {2391} INFO -  at 590.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:46] {2218} INFO - iteration 402, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:46] {2391} INFO -  at 590.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:46] {2218} INFO - iteration 403, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:47] {2391} INFO -  at 591.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:47] {2218} INFO - iteration 404, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:47] {2391} INFO -  at 591.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:47] {2218} INFO - iteration 405, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:48] {2391} INFO -  at 592.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:48] {2218} INFO - iteration 406, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:48] {2391} INFO -  at 592.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:48] {2218} INFO - iteration 407, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:49] {2391} INFO -  at 593.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:49] {2218} INFO - iteration 408, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:50] {2391} INFO -  at 594.6s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:50] {2218} INFO - iteration 409, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:51] {2391} INFO -  at 595.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:51] {2218} INFO - iteration 410, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:51] {2391} INFO -  at 595.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:51] {2218} INFO - iteration 411, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:52] {2391} INFO -  at 596.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:52] {2218} INFO - iteration 412, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:52] {2391} INFO -  at 596.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:52] {2218} INFO - iteration 413, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:53] {2391} INFO -  at 597.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:53] {2218} INFO - iteration 414, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:44:57] {2391} INFO -  at 601.4s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:57] {2218} INFO - iteration 415, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:44:58] {2391} INFO -  at 601.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:44:58] {2218} INFO - iteration 416, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:45:05] {2391} INFO -  at 609.1s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:05] {2218} INFO - iteration 417, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:05] {2391} INFO -  at 609.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:05] {2218} INFO - iteration 418, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:06] {2391} INFO -  at 610.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:06] {2218} INFO - iteration 419, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:06] {2391} INFO -  at 610.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:06] {2218} INFO - iteration 420, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:07] {2391} INFO -  at 611.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:07] {2218} INFO - iteration 421, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:07] {2391} INFO -  at 611.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:07] {2218} INFO - iteration 422, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:08] {2391} INFO -  at 612.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:08] {2218} INFO - iteration 423, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:08] {2391} INFO -  at 612.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:08] {2218} INFO - iteration 424, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:09] {2391} INFO -  at 613.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:09] {2218} INFO - iteration 425, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:10] {2391} INFO -  at 613.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:10] {2218} INFO - iteration 426, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:45:13] {2391} INFO -  at 617.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:13] {2218} INFO - iteration 427, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:23] {2391} INFO -  at 626.9s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:23] {2218} INFO - iteration 428, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:23] {2391} INFO -  at 627.7s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:23] {2218} INFO - iteration 429, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:24] {2391} INFO -  at 628.2s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:24] {2218} INFO - iteration 430, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:24] {2391} INFO -  at 628.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:24] {2218} INFO - iteration 431, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:45:26] {2391} INFO -  at 630.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:26] {2218} INFO - iteration 432, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:41] {2391} INFO -  at 644.9s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:41] {2218} INFO - iteration 433, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:41] {2391} INFO -  at 645.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:41] {2218} INFO - iteration 434, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:42] {2391} INFO -  at 645.9s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:42] {2218} INFO - iteration 435, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:42] {2391} INFO -  at 646.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:42] {2218} INFO - iteration 436, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:43] {2391} INFO -  at 647.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:43] {2218} INFO - iteration 437, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:43] {2391} INFO -  at 647.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:43] {2218} INFO - iteration 438, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:44] {2391} INFO -  at 648.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:44] {2218} INFO - iteration 439, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:44] {2391} INFO -  at 648.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:44] {2218} INFO - iteration 440, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:45] {2391} INFO -  at 649.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:45] {2218} INFO - iteration 441, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:46] {2391} INFO -  at 649.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:46] {2218} INFO - iteration 442, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:46] {2391} INFO -  at 650.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:46] {2218} INFO - iteration 443, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:52] {2391} INFO -  at 655.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:52] {2218} INFO - iteration 444, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:52] {2391} INFO -  at 656.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:52] {2218} INFO - iteration 445, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:45:57] {2391} INFO -  at 661.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:57] {2218} INFO - iteration 446, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:45:58] {2391} INFO -  at 662.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:58] {2218} INFO - iteration 447, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:45:59] {2391} INFO -  at 663.5s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:45:59] {2218} INFO - iteration 448, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:00] {2391} INFO -  at 664.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:00] {2218} INFO - iteration 449, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:46:01] {2391} INFO -  at 665.6s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:01] {2218} INFO - iteration 450, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:02] {2391} INFO -  at 666.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:02] {2218} INFO - iteration 451, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:02] {2391} INFO -  at 666.6s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:02] {2218} INFO - iteration 452, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:03] {2391} INFO -  at 667.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:03] {2218} INFO - iteration 453, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:03] {2391} INFO -  at 667.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:03] {2218} INFO - iteration 454, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:46:10] {2391} INFO -  at 674.5s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:10] {2218} INFO - iteration 455, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:46:12] {2391} INFO -  at 676.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:12] {2218} INFO - iteration 456, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:12] {2391} INFO -  at 676.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:12] {2218} INFO - iteration 457, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:13] {2391} INFO -  at 677.4s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:13] {2218} INFO - iteration 458, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:14] {2391} INFO -  at 678.0s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:14] {2218} INFO - iteration 459, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:14] {2391} INFO -  at 678.5s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:14] {2218} INFO - iteration 460, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:15] {2391} INFO -  at 679.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:15] {2218} INFO - iteration 461, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:15] {2391} INFO -  at 679.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:15] {2218} INFO - iteration 462, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:16] {2391} INFO -  at 680.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:16] {2218} INFO - iteration 463, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:17] {2391} INFO -  at 680.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:17] {2218} INFO - iteration 464, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:17] {2391} INFO -  at 681.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:17] {2218} INFO - iteration 465, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:46:41] {2391} INFO -  at 704.8s,\testimator xgboost's best error=70.9721,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:41] {2218} INFO - iteration 466, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:46:44] {2391} INFO -  at 708.6s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:44] {2218} INFO - iteration 467, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:45] {2391} INFO -  at 709.1s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:45] {2218} INFO - iteration 468, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:46:49] {2391} INFO -  at 713.7s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:49] {2218} INFO - iteration 469, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:50] {2391} INFO -  at 714.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:50] {2218} INFO - iteration 470, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:50] {2391} INFO -  at 714.7s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:50] {2218} INFO - iteration 471, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:51] {2391} INFO -  at 715.2s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:51] {2218} INFO - iteration 472, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:52] {2391} INFO -  at 715.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:52] {2218} INFO - iteration 473, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:52] {2391} INFO -  at 716.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:52] {2218} INFO - iteration 474, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:53] {2391} INFO -  at 716.8s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:53] {2218} INFO - iteration 475, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:53] {2391} INFO -  at 717.3s,\testimator extra_tree's best error=68.8126,\tbest estimator extra_tree's best error=68.8126\n",
      "[flaml.automl.logger: 11-12 20:46:53] {2218} INFO - iteration 476, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:54] {2391} INFO -  at 717.9s,\testimator extra_tree's best error=68.7784,\tbest estimator extra_tree's best error=68.7784\n",
      "[flaml.automl.logger: 11-12 20:46:54] {2218} INFO - iteration 477, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:54] {2391} INFO -  at 718.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:54] {2218} INFO - iteration 478, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:55] {2391} INFO -  at 719.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:55] {2218} INFO - iteration 479, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:56] {2391} INFO -  at 719.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:56] {2218} INFO - iteration 480, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:56] {2391} INFO -  at 720.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:56] {2218} INFO - iteration 481, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:57] {2391} INFO -  at 721.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:57] {2218} INFO - iteration 482, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:58] {2391} INFO -  at 721.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:58] {2218} INFO - iteration 483, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:58] {2391} INFO -  at 722.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:58] {2218} INFO - iteration 484, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:46:59] {2391} INFO -  at 723.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:46:59] {2218} INFO - iteration 485, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:00] {2391} INFO -  at 723.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:00] {2218} INFO - iteration 486, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:00] {2391} INFO -  at 724.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:00] {2218} INFO - iteration 487, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:01] {2391} INFO -  at 725.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:01] {2218} INFO - iteration 488, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:01] {2391} INFO -  at 725.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:01] {2218} INFO - iteration 489, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:02] {2391} INFO -  at 726.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:02] {2218} INFO - iteration 490, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:03] {2391} INFO -  at 727.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:03] {2218} INFO - iteration 491, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:03] {2391} INFO -  at 727.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:03] {2218} INFO - iteration 492, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:04] {2391} INFO -  at 728.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:04] {2218} INFO - iteration 493, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:47:08] {2391} INFO -  at 731.8s,\testimator rf's best error=69.4437,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:08] {2218} INFO - iteration 494, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:08] {2391} INFO -  at 732.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:08] {2218} INFO - iteration 495, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:09] {2391} INFO -  at 733.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:09] {2218} INFO - iteration 496, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:10] {2391} INFO -  at 733.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:10] {2218} INFO - iteration 497, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:10] {2391} INFO -  at 734.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:10] {2218} INFO - iteration 498, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:11] {2391} INFO -  at 735.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:11] {2218} INFO - iteration 499, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:12] {2391} INFO -  at 735.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:12] {2218} INFO - iteration 500, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:12] {2391} INFO -  at 736.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:12] {2218} INFO - iteration 501, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:13] {2391} INFO -  at 737.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:13] {2218} INFO - iteration 502, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:14] {2391} INFO -  at 737.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:14] {2218} INFO - iteration 503, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:14] {2391} INFO -  at 738.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:14] {2218} INFO - iteration 504, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:15] {2391} INFO -  at 739.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:15] {2218} INFO - iteration 505, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:16] {2391} INFO -  at 739.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:16] {2218} INFO - iteration 506, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:16] {2391} INFO -  at 740.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:16] {2218} INFO - iteration 507, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:17] {2391} INFO -  at 741.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:17] {2218} INFO - iteration 508, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:18] {2391} INFO -  at 741.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:18] {2218} INFO - iteration 509, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:18] {2391} INFO -  at 742.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:18] {2218} INFO - iteration 510, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:19] {2391} INFO -  at 743.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:19] {2218} INFO - iteration 511, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:20] {2391} INFO -  at 743.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:20] {2218} INFO - iteration 512, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:20] {2391} INFO -  at 744.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:20] {2218} INFO - iteration 513, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:21] {2391} INFO -  at 745.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:21] {2218} INFO - iteration 514, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:22] {2391} INFO -  at 745.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:22] {2218} INFO - iteration 515, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:22] {2391} INFO -  at 746.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:22] {2218} INFO - iteration 516, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:23] {2391} INFO -  at 747.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:23] {2218} INFO - iteration 517, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:24] {2391} INFO -  at 747.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:24] {2218} INFO - iteration 518, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:47:26] {2391} INFO -  at 750.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:26] {2218} INFO - iteration 519, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:27] {2391} INFO -  at 751.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:27] {2218} INFO - iteration 520, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:27] {2391} INFO -  at 751.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:27] {2218} INFO - iteration 521, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:28] {2391} INFO -  at 752.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:28] {2218} INFO - iteration 522, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:29] {2391} INFO -  at 752.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:29] {2218} INFO - iteration 523, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:29] {2391} INFO -  at 753.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:29] {2218} INFO - iteration 524, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:30] {2391} INFO -  at 754.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:30] {2218} INFO - iteration 525, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:47:30] {2391} INFO -  at 754.4s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:30] {2218} INFO - iteration 526, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:31] {2391} INFO -  at 755.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:31] {2218} INFO - iteration 527, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:32] {2391} INFO -  at 755.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:32] {2218} INFO - iteration 528, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:32] {2391} INFO -  at 756.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:32] {2218} INFO - iteration 529, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:33] {2391} INFO -  at 757.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:33] {2218} INFO - iteration 530, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:34] {2391} INFO -  at 757.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:34] {2218} INFO - iteration 531, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:47:38] {2391} INFO -  at 762.4s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:38] {2218} INFO - iteration 532, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:39] {2391} INFO -  at 763.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:39] {2218} INFO - iteration 533, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:47:44] {2391} INFO -  at 768.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:44] {2218} INFO - iteration 534, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:45] {2391} INFO -  at 769.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:45] {2218} INFO - iteration 535, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:45] {2391} INFO -  at 769.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:45] {2218} INFO - iteration 536, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:46] {2391} INFO -  at 770.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:46] {2218} INFO - iteration 537, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:47] {2391} INFO -  at 770.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:47] {2218} INFO - iteration 538, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:47] {2391} INFO -  at 771.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:47] {2218} INFO - iteration 539, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:48] {2391} INFO -  at 772.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:48] {2218} INFO - iteration 540, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:49] {2391} INFO -  at 773.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:49] {2218} INFO - iteration 541, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:49] {2391} INFO -  at 773.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:49] {2218} INFO - iteration 542, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:50] {2391} INFO -  at 774.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:50] {2218} INFO - iteration 543, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:51] {2391} INFO -  at 775.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:51] {2218} INFO - iteration 544, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:47:57] {2391} INFO -  at 781.1s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:57] {2218} INFO - iteration 545, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:58] {2391} INFO -  at 781.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:58] {2218} INFO - iteration 546, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:58] {2391} INFO -  at 782.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:58] {2218} INFO - iteration 547, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:59] {2391} INFO -  at 783.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:59] {2218} INFO - iteration 548, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:47:59] {2391} INFO -  at 783.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:47:59] {2218} INFO - iteration 549, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:00] {2391} INFO -  at 784.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:00] {2218} INFO - iteration 550, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:01] {2391} INFO -  at 785.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:01] {2218} INFO - iteration 551, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:48:03] {2391} INFO -  at 786.8s,\testimator lgbm's best error=69.0728,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:03] {2218} INFO - iteration 552, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:03] {2391} INFO -  at 787.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:03] {2218} INFO - iteration 553, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:04] {2391} INFO -  at 788.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:04] {2218} INFO - iteration 554, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:05] {2391} INFO -  at 788.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:05] {2218} INFO - iteration 555, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:05] {2391} INFO -  at 789.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:05] {2218} INFO - iteration 556, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:06] {2391} INFO -  at 790.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:06] {2218} INFO - iteration 557, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:07] {2391} INFO -  at 790.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:07] {2218} INFO - iteration 558, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:07] {2391} INFO -  at 791.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:07] {2218} INFO - iteration 559, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:08] {2391} INFO -  at 792.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:08] {2218} INFO - iteration 560, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:08] {2391} INFO -  at 792.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:08] {2218} INFO - iteration 561, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:09] {2391} INFO -  at 793.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:09] {2218} INFO - iteration 562, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:10] {2391} INFO -  at 794.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:10] {2218} INFO - iteration 563, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:10] {2391} INFO -  at 794.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:10] {2218} INFO - iteration 564, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:11] {2391} INFO -  at 795.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:11] {2218} INFO - iteration 565, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:12] {2391} INFO -  at 796.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:12] {2218} INFO - iteration 566, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:48:17] {2391} INFO -  at 800.8s,\testimator lgbm's best error=68.8596,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:17] {2218} INFO - iteration 567, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:17] {2391} INFO -  at 801.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:17] {2218} INFO - iteration 568, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:18] {2391} INFO -  at 802.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:18] {2218} INFO - iteration 569, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:48:25] {2391} INFO -  at 809.6s,\testimator lgbm's best error=68.8596,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:25] {2218} INFO - iteration 570, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:26] {2391} INFO -  at 810.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:26] {2218} INFO - iteration 571, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:27] {2391} INFO -  at 810.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:27] {2218} INFO - iteration 572, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:27] {2391} INFO -  at 811.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:27] {2218} INFO - iteration 573, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:28] {2391} INFO -  at 812.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:28] {2218} INFO - iteration 574, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:29] {2391} INFO -  at 812.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:29] {2218} INFO - iteration 575, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:48:32] {2391} INFO -  at 816.1s,\testimator lgbm's best error=68.8596,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:32] {2218} INFO - iteration 576, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:32] {2391} INFO -  at 816.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:32] {2218} INFO - iteration 577, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:33] {2391} INFO -  at 817.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:33] {2218} INFO - iteration 578, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:34] {2391} INFO -  at 818.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:34] {2218} INFO - iteration 579, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:34] {2391} INFO -  at 818.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:34] {2218} INFO - iteration 580, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:35] {2391} INFO -  at 819.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:35] {2218} INFO - iteration 581, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:36] {2391} INFO -  at 820.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:36] {2218} INFO - iteration 582, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:37] {2391} INFO -  at 820.8s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:37] {2218} INFO - iteration 583, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:37] {2391} INFO -  at 821.5s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:37] {2218} INFO - iteration 584, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:38] {2391} INFO -  at 822.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:38] {2218} INFO - iteration 585, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:38] {2391} INFO -  at 822.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:38] {2218} INFO - iteration 586, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:39] {2391} INFO -  at 823.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:39] {2218} INFO - iteration 587, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:48:45] {2391} INFO -  at 829.1s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:45] {2218} INFO - iteration 588, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:45] {2391} INFO -  at 829.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:45] {2218} INFO - iteration 589, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:46] {2391} INFO -  at 830.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:46] {2218} INFO - iteration 590, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:47] {2391} INFO -  at 831.0s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:47] {2218} INFO - iteration 591, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:47] {2391} INFO -  at 831.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:47] {2218} INFO - iteration 592, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:48] {2391} INFO -  at 832.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:48] {2218} INFO - iteration 593, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:48:49] {2391} INFO -  at 833.1s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:48:49] {2218} INFO - iteration 594, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:49:25] {2391} INFO -  at 869.6s,\testimator xgboost's best error=70.0773,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:25] {2218} INFO - iteration 595, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:26] {2391} INFO -  at 870.3s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:26] {2218} INFO - iteration 596, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:27] {2391} INFO -  at 870.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:27] {2218} INFO - iteration 597, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:27] {2391} INFO -  at 871.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:27] {2218} INFO - iteration 598, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:28] {2391} INFO -  at 872.2s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:28] {2218} INFO - iteration 599, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:29] {2391} INFO -  at 872.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:29] {2218} INFO - iteration 600, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:49:33] {2391} INFO -  at 876.9s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:33] {2218} INFO - iteration 601, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:49:36] {2391} INFO -  at 880.3s,\testimator catboost's best error=69.7429,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:36] {2218} INFO - iteration 602, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:49:39] {2391} INFO -  at 882.9s,\testimator lgbm's best error=68.8596,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:39] {2218} INFO - iteration 603, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:39] {2391} INFO -  at 883.6s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:39] {2218} INFO - iteration 604, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:49:53] {2391} INFO -  at 897.1s,\testimator lgbm's best error=68.8596,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:53] {2218} INFO - iteration 605, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:53] {2391} INFO -  at 897.7s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:53] {2218} INFO - iteration 606, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:54] {2391} INFO -  at 898.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:54] {2218} INFO - iteration 607, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2391} INFO -  at 898.9s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2218} INFO - iteration 608, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2391} INFO -  at 899.4s,\testimator extra_tree's best error=68.6070,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2218} INFO - iteration 609, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2391} INFO -  at 899.6s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:55] {2218} INFO - iteration 610, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:49:56] {2391} INFO -  at 899.9s,\testimator xgb_limitdepth's best error=71.2641,\tbest estimator extra_tree's best error=68.6070\n",
      "[flaml.automl.logger: 11-12 20:49:56] {2493} INFO - selected model: ExtraTreesRegressor(max_features=0.9179006169231322, max_leaf_nodes=78,\n",
      "                    n_estimators=20, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 11-12 20:49:56] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 20:49:56] {1931} INFO - Time taken to find the best model: 718.5333511829376\n",
      "[flaml.automl.logger: 11-12 20:49:56] {1941} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
      "[flaml.automl.logger: 11-12 20:50:06] {1679} INFO - task = regression\n",
      "[flaml.automl.logger: 11-12 20:50:06] {1687} INFO - Data split method: uniform\n",
      "[flaml.automl.logger: 11-12 20:50:06] {1690} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-12 20:50:06] {1788} INFO - Minimizing error metric: mae\n",
      "[flaml.automl.logger: 11-12 20:50:06] {1900} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2344} INFO - Estimated sufficient time budget=1206s. Estimated necessary time budget=10s.\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 0.8s,\testimator lgbm's best error=151.2353,\tbest estimator lgbm's best error=151.2353\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 0.9s,\testimator lgbm's best error=135.2081,\tbest estimator lgbm's best error=135.2081\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 1.0s,\testimator lgbm's best error=135.2081,\tbest estimator lgbm's best error=135.2081\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 1.1s,\testimator lgbm's best error=133.7742,\tbest estimator lgbm's best error=133.7742\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 1.2s,\testimator lgbm's best error=94.6564,\tbest estimator lgbm's best error=94.6564\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2391} INFO -  at 1.4s,\testimator lgbm's best error=66.7633,\tbest estimator lgbm's best error=66.7633\n",
      "[flaml.automl.logger: 11-12 20:50:06] {2218} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 1.5s,\testimator lgbm's best error=66.7633,\tbest estimator lgbm's best error=66.7633\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 1.6s,\testimator lgbm's best error=66.7633,\tbest estimator lgbm's best error=66.7633\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 1.7s,\testimator xgboost's best error=151.1638,\tbest estimator lgbm's best error=66.7633\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 1.8s,\testimator xgboost's best error=135.0624,\tbest estimator lgbm's best error=66.7633\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 1.9s,\testimator lgbm's best error=59.2053,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 2.1s,\testimator extra_tree's best error=77.6340,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2391} INFO -  at 2.3s,\testimator extra_tree's best error=77.6340,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:07] {2218} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2391} INFO -  at 2.4s,\testimator xgboost's best error=135.0624,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2218} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2391} INFO -  at 2.6s,\testimator extra_tree's best error=66.8520,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2218} INFO - iteration 15, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2391} INFO -  at 3.2s,\testimator rf's best error=80.4577,\tbest estimator lgbm's best error=59.2053\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2218} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2391} INFO -  at 3.4s,\testimator lgbm's best error=58.7955,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:08] {2218} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 3.6s,\testimator extra_tree's best error=60.0754,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 3.7s,\testimator lgbm's best error=58.7955,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 4.0s,\testimator extra_tree's best error=60.0754,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 4.1s,\testimator lgbm's best error=58.7955,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 4.2s,\testimator xgboost's best error=135.0624,\tbest estimator lgbm's best error=58.7955\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2391} INFO -  at 4.4s,\testimator lgbm's best error=58.6660,\tbest estimator lgbm's best error=58.6660\n",
      "[flaml.automl.logger: 11-12 20:50:09] {2218} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2391} INFO -  at 4.6s,\testimator lgbm's best error=58.6660,\tbest estimator lgbm's best error=58.6660\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2218} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2391} INFO -  at 4.8s,\testimator extra_tree's best error=60.0754,\tbest estimator lgbm's best error=58.6660\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2218} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2391} INFO -  at 5.2s,\testimator extra_tree's best error=58.4890,\tbest estimator extra_tree's best error=58.4890\n",
      "[flaml.automl.logger: 11-12 20:50:10] {2218} INFO - iteration 26, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:11] {2391} INFO -  at 6.2s,\testimator rf's best error=80.4577,\tbest estimator extra_tree's best error=58.4890\n",
      "[flaml.automl.logger: 11-12 20:50:11] {2218} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2391} INFO -  at 6.5s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2218} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2391} INFO -  at 6.8s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2218} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2391} INFO -  at 7.0s,\testimator extra_tree's best error=58.0422,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:12] {2218} INFO - iteration 30, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:13] {2391} INFO -  at 7.7s,\testimator rf's best error=67.6800,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:13] {2218} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:13] {2391} INFO -  at 7.9s,\testimator lgbm's best error=58.6660,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:13] {2218} INFO - iteration 32, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:14] {2391} INFO -  at 8.7s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=58.0422\n",
      "[flaml.automl.logger: 11-12 20:50:14] {2218} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:14] {2391} INFO -  at 9.2s,\testimator extra_tree's best error=57.3735,\tbest estimator extra_tree's best error=57.3735\n",
      "[flaml.automl.logger: 11-12 20:50:14] {2218} INFO - iteration 34, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:15] {2391} INFO -  at 10.3s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=57.3735\n",
      "[flaml.automl.logger: 11-12 20:50:15] {2218} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:16] {2391} INFO -  at 10.6s,\testimator xgboost's best error=66.7444,\tbest estimator extra_tree's best error=57.3735\n",
      "[flaml.automl.logger: 11-12 20:50:16] {2218} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:16] {2391} INFO -  at 11.2s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:16] {2218} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2391} INFO -  at 11.8s,\testimator xgboost's best error=66.7444,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2218} INFO - iteration 38, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2391} INFO -  at 12.2s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2218} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2391} INFO -  at 12.4s,\testimator xgboost's best error=62.8048,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:17] {2218} INFO - iteration 40, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:18] {2391} INFO -  at 13.0s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:18] {2218} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2391} INFO -  at 13.5s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2218} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2391} INFO -  at 13.7s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2218} INFO - iteration 43, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2391} INFO -  at 14.1s,\testimator extra_tree's best error=56.9628,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:19] {2218} INFO - iteration 44, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2391} INFO -  at 15.6s,\testimator catboost's best error=58.7710,\tbest estimator extra_tree's best error=56.9628\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2218} INFO - iteration 45, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2391} INFO -  at 16.2s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2218} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2391} INFO -  at 16.3s,\testimator xgboost's best error=62.8048,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:21] {2218} INFO - iteration 47, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:22] {2391} INFO -  at 17.1s,\testimator rf's best error=60.6179,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:22] {2218} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:23] {2391} INFO -  at 17.5s,\testimator xgboost's best error=62.8048,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:23] {2218} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:23] {2391} INFO -  at 17.7s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:23] {2218} INFO - iteration 50, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:26] {2391} INFO -  at 21.4s,\testimator catboost's best error=58.1780,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:26] {2218} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:27] {2391} INFO -  at 21.8s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:27] {2218} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:29] {2391} INFO -  at 23.5s,\testimator rf's best error=58.5857,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:29] {2218} INFO - iteration 53, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2391} INFO -  at 24.8s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2218} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2391} INFO -  at 24.9s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2218} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2391} INFO -  at 25.3s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:30] {2218} INFO - iteration 56, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:31] {2391} INFO -  at 26.0s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:31] {2218} INFO - iteration 57, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:50:33] {2391} INFO -  at 27.7s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:33] {2218} INFO - iteration 58, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:36] {2391} INFO -  at 30.8s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:36] {2218} INFO - iteration 59, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:36] {2391} INFO -  at 31.0s,\testimator xgboost's best error=62.8048,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:36] {2218} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:37] {2391} INFO -  at 31.7s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:37] {2218} INFO - iteration 61, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:37] {2391} INFO -  at 32.1s,\testimator xgboost's best error=59.6248,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:37] {2218} INFO - iteration 62, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:41] {2391} INFO -  at 35.8s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:41] {2218} INFO - iteration 63, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:41] {2391} INFO -  at 36.3s,\testimator xgboost's best error=59.6248,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:41] {2218} INFO - iteration 64, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2391} INFO -  at 36.7s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2218} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2391} INFO -  at 36.9s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2218} INFO - iteration 66, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2391} INFO -  at 37.3s,\testimator xgboost's best error=59.6248,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:42] {2218} INFO - iteration 67, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:43] {2391} INFO -  at 38.2s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:43] {2218} INFO - iteration 68, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:45] {2391} INFO -  at 40.3s,\testimator xgboost's best error=58.6264,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:45] {2218} INFO - iteration 69, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:46] {2391} INFO -  at 40.6s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:46] {2218} INFO - iteration 70, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:46] {2391} INFO -  at 41.1s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:46] {2218} INFO - iteration 71, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:47] {2391} INFO -  at 41.9s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:47] {2218} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:50:47] {2391} INFO -  at 42.4s,\testimator xgboost's best error=58.6264,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:47] {2218} INFO - iteration 73, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:48] {2391} INFO -  at 42.7s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:48] {2218} INFO - iteration 74, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2391} INFO -  at 43.8s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2218} INFO - iteration 75, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2391} INFO -  at 44.2s,\testimator extra_tree's best error=56.8253,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2218} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2391} INFO -  at 44.4s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:49] {2218} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:50] {2391} INFO -  at 44.7s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.8253\n",
      "[flaml.automl.logger: 11-12 20:50:50] {2218} INFO - iteration 78, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:50] {2391} INFO -  at 45.4s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:50] {2218} INFO - iteration 79, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2391} INFO -  at 47.6s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2218} INFO - iteration 80, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2391} INFO -  at 47.8s,\testimator lgbm's best error=58.1043,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2218} INFO - iteration 81, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2391} INFO -  at 48.3s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:53] {2218} INFO - iteration 82, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:50:56] {2391} INFO -  at 51.4s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:56] {2218} INFO - iteration 83, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:58] {2391} INFO -  at 52.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:58] {2218} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:59] {2391} INFO -  at 54.0s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:59] {2218} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:50:59] {2391} INFO -  at 54.4s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:50:59] {2218} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:00] {2391} INFO -  at 55.1s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:00] {2218} INFO - iteration 87, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:01] {2391} INFO -  at 55.9s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:01] {2218} INFO - iteration 88, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:02] {2391} INFO -  at 57.4s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:02] {2218} INFO - iteration 89, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:03] {2391} INFO -  at 58.0s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:03] {2218} INFO - iteration 90, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:04] {2391} INFO -  at 58.8s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:04] {2218} INFO - iteration 91, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:51:05] {2391} INFO -  at 59.9s,\testimator xgboost's best error=58.6264,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:05] {2218} INFO - iteration 92, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:06] {2391} INFO -  at 60.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:06] {2218} INFO - iteration 93, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:06] {2391} INFO -  at 61.2s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:06] {2218} INFO - iteration 94, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:07] {2391} INFO -  at 62.0s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:07] {2218} INFO - iteration 95, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:07] {2391} INFO -  at 62.2s,\testimator xgb_limitdepth's best error=60.6563,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:07] {2218} INFO - iteration 96, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2391} INFO -  at 62.5s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2218} INFO - iteration 97, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2391} INFO -  at 62.7s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2218} INFO - iteration 98, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2391} INFO -  at 62.8s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:08] {2218} INFO - iteration 99, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:10] {2391} INFO -  at 64.4s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:10] {2218} INFO - iteration 100, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:11] {2391} INFO -  at 65.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:11] {2218} INFO - iteration 101, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:13] {2391} INFO -  at 67.6s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:13] {2218} INFO - iteration 102, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:13] {2391} INFO -  at 68.2s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:13] {2218} INFO - iteration 103, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2391} INFO -  at 68.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2218} INFO - iteration 104, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2391} INFO -  at 69.0s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2218} INFO - iteration 105, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2391} INFO -  at 69.1s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:14] {2218} INFO - iteration 106, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:51:18] {2391} INFO -  at 72.8s,\testimator xgboost's best error=58.5408,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:18] {2218} INFO - iteration 107, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:18] {2391} INFO -  at 73.0s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:18] {2218} INFO - iteration 108, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2391} INFO -  at 73.7s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2218} INFO - iteration 109, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2391} INFO -  at 74.3s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2218} INFO - iteration 110, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2391} INFO -  at 74.4s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:19] {2218} INFO - iteration 111, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:20] {2391} INFO -  at 75.3s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:20] {2218} INFO - iteration 112, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:22] {2391} INFO -  at 77.3s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:22] {2218} INFO - iteration 113, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:23] {2391} INFO -  at 77.6s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:23] {2218} INFO - iteration 114, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:23] {2391} INFO -  at 77.9s,\testimator xgb_limitdepth's best error=58.6274,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:23] {2218} INFO - iteration 115, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:24] {2391} INFO -  at 78.8s,\testimator xgb_limitdepth's best error=58.2184,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:24] {2218} INFO - iteration 116, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:27] {2391} INFO -  at 81.5s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:27] {2218} INFO - iteration 117, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:28] {2391} INFO -  at 83.2s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:28] {2218} INFO - iteration 118, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:29] {2391} INFO -  at 83.5s,\testimator xgb_limitdepth's best error=58.2184,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:29] {2218} INFO - iteration 119, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:30] {2391} INFO -  at 84.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:30] {2218} INFO - iteration 120, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:31] {2391} INFO -  at 86.4s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:31] {2218} INFO - iteration 121, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:32] {2391} INFO -  at 86.9s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:32] {2218} INFO - iteration 122, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:33] {2391} INFO -  at 88.0s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:33] {2218} INFO - iteration 123, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:34] {2391} INFO -  at 88.6s,\testimator xgb_limitdepth's best error=58.2184,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:34] {2218} INFO - iteration 124, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:34] {2391} INFO -  at 89.2s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:34] {2218} INFO - iteration 125, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:35] {2391} INFO -  at 89.9s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:35] {2218} INFO - iteration 126, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:36] {2391} INFO -  at 91.3s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:36] {2218} INFO - iteration 127, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:38] {2391} INFO -  at 92.4s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:38] {2218} INFO - iteration 128, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:51:39] {2391} INFO -  at 94.0s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:39] {2218} INFO - iteration 129, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:43] {2391} INFO -  at 97.5s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:43] {2218} INFO - iteration 130, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:44] {2391} INFO -  at 98.7s,\testimator xgb_limitdepth's best error=58.2184,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:44] {2218} INFO - iteration 131, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:45] {2391} INFO -  at 99.8s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:45] {2218} INFO - iteration 132, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:46] {2391} INFO -  at 100.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:46] {2218} INFO - iteration 133, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:46] {2391} INFO -  at 101.2s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:46] {2218} INFO - iteration 134, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:48] {2391} INFO -  at 103.0s,\testimator catboost's best error=57.3623,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:48] {2218} INFO - iteration 135, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:51:51] {2391} INFO -  at 105.8s,\testimator catboost's best error=57.2765,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:51] {2218} INFO - iteration 136, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:52] {2391} INFO -  at 106.9s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:52] {2218} INFO - iteration 137, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2391} INFO -  at 111.6s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2218} INFO - iteration 138, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2391} INFO -  at 112.2s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2218} INFO - iteration 139, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2391} INFO -  at 112.4s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:57] {2218} INFO - iteration 140, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:58] {2391} INFO -  at 113.4s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:58] {2218} INFO - iteration 141, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:51:59] {2391} INFO -  at 113.9s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:51:59] {2218} INFO - iteration 142, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:00] {2391} INFO -  at 114.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:00] {2218} INFO - iteration 143, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:01] {2391} INFO -  at 115.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:01] {2218} INFO - iteration 144, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:52:01] {2391} INFO -  at 116.3s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:01] {2218} INFO - iteration 145, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:03] {2391} INFO -  at 117.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:03] {2218} INFO - iteration 146, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:03] {2391} INFO -  at 118.1s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:03] {2218} INFO - iteration 147, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:04] {2391} INFO -  at 119.2s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:04] {2218} INFO - iteration 148, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:05] {2391} INFO -  at 120.4s,\testimator rf's best error=58.2122,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:05] {2218} INFO - iteration 149, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:07] {2391} INFO -  at 121.8s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:07] {2218} INFO - iteration 150, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:08] {2391} INFO -  at 123.0s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:08] {2218} INFO - iteration 151, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:10] {2391} INFO -  at 124.7s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:10] {2218} INFO - iteration 152, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:11] {2391} INFO -  at 126.1s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:11] {2218} INFO - iteration 153, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:12] {2391} INFO -  at 126.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:12] {2218} INFO - iteration 154, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:13] {2391} INFO -  at 127.5s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:13] {2218} INFO - iteration 155, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:52:13] {2391} INFO -  at 127.8s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:13] {2218} INFO - iteration 156, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:15] {2391} INFO -  at 129.5s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:15] {2218} INFO - iteration 157, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:52:17] {2391} INFO -  at 132.1s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:17] {2218} INFO - iteration 158, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:18] {2391} INFO -  at 132.7s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:18] {2218} INFO - iteration 159, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:19] {2391} INFO -  at 133.7s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:19] {2218} INFO - iteration 160, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:21] {2391} INFO -  at 135.5s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:21] {2218} INFO - iteration 161, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:21] {2391} INFO -  at 136.3s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:21] {2218} INFO - iteration 162, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:22] {2391} INFO -  at 137.3s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:22] {2218} INFO - iteration 163, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:23] {2391} INFO -  at 138.0s,\testimator extra_tree's best error=56.7773,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:23] {2218} INFO - iteration 164, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:52:30] {2391} INFO -  at 144.7s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.7773\n",
      "[flaml.automl.logger: 11-12 20:52:30] {2218} INFO - iteration 165, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:31] {2391} INFO -  at 145.5s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:31] {2218} INFO - iteration 166, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:32] {2391} INFO -  at 147.3s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:32] {2218} INFO - iteration 167, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:33] {2391} INFO -  at 148.0s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:33] {2218} INFO - iteration 168, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:34] {2391} INFO -  at 148.9s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:34] {2218} INFO - iteration 169, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:35] {2391} INFO -  at 149.7s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:35] {2218} INFO - iteration 170, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:36] {2391} INFO -  at 150.8s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:36] {2218} INFO - iteration 171, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:52:36] {2391} INFO -  at 151.0s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:36] {2218} INFO - iteration 172, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:38] {2391} INFO -  at 152.6s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:38] {2218} INFO - iteration 173, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:38] {2391} INFO -  at 153.4s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:38] {2218} INFO - iteration 174, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:39] {2391} INFO -  at 154.0s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:39] {2218} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:40] {2391} INFO -  at 154.9s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:40] {2218} INFO - iteration 176, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:52:42] {2391} INFO -  at 156.8s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:42] {2218} INFO - iteration 177, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:52:44] {2391} INFO -  at 158.7s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:44] {2218} INFO - iteration 178, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:52:45] {2391} INFO -  at 159.6s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:52:45] {2218} INFO - iteration 179, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:53:23] {2391} INFO -  at 198.3s,\testimator xgboost's best error=57.4171,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:53:23] {2218} INFO - iteration 180, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:24] {2391} INFO -  at 199.0s,\testimator extra_tree's best error=56.7102,\tbest estimator extra_tree's best error=56.7102\n",
      "[flaml.automl.logger: 11-12 20:53:24] {2218} INFO - iteration 181, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:25] {2391} INFO -  at 199.6s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:25] {2218} INFO - iteration 182, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:25] {2391} INFO -  at 200.1s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:25] {2218} INFO - iteration 183, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:26] {2391} INFO -  at 201.0s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:26] {2218} INFO - iteration 184, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:53:27] {2391} INFO -  at 202.1s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:27] {2218} INFO - iteration 185, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:28] {2391} INFO -  at 202.8s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:28] {2218} INFO - iteration 186, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:29] {2391} INFO -  at 203.5s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:29] {2218} INFO - iteration 187, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:53:32] {2391} INFO -  at 207.2s,\testimator xgboost's best error=57.4171,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:32] {2218} INFO - iteration 188, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:33] {2391} INFO -  at 207.7s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:33] {2218} INFO - iteration 189, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:53:34] {2391} INFO -  at 209.2s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:34] {2218} INFO - iteration 190, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:35] {2391} INFO -  at 209.9s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:35] {2218} INFO - iteration 191, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:36] {2391} INFO -  at 210.5s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:36] {2218} INFO - iteration 192, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:53:45] {2391} INFO -  at 220.2s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:45] {2218} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:46] {2391} INFO -  at 221.1s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:46] {2218} INFO - iteration 194, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:47] {2391} INFO -  at 221.8s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:47] {2218} INFO - iteration 195, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:53:48] {2391} INFO -  at 222.4s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:53:48] {2218} INFO - iteration 196, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:54:26] {2391} INFO -  at 260.9s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:54:26] {2218} INFO - iteration 197, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:54:27] {2391} INFO -  at 262.0s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:54:27] {2218} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:28] {2391} INFO -  at 262.7s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:54:28] {2218} INFO - iteration 199, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:28] {2391} INFO -  at 263.3s,\testimator extra_tree's best error=56.6690,\tbest estimator extra_tree's best error=56.6690\n",
      "[flaml.automl.logger: 11-12 20:54:28] {2218} INFO - iteration 200, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:29] {2391} INFO -  at 264.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:29] {2218} INFO - iteration 201, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:30] {2391} INFO -  at 264.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:30] {2218} INFO - iteration 202, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:54:31] {2391} INFO -  at 266.3s,\testimator rf's best error=57.3571,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:31] {2218} INFO - iteration 203, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:54:37] {2391} INFO -  at 272.1s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:37] {2218} INFO - iteration 204, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:38] {2391} INFO -  at 273.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:38] {2218} INFO - iteration 205, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:39] {2391} INFO -  at 273.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:39] {2218} INFO - iteration 206, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:39] {2391} INFO -  at 274.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:39] {2218} INFO - iteration 207, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:40] {2391} INFO -  at 275.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:40] {2218} INFO - iteration 208, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:41] {2391} INFO -  at 276.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:41] {2218} INFO - iteration 209, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:42] {2391} INFO -  at 276.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:42] {2218} INFO - iteration 210, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:42] {2391} INFO -  at 277.4s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:42] {2218} INFO - iteration 211, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:43] {2391} INFO -  at 278.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:43] {2218} INFO - iteration 212, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:44] {2391} INFO -  at 278.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:44] {2218} INFO - iteration 213, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:45] {2391} INFO -  at 279.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:45] {2218} INFO - iteration 214, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:45] {2391} INFO -  at 280.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:45] {2218} INFO - iteration 215, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:46] {2391} INFO -  at 281.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:46] {2218} INFO - iteration 216, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:47] {2391} INFO -  at 281.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:47] {2218} INFO - iteration 217, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:54:48] {2391} INFO -  at 282.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:54:48] {2218} INFO - iteration 218, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:55:05] {2391} INFO -  at 299.7s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:05] {2218} INFO - iteration 219, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:55:08] {2391} INFO -  at 302.8s,\testimator catboost's best error=57.2765,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:08] {2218} INFO - iteration 220, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:55:10] {2391} INFO -  at 305.1s,\testimator catboost's best error=57.2765,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:10] {2218} INFO - iteration 221, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:11] {2391} INFO -  at 305.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:11] {2218} INFO - iteration 222, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:55:12] {2391} INFO -  at 306.9s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:12] {2218} INFO - iteration 223, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:13] {2391} INFO -  at 307.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:13] {2218} INFO - iteration 224, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:13] {2391} INFO -  at 308.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:13] {2218} INFO - iteration 225, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:55:15] {2391} INFO -  at 309.6s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:15] {2218} INFO - iteration 226, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:55:21] {2391} INFO -  at 316.2s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:21] {2218} INFO - iteration 227, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:22] {2391} INFO -  at 316.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:22] {2218} INFO - iteration 228, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:55:23] {2391} INFO -  at 317.9s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:23] {2218} INFO - iteration 229, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:24] {2391} INFO -  at 318.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:24] {2218} INFO - iteration 230, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:24] {2391} INFO -  at 319.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:24] {2218} INFO - iteration 231, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:25] {2391} INFO -  at 320.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:25] {2218} INFO - iteration 232, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:26] {2391} INFO -  at 320.8s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:26] {2218} INFO - iteration 233, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:27] {2391} INFO -  at 321.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:27] {2218} INFO - iteration 234, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:27] {2391} INFO -  at 322.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:27] {2218} INFO - iteration 235, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:28] {2391} INFO -  at 322.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:28] {2218} INFO - iteration 236, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:29] {2391} INFO -  at 323.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:29] {2218} INFO - iteration 237, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:29] {2391} INFO -  at 324.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:29] {2218} INFO - iteration 238, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:30] {2391} INFO -  at 325.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:30] {2218} INFO - iteration 239, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:55:40] {2391} INFO -  at 334.5s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:40] {2218} INFO - iteration 240, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:40] {2391} INFO -  at 335.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:40] {2218} INFO - iteration 241, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:41] {2391} INFO -  at 336.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:41] {2218} INFO - iteration 242, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:55:41] {2391} INFO -  at 336.1s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:41] {2218} INFO - iteration 243, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:42] {2391} INFO -  at 336.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:42] {2218} INFO - iteration 244, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:43] {2391} INFO -  at 337.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:43] {2218} INFO - iteration 245, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:55:43] {2391} INFO -  at 338.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:55:43] {2218} INFO - iteration 246, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:56:08] {2391} INFO -  at 363.4s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:08] {2218} INFO - iteration 247, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:09] {2391} INFO -  at 364.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:09] {2218} INFO - iteration 248, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:10] {2391} INFO -  at 364.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:10] {2218} INFO - iteration 249, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:56:11] {2391} INFO -  at 366.4s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:11] {2218} INFO - iteration 250, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:12] {2391} INFO -  at 367.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:12] {2218} INFO - iteration 251, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:13] {2391} INFO -  at 367.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:13] {2218} INFO - iteration 252, current learner rf\n",
      "[flaml.automl.logger: 11-12 20:56:14] {2391} INFO -  at 368.7s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:14] {2218} INFO - iteration 253, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:14] {2391} INFO -  at 369.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:14] {2218} INFO - iteration 254, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:15] {2391} INFO -  at 370.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:15] {2218} INFO - iteration 255, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:16] {2391} INFO -  at 370.8s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:16] {2218} INFO - iteration 256, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 20:56:16] {2391} INFO -  at 371.2s,\testimator lgbm's best error=58.0886,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:16] {2218} INFO - iteration 257, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:56:26] {2391} INFO -  at 381.4s,\testimator xgb_limitdepth's best error=57.9550,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:26] {2218} INFO - iteration 258, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:56:29] {2391} INFO -  at 383.8s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:29] {2218} INFO - iteration 259, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:56:31] {2391} INFO -  at 386.3s,\testimator xgb_limitdepth's best error=57.3860,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:31] {2218} INFO - iteration 260, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:32] {2391} INFO -  at 387.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:32] {2218} INFO - iteration 261, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:56:37] {2391} INFO -  at 391.8s,\testimator xgb_limitdepth's best error=57.3860,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:37] {2218} INFO - iteration 262, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:38] {2391} INFO -  at 392.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:38] {2218} INFO - iteration 263, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:38] {2391} INFO -  at 393.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:38] {2218} INFO - iteration 264, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:56:47] {2391} INFO -  at 402.0s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:47] {2218} INFO - iteration 265, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:48] {2391} INFO -  at 402.8s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:48] {2218} INFO - iteration 266, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:56:48] {2391} INFO -  at 403.4s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:56:48] {2218} INFO - iteration 267, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:57:00] {2391} INFO -  at 414.8s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:00] {2218} INFO - iteration 268, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:57:02] {2391} INFO -  at 417.3s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:02] {2218} INFO - iteration 269, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:57:13] {2391} INFO -  at 428.3s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:13] {2218} INFO - iteration 270, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:57:24] {2391} INFO -  at 439.2s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:24] {2218} INFO - iteration 271, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:57:25] {2391} INFO -  at 439.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:25] {2218} INFO - iteration 272, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:57:47] {2391} INFO -  at 461.7s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:47] {2218} INFO - iteration 273, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:57:47] {2391} INFO -  at 462.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:47] {2218} INFO - iteration 274, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:57:48] {2391} INFO -  at 463.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:48] {2218} INFO - iteration 275, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:57:49] {2391} INFO -  at 463.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:49] {2218} INFO - iteration 276, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:57:50] {2391} INFO -  at 464.6s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:57:50] {2218} INFO - iteration 277, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 20:58:08] {2391} INFO -  at 482.5s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:08] {2218} INFO - iteration 278, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:58:09] {2391} INFO -  at 483.8s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:09] {2218} INFO - iteration 279, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:58:09] {2391} INFO -  at 484.4s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:09] {2218} INFO - iteration 280, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:58:16] {2391} INFO -  at 490.7s,\testimator xgb_limitdepth's best error=56.6869,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:16] {2218} INFO - iteration 281, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:58:17] {2391} INFO -  at 491.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:17] {2218} INFO - iteration 282, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:58:23] {2391} INFO -  at 498.3s,\testimator catboost's best error=57.2765,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:23] {2218} INFO - iteration 283, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:58:24] {2391} INFO -  at 499.0s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:24] {2218} INFO - iteration 284, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:58:25] {2391} INFO -  at 499.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:25] {2218} INFO - iteration 285, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:58:41] {2391} INFO -  at 515.6s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:41] {2218} INFO - iteration 286, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:58:57] {2391} INFO -  at 531.9s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:58:57] {2218} INFO - iteration 287, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 20:59:49] {2391} INFO -  at 584.3s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:59:49] {2218} INFO - iteration 288, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 20:59:50] {2391} INFO -  at 585.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:59:50] {2218} INFO - iteration 289, current learner catboost\n",
      "[flaml.automl.logger: 11-12 20:59:57] {2391} INFO -  at 591.6s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 20:59:57] {2218} INFO - iteration 290, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:00:00] {2391} INFO -  at 594.9s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:00] {2218} INFO - iteration 291, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:01] {2391} INFO -  at 595.7s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:01] {2218} INFO - iteration 292, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:00:49] {2391} INFO -  at 644.4s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:49] {2218} INFO - iteration 293, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:51] {2391} INFO -  at 645.5s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:51] {2218} INFO - iteration 294, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:51] {2391} INFO -  at 646.2s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:51] {2218} INFO - iteration 295, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:52] {2391} INFO -  at 647.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:52] {2218} INFO - iteration 296, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:00:56] {2391} INFO -  at 651.2s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:56] {2218} INFO - iteration 297, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:57] {2391} INFO -  at 652.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:57] {2218} INFO - iteration 298, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:00:58] {2391} INFO -  at 652.9s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:00:58] {2218} INFO - iteration 299, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:01:13] {2391} INFO -  at 668.1s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:13] {2218} INFO - iteration 300, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:01:17] {2391} INFO -  at 672.1s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:17] {2218} INFO - iteration 301, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:18] {2391} INFO -  at 673.1s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:18] {2218} INFO - iteration 302, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:01:22] {2391} INFO -  at 676.9s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:22] {2218} INFO - iteration 303, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:23] {2391} INFO -  at 678.3s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:23] {2218} INFO - iteration 304, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 21:01:28] {2391} INFO -  at 682.7s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:28] {2218} INFO - iteration 305, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:29] {2391} INFO -  at 683.7s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:29] {2218} INFO - iteration 306, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:30] {2391} INFO -  at 684.4s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:30] {2218} INFO - iteration 307, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:31] {2391} INFO -  at 685.4s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:31] {2218} INFO - iteration 308, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:31] {2391} INFO -  at 686.3s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:31] {2218} INFO - iteration 309, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:32] {2391} INFO -  at 687.1s,\testimator extra_tree's best error=56.5437,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:32] {2218} INFO - iteration 310, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:34] {2391} INFO -  at 688.6s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:34] {2218} INFO - iteration 311, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:01:39] {2391} INFO -  at 693.9s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:39] {2218} INFO - iteration 312, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:01:42] {2391} INFO -  at 697.1s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:42] {2218} INFO - iteration 313, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:43] {2391} INFO -  at 698.1s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.5437\n",
      "[flaml.automl.logger: 11-12 21:01:43] {2218} INFO - iteration 314, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:44] {2391} INFO -  at 699.0s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:44] {2218} INFO - iteration 315, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:01:48] {2391} INFO -  at 702.5s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:48] {2218} INFO - iteration 316, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:48] {2391} INFO -  at 703.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:48] {2218} INFO - iteration 317, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:49] {2391} INFO -  at 704.1s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:49] {2218} INFO - iteration 318, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:01:52] {2391} INFO -  at 707.2s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:52] {2218} INFO - iteration 319, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:54] {2391} INFO -  at 708.6s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:54] {2218} INFO - iteration 320, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:55] {2391} INFO -  at 709.6s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:55] {2218} INFO - iteration 321, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:56] {2391} INFO -  at 710.6s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:56] {2218} INFO - iteration 322, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:01:56] {2391} INFO -  at 711.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:56] {2218} INFO - iteration 323, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:01:58] {2391} INFO -  at 712.7s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:01:58] {2218} INFO - iteration 324, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:02:03] {2391} INFO -  at 717.9s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:02:03] {2218} INFO - iteration 325, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:02:04] {2391} INFO -  at 719.1s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:02:04] {2218} INFO - iteration 326, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:03:09] {2391} INFO -  at 784.0s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:09] {2218} INFO - iteration 327, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:10] {2391} INFO -  at 784.9s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:10] {2218} INFO - iteration 328, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:03:13] {2391} INFO -  at 788.0s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:13] {2218} INFO - iteration 329, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:03:15] {2391} INFO -  at 789.5s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:15] {2218} INFO - iteration 330, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:16] {2391} INFO -  at 790.4s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:16] {2218} INFO - iteration 331, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:17] {2391} INFO -  at 791.6s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:17] {2218} INFO - iteration 332, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:03:18] {2391} INFO -  at 792.7s,\testimator rf's best error=57.0367,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:18] {2218} INFO - iteration 333, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:18] {2391} INFO -  at 793.4s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:18] {2218} INFO - iteration 334, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:19] {2391} INFO -  at 794.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:19] {2218} INFO - iteration 335, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:20] {2391} INFO -  at 795.2s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:20] {2218} INFO - iteration 336, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:03:22] {2391} INFO -  at 796.6s,\testimator rf's best error=56.9373,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:22] {2218} INFO - iteration 337, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:23] {2391} INFO -  at 797.6s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:23] {2218} INFO - iteration 338, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:03:26] {2391} INFO -  at 801.2s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:26] {2218} INFO - iteration 339, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:27] {2391} INFO -  at 802.1s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:27] {2218} INFO - iteration 340, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:28] {2391} INFO -  at 803.2s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:28] {2218} INFO - iteration 341, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:29] {2391} INFO -  at 803.9s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:29] {2218} INFO - iteration 342, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:29] {2391} INFO -  at 804.2s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:29] {2218} INFO - iteration 343, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:30] {2391} INFO -  at 805.0s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:30] {2218} INFO - iteration 344, current learner xgboost\n",
      "[flaml.automl.logger: 11-12 21:03:33] {2391} INFO -  at 808.1s,\testimator xgboost's best error=56.7051,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:33] {2218} INFO - iteration 345, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:36] {2391} INFO -  at 810.5s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:36] {2218} INFO - iteration 346, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:36] {2391} INFO -  at 811.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:36] {2218} INFO - iteration 347, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:37] {2391} INFO -  at 812.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:37] {2218} INFO - iteration 348, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:38] {2391} INFO -  at 813.2s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:38] {2218} INFO - iteration 349, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:39] {2391} INFO -  at 814.2s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:39] {2218} INFO - iteration 350, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:40] {2391} INFO -  at 814.4s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:40] {2218} INFO - iteration 351, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:03:41] {2391} INFO -  at 815.6s,\testimator rf's best error=56.9373,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:41] {2218} INFO - iteration 352, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:03:47] {2391} INFO -  at 821.5s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:47] {2218} INFO - iteration 353, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:51] {2391} INFO -  at 826.3s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:51] {2218} INFO - iteration 354, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:52] {2391} INFO -  at 827.2s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:52] {2218} INFO - iteration 355, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:53] {2391} INFO -  at 827.4s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:53] {2218} INFO - iteration 356, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:53] {2391} INFO -  at 828.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:53] {2218} INFO - iteration 357, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:54] {2391} INFO -  at 829.0s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:54] {2218} INFO - iteration 358, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:57] {2391} INFO -  at 832.1s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:57] {2218} INFO - iteration 359, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:03:58] {2391} INFO -  at 832.7s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:58] {2218} INFO - iteration 360, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:03:59] {2391} INFO -  at 833.8s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:03:59] {2218} INFO - iteration 361, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:04:00] {2391} INFO -  at 834.5s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:00] {2218} INFO - iteration 362, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:04:01] {2391} INFO -  at 835.5s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:01] {2218} INFO - iteration 363, current learner rf\n",
      "[flaml.automl.logger: 11-12 21:04:02] {2391} INFO -  at 837.0s,\testimator rf's best error=56.9373,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:02] {2218} INFO - iteration 364, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:04:03] {2391} INFO -  at 837.8s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:03] {2218} INFO - iteration 365, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 21:04:06] {2391} INFO -  at 840.5s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:06] {2218} INFO - iteration 366, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:04:07] {2391} INFO -  at 841.4s,\testimator lgbm's best error=57.5178,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:07] {2218} INFO - iteration 367, current learner lgbm\n",
      "[flaml.automl.logger: 11-12 21:04:08] {2391} INFO -  at 843.1s,\testimator lgbm's best error=57.1190,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:08] {2218} INFO - iteration 368, current learner extra_tree\n",
      "[flaml.automl.logger: 11-12 21:04:09] {2391} INFO -  at 844.3s,\testimator extra_tree's best error=56.4628,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:09] {2218} INFO - iteration 369, current learner catboost\n",
      "[flaml.automl.logger: 11-12 21:04:14] {2391} INFO -  at 848.9s,\testimator catboost's best error=57.0255,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:04:14] {2218} INFO - iteration 370, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-12 21:05:05] {2391} INFO -  at 899.7s,\testimator xgb_limitdepth's best error=56.6147,\tbest estimator extra_tree's best error=56.4628\n",
      "[flaml.automl.logger: 11-12 21:05:05] {2493} INFO - selected model: ExtraTreesRegressor(max_features=0.8963598169194746, max_leaf_nodes=286,\n",
      "                    n_estimators=41, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 11-12 21:05:05] {1930} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-12 21:05:05] {1931} INFO - Time taken to find the best model: 699.0210132598877\n",
      "[flaml.automl.logger: 11-12 21:05:05] {1941} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "FLAML_TIME_BUDGET_A = 30*60\n",
    "FLAML_TIME_BUDGET_B_C = 15*60\n",
    "\n",
    "def trainFlamAutoML(letter, preprocessor, time_budget=60):\n",
    "    X, y = pre.general_read_flaml(letter)\n",
    "    X = pre.concatenate_dfs(X)\n",
    "    X_train, y_train,X_test, y_test = pre.train_test_split_may_june_july(X,y,letter)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    y_train = y_train[\"target\"]\n",
    "    y_test = y_test[\"target\"]\n",
    "    automl = AutoML()\n",
    "\n",
    "    automl_settings = {\n",
    "        \"time_budget\": time_budget,  # in seconds\n",
    "        \"metric\": 'mae',\n",
    "        \"task\": 'regression',\n",
    "        \"log_file_name\": f\"flaml_{letter}.log\",\n",
    "        \"seed\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "    automl.fit(X_train=X_train, y_train=y_train, X_val=X_test, y_val=y_test, **automl_settings)\n",
    "    return automl\n",
    "\n",
    "PREPROCESSORS = [\"quarters\"]\n",
    "\n",
    "for preprocessor in PREPROCESSORS:\n",
    "    flaml_preprocessor = pre.choose_transformer(preprocessor)\n",
    "    flaml_A = trainFlamAutoML(\"A\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_A)\n",
    "    flaml_B = trainFlamAutoML(\"B\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_B_C)\n",
    "    flaml_C = trainFlamAutoML(\"C\", preprocessor=flaml_preprocessor, time_budget=FLAML_TIME_BUDGET_B_C)\n",
    "    \n",
    "post.makePredictionWithModelAndPreprocessor(flaml_A,flaml_B,flaml_C,flaml_preprocessor,f\"{FOLDER_NAME}/flaml_{preprocessor}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11851\n",
      "[LightGBM] [Info] Number of data points in the train set: 31373, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 578.107824\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11725\n",
      "[LightGBM] [Info] Number of data points in the train set: 26551, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 83.214852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11932\n",
      "[LightGBM] [Info] Number of data points in the train set: 23564, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score 68.826784\n",
      "84/84 [==============================] - 0s 563us/step\n",
      "68/68 [==============================] - 0s 475us/step\n",
      "57/57 [==============================] - 0s 486us/step\n",
      "{'Catboost': 310.27867073622474, 'RandomForest': 321.7743821887169, 'LightGBM': 292.3713643246309, 'MLP': 365.449453076533, 'XGBoost': 350.61688006163}\n",
      "{'Catboost': 60.2924147966966, 'RandomForest': 60.58727115957187, 'LightGBM': 56.94559175369893, 'MLP': 59.006755385407, 'XGBoost': 64.2850922064208}\n",
      "{'Catboost': 46.81474067163569, 'RandomForest': 48.78346137769217, 'LightGBM': 47.25493023029752, 'MLP': 48.17339960303064, 'XGBoost': 52.67617921126694}\n",
      "{'Catboost': 310.27867073622474, 'RandomForest': 321.7743821887169, 'LightGBM': 292.3713643246309, 'MLP': 365.449453076533, 'XGBoost': 350.61688006163}\n",
      "Model score: 0.9825412226152855\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Catboost': 60.2924147966966, 'RandomForest': 60.58727115957187, 'LightGBM': 56.94559175369893, 'MLP': 59.006755385407, 'XGBoost': 64.2850922064208}\n",
      "Model score: 0.9828911443562427\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Catboost': 46.81474067163569, 'RandomForest': 48.78346137769217, 'LightGBM': 47.25493023029752, 'MLP': 48.17339960303064, 'XGBoost': 52.67617921126694}\n",
      "Model score: 0.9856590756077225\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 623us/step\n",
      "23/23 [==============================] - 0s 620us/step\n",
      "23/23 [==============================] - 0s 662us/step\n",
      "19.098437013333335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import catboost as cb\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def create_model_mlp(num_features, params):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            num_features,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"normal\",\n",
    "            input_shape=(num_features,),\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(16, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "    model.add(layers.Dense(1, kernel_initializer=\"normal\", activation=\"linear\"))\n",
    "\n",
    "    optimizer = SGD(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "    )\n",
    "    model.compile(loss=\"mean_absolute_error\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def blob(letter):\n",
    "\n",
    "\n",
    "\n",
    "    A_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.07524503439954632, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'learning_rate': 0.21457171264708794, 'iterations': 1743, 'l2_leaf_reg': 3.2654906719939096, 'border_count': 125, 'random_strength': 4, 'subsample': 0.9647762909882722},\n",
    "    {'n_estimators': 728, 'max_depth': 32, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'},\n",
    "    {'lambda_l1': 0.046833503396687244, 'lambda_l2': 0.006793841288240755, 'num_leaves': 163, 'feature_fraction': 0.929721178994391, 'bagging_fraction': 0.9044770721877625, 'bagging_freq': 6, 'min_child_samples': 16},\n",
    "    {'learning_rate': 0.0011881356000584273, 'momentum': 0.007427232252863111},\n",
    "    {'lambda': 4.008350742341045, 'alpha': 0.5938349159130611, 'eta': 0.41872596427506, 'gamma': 0.3960437023698051, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.6306741879739037, 'colsample_bytree': 0.8160324662642783, 'colsample_bylevel': 0.6887672828499098, 'colsample_bynode': 0.6657703558661234}]\n",
    "\n",
    "    B_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.0478383929191963, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'learning_rate': 0.2914259300415878, 'iterations': 1482, 'l2_leaf_reg': 0.01173059472637936, 'border_count': 67, 'random_strength': 1},\n",
    "    {'n_estimators': 1812, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'},\n",
    "    {'lambda_l1': 0.3292151184937159, 'lambda_l2': 0.00017039816288479435, 'num_leaves': 121, 'feature_fraction': 0.7992531492630386, 'bagging_fraction': 0.9062024635089169, 'bagging_freq': 1, 'min_child_samples': 12},\n",
    "    {'learning_rate': 0.00040347911788473766, 'momentum': 0.39280565026198133},\n",
    "    {'lambda': 0.020066610743154346, 'alpha': 0.020699686769877438, 'eta': 0.38009204594957435, 'gamma': 0.019310696968323503, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.7467461954995296, 'colsample_bytree': 0.9341895261741568, 'colsample_bylevel': 0.6869510780278182, 'colsample_bynode': 0.3781649042157331}]\n",
    "\n",
    "    C_params=[{'loss_function': 'MAE', 'colsample_bylevel': 0.07122595559105349, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'learning_rate': 0.1428232280057889, 'iterations': 1231, 'l2_leaf_reg': 0.03177024578576441, 'border_count': 111, 'random_strength': 7, 'bagging_temperature': 0.5470894828054673},\n",
    "    {'n_estimators': 1188, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'},\n",
    "    {'lambda_l1': 0.010132697944440957, 'lambda_l2': 1.3131072935454758e-08, 'num_leaves': 46, 'feature_fraction': 0.8270943641777372, 'bagging_fraction': 0.6628880080664098, 'bagging_freq': 4, 'min_child_samples': 19},\n",
    "    {'learning_rate': 0.0005264021357158383, 'momentum': 0.005758505842856219},\n",
    "    {'lambda': 4.0646418744914525, 'alpha': 0.01032666426760746, 'eta': 0.5072545314266129, 'gamma': 0.45062371002987023, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.9508152944450243, 'colsample_bytree': 0.5386940634341901, 'colsample_bylevel': 0.5562947611307176, 'colsample_bynode': 0.649251076479528}]\n",
    "\n",
    "    ens_params = {\n",
    "        \"A\": A_params,\n",
    "        \"B\": B_params,\n",
    "        \"C\": C_params\n",
    "    }\n",
    "\n",
    "    df = ensPre.trim(ensPre.readDataSet(letter), [])\n",
    "    df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "    y = df[\"target\"]\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y, holdout_X, holdout_y  = ensPre.new_train_test_split(X, y, letter,True)\n",
    "     \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled=pd.DataFrame(X_scaled,columns=X.columns,index=X.index)\n",
    "    train_x_scaled, valid_x_scaled, train_y_scaled, valid_y_scaled,holdout_X_scaled, holdout_y_scaled  = ensPre.new_train_test_split(X_scaled, y,letter,True)\n",
    "\n",
    "    train_x=pd.concat([train_x,valid_x],axis=0)\n",
    "    train_y=pd.concat([train_y,valid_y],axis=0)\n",
    "\n",
    "    train_x_scaled=pd.concat([train_x_scaled,valid_x_scaled],axis=0)\n",
    "    train_y_scaled=pd.concat([train_y_scaled,valid_y_scaled],axis=0)\n",
    "\n",
    "\n",
    "    Models={}\n",
    "    \n",
    "    study0_model=cb.CatBoostRegressor(**ens_params[letter][0])\n",
    "    study0_model.fit(train_x,train_y,verbose=0)\n",
    "    Models[\"Catboost\"]=study0_model\n",
    "\n",
    "    study1_model=RandomForestRegressor(**ens_params[letter][1])\n",
    "    study1_model.fit(train_x,train_y)\n",
    "    Models[\"RandomForest\"]=study1_model\n",
    "\n",
    "    study2_model=lgb.train(ens_params[letter][2],lgb.Dataset(train_x, label=train_y))\n",
    "    Models[\"LightGBM\"]=study2_model\n",
    "    \n",
    "    study3_model=create_model_mlp(X.shape[1], ens_params[letter][3])\n",
    "    study3_model.fit(train_x_scaled,train_y_scaled,shuffle=True,\n",
    "        batch_size=16,\n",
    "        epochs=100,\n",
    "        verbose=False)\n",
    "    Models[\"MLP\"]=study3_model\n",
    "\n",
    "    study4_model=xgb.train(ens_params[letter][4], xgb.DMatrix(train_x_scaled, label=train_y_scaled), verbose_eval=False)\n",
    "    Models[\"XGBoost\"]=study4_model\n",
    "    Models[\"Scaler\"]=scaler\n",
    "    \n",
    "    return Models\n",
    "\n",
    "def predict_base_models(X,base_models):\n",
    "\n",
    "    predictions = pd.DataFrame()\n",
    "    scaler = base_models[\"Scaler\"]\n",
    "    \n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled=pd.DataFrame(X_scaled,columns=X.columns,index=X.index)\n",
    "\n",
    "    for model_name, model in base_models.items():\n",
    "        # Make predictions with the current model\n",
    "        if model_name==\"MLP\":\n",
    "            pred = model.predict(X_scaled)\n",
    "        elif model_name==\"XGBoost\":\n",
    "            pred = model.predict(xgb.DMatrix(X_scaled))\n",
    "        elif model_name==\"Scaler\":\n",
    "            continue\n",
    "        else:\n",
    "            pred = model.predict(X)\n",
    "        # Add the predictions to the DataFrame\n",
    "        predictions[model_name] = pred\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def prepare_data(letter):\n",
    "    df = ensPre.trim(ensPre.readDataSet(letter), [])\n",
    "    df.columns = [\"\".join(c if c.isalnum() or c == '_' else '_' for c in str(x)) for x in df.columns]\n",
    "\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "    y = df[\"target\"]\n",
    "    train_x, valid_x, train_y, valid_y, holdout_X, holdout_y  = ensPre.new_train_test_split(X, y, letter,True)\n",
    "    return train_x, valid_x, train_y, valid_y, holdout_X, holdout_y\n",
    "\n",
    "\n",
    "A_base_models=blob(\"A\")\n",
    "B_base_models=blob(\"B\")\n",
    "C_base_models=blob(\"C\")\n",
    "\n",
    "\n",
    "A_train_x, A_valid_x, A_train_y, A_valid_y, A_holdout_X, A_holdout_y=prepare_data(\"A\")\n",
    "B_train_x, B_valid_x, B_train_y, B_valid_y, B_holdout_X, B_holdout_y=prepare_data(\"B\")\n",
    "C_train_x, C_valid_x, C_train_y, C_valid_y, C_holdout_X, C_holdout_y=prepare_data(\"C\")\n",
    "\n",
    "A_predictions_fitonval=(predict_base_models(A_holdout_X,A_base_models))\n",
    "B_predictions_fitonval=(predict_base_models(B_holdout_X,B_base_models))\n",
    "C_predictions_fitonval=(predict_base_models(C_holdout_X,C_base_models))\n",
    "\n",
    "A_predictions_fitonval[\"valdid\"]=A_holdout_y.values\n",
    "B_predictions_fitonval[\"valdid\"]=B_holdout_y.values\n",
    "C_predictions_fitonval[\"valdid\"]=C_holdout_y.values\n",
    " \n",
    "def calculate_mae(predictions_df, valid_column='valdid'):\n",
    "    mae_scores = {}\n",
    "\n",
    "    for model_name in predictions_df.columns:\n",
    "        if model_name != valid_column:\n",
    "            mae = mean_absolute_error(predictions_df[valid_column], predictions_df[model_name])\n",
    "            mae_scores[model_name] = mae\n",
    "\n",
    "    return mae_scores\n",
    "\n",
    "print(calculate_mae(A_predictions_fitonval))\n",
    "print(calculate_mae(B_predictions_fitonval))\n",
    "print(calculate_mae(C_predictions_fitonval))\n",
    "\n",
    "def trainmeta(letter):\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y,holdout_X, holdout_y  = prepare_data(letter)\n",
    "    train_x=pd.concat([train_x,valid_x,holdout_X],axis=0)\n",
    "    train_y=pd.concat([train_y,valid_y,holdout_y],axis=0)\n",
    "\n",
    "    # Replace 'A' with 'letter'\n",
    "    predictions_fitonval = globals()[f'{letter}_predictions_fitonval']\n",
    "    X_meta = predictions_fitonval[[\"Catboost\",\"RandomForest\",\"LightGBM\",\"MLP\",\"XGBoost\"]]\n",
    "    y_meta = holdout_y\n",
    "\n",
    "    mae_values = {}\n",
    "    for column in predictions_fitonval[[\"Catboost\",\"RandomForest\",\"LightGBM\",\"MLP\",\"XGBoost\"]]:\n",
    "    \n",
    "        mae = mean_absolute_error(y_meta, predictions_fitonval[column])\n",
    "        mae_values[column] = mae\n",
    "\n",
    "    print(mae_values)\n",
    "\n",
    "    # Create and train the Linear Regression model\n",
    "    meta_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    meta_model.fit(X_meta, y_meta)\n",
    "\n",
    "    # Score the model\n",
    "    score = meta_model.score(X_meta, y_meta)\n",
    "    print(f\"Model score: {score}\")\n",
    "\n",
    "    # Make predictions and add them to the DataFrame\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "# Get feature importances\n",
    "    importances = meta_model.feature_importances_\n",
    "\n",
    "    # Get the indices of the importances sorted by their values\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'Feature Importances for {letter}')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [X_meta.columns[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    return meta_model\n",
    "\n",
    "\n",
    "meta_A=trainmeta(\"A\")\n",
    "meta_B=trainmeta(\"B\")\n",
    "meta_C=trainmeta(\"C\")\n",
    "\n",
    "\n",
    "\n",
    "A_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"A\")))\n",
    "A_x_test=ensPre.readtest(\"A\")\n",
    "A_x_test=ensPre.preptest(A_x_test,[],A_stats)\n",
    "\n",
    "B_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"B\")))\n",
    "B_x_test=ensPre.readtest(\"B\")\n",
    "B_x_test=ensPre.preptest(B_x_test,[],B_stats)\n",
    "\n",
    "C_stats=ensPre.calculate_hourly_monthly_means(ensPre.clean(ensPre.readDataSet(\"C\")))\n",
    "C_x_test=ensPre.readtest(\"C\")\n",
    "C_x_test=ensPre.preptest(C_x_test,[],C_stats)\n",
    "\n",
    "\n",
    "A_base_preds=(predict_base_models(A_x_test,A_base_models))\n",
    "B_base_preds=(predict_base_models(B_x_test,B_base_models))\n",
    "C_base_preds=(predict_base_models(C_x_test,C_base_models))\n",
    "\n",
    "A_y_pred=meta_A.predict(A_base_preds)\n",
    "B_y_pred=meta_B.predict(B_base_preds)\n",
    "C_y_pred=meta_C.predict(C_base_preds)\n",
    "\n",
    "A_y_pred=pd.DataFrame(A_y_pred, index=A_x_test.index, columns=['y_pred'])\n",
    "print(0.016*A_y_pred[\"y_pred\"].mean())\n",
    "A_y_pred[A_y_pred[\"y_pred\"]<0.0016*A_y_pred[\"y_pred\"].mean()]=0\n",
    "\n",
    "B_y_pred=pd.DataFrame(B_y_pred, index=B_x_test.index, columns=['y_pred'])\n",
    "B_y_pred[B_y_pred[\"y_pred\"]<0.0016*B_y_pred[\"y_pred\"].mean()]=0\n",
    "C_y_pred=pd.DataFrame(C_y_pred, index=C_x_test.index, columns=['y_pred'])\n",
    "C_y_pred[C_y_pred[\"y_pred\"]<0.0016*C_y_pred[\"y_pred\"].mean()]=0\n",
    "\n",
    "\n",
    "combined_pred = pd.concat([A_y_pred, B_y_pred, C_y_pred], axis=0)\n",
    "combined_pred[\"y_pred\"] = combined_pred[\"y_pred\"].clip(lower=0)\n",
    "combined_pred.to_csv(\"combined_pred.csv\", index=True)\n",
    "combined_pred=(pd.read_csv(\"combined_pred.csv\"))\n",
    "\n",
    "# Add the \"Id\" column\n",
    "combined_pred.insert(0, \"id\", range(len(combined_pred)))\n",
    "combined_pred = combined_pred.rename(columns={combined_pred.columns[-1]: \"prediction\"})\n",
    "combined_pred[[\"id\", \"prediction\"]].to_csv(f\"{FOLDER_NAME}/Ensemble_prediction.csv\", index=False)\n",
    "os.remove(\"combined_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking with weights csvfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wheights chosen from intuition and trial.\n",
    "dataframes = {}\n",
    "for filename in os.listdir(FOLDER_NAME):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(FOLDER_NAME, filename)\n",
    "        dataframe_name = filename.split('.')[0]  # Get the name of the file without the extension\n",
    "        dataframes[dataframe_name] = pd.read_csv(file_path)\n",
    "\n",
    "data = dataframes[\"Ensemble_prediction\"][\"prediction\"]*0.2 + dataframes[\"DNN\"][\"prediction\"]*0.4 + dataframes[\"flaml_quarters\"][\"prediction\"]*0.4\n",
    "data = pd.DataFrame(data, columns=[\"prediction\"])\n",
    "\n",
    "\n",
    "data.index.name = \"id\"\n",
    "\n",
    "\n",
    "data.to_csv(\"Short_notebook1_submit.csv\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
